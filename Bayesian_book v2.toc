\contentsline {chapter}{\numberline {1}How to best use this book}{21}
\contentsline {section}{\numberline {1.1}The purpose of this book}{21}
\contentsline {section}{\numberline {1.2}Who is this book for?}{23}
\contentsline {section}{\numberline {1.3}Pre-requisites}{23}
\contentsline {section}{\numberline {1.4}Book outline}{24}
\contentsline {section}{\numberline {1.5}Route planner - suggested journeys through Bayesland}{26}
\contentsline {section}{\numberline {1.6}Video}{27}
\contentsline {section}{\numberline {1.7}Interactive elements}{28}
\contentsline {section}{\numberline {1.8}Interactive problem sets}{28}
\contentsline {section}{\numberline {1.9}Code}{28}
\contentsline {section}{\numberline {1.10}R, Stan and JAGS}{29}
\contentsline {section}{\numberline {1.11}Why don't more people use Bayesian statistics?}{30}
\contentsline {section}{\numberline {1.12}What are the tangible (non-academic) benefits of Bayesian statistics?}{31}
\contentsline {section}{\numberline {1.13}Suggested further reading}{32}
\contentsline {part}{I\hspace {1em}An introduction to Bayesian inference}{33}
\contentsline {section}{\numberline {1.14}Part mission statement}{35}
\contentsline {section}{\numberline {1.15}Part goals}{35}
\contentsline {chapter}{\numberline {2}The subjective worlds of Frequentist and Bayesian statistics}{37}
\contentsline {section}{\numberline {2.1}Chapter mission statement}{37}
\contentsline {section}{\numberline {2.2}Chapter goals}{37}
\contentsline {section}{\numberline {2.3}Bayes' rule - allowing us to go from the effect back to its cause}{38}
\contentsline {section}{\numberline {2.4}The purpose of statistical inference}{39}
\contentsline {section}{\numberline {2.5}The world according to Frequentists}{40}
\contentsline {section}{\numberline {2.6}The world according to Bayesians}{42}
\contentsline {section}{\numberline {2.7}Frequentist and Bayesian inference}{43}
\contentsline {subsection}{\numberline {2.7.1}The Frequentist and Bayesian murder trials}{44}
\contentsline {subsection}{\numberline {2.7.2}Radio control towers: example}{45}
\contentsline {section}{\numberline {2.8}Bayesian inference via Bayes' rule}{46}
\contentsline {subsection}{\numberline {2.8.1}Likelihoods}{47}
\contentsline {subsection}{\numberline {2.8.2}Priors}{48}
\contentsline {subsection}{\numberline {2.8.3}The denominator}{49}
\contentsline {subsection}{\numberline {2.8.4}Posteriors: the goal of Bayesian inference}{49}
\contentsline {section}{\numberline {2.9}Implicit vs Explicit subjectivity}{51}
\contentsline {section}{\numberline {2.10}Chapter summary}{53}
\contentsline {section}{\numberline {2.11}Chapter outcomes}{53}
\contentsline {section}{\numberline {2.12}Problem set}{53}
\contentsline {subsection}{\numberline {2.12.1}The deterministic nature of random coin throwing.}{53}
\contentsline {subsubsection}{Suppose that all combinations of angles and heights are equally likely to be chosen. What is the probability that the coin lands on heads?}{54}
\contentsline {subsubsection}{What are the new probabilities that the coin lands heads-up?}{55}
\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at an angle of 45 degrees. What is the probability that the coin lands heads-up?}{55}
\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at a height of 0.2m. What is the probability that the coin lands heads-up?}{55}
\contentsline {subsubsection}{If we constrained the angle and height to be fixed, what would happen in repetitions of the same experiment?}{55}
\contentsline {subsubsection}{In light of the previous question, comment on the Frequentist assumption of \textit {exact repetitions} of a given experiment.}{55}
\contentsline {subsection}{\numberline {2.12.2}Model choice}{55}
\contentsline {subsubsection}{Fit a linear regression model using classical least squares. How reasonable is the fit?}{56}
\contentsline {subsubsection}{Fit a quintic (powers up to the 5th) model to the data. How does its fit compare to that of the linear model?}{56}
\contentsline {subsubsection}{Fit a linear regression to each of the data sets, and similarly for the quintic model. Which of these performs best?}{56}
\contentsline {subsubsection}{Using the fits from the first part of this question, compare the performance of the linear regression model, with that of the quintic model.}{56}
\contentsline {subsubsection}{Which of the two models do you prefer, and why?}{56}
\contentsline {subsubsection}{If you then found out that the data were years of education (x), and salary in \$000s (y). Which model would you favour?}{56}
\contentsline {section}{\numberline {2.13}Appendix}{56}
\contentsline {subsection}{\numberline {2.13.1}The Frequentist and Bayesian murder trial}{56}
\contentsline {chapter}{\numberline {3}Probability - the nuts and bolts of Bayesian inference}{59}
\contentsline {section}{\numberline {3.1}Chapter mission statement}{59}
\contentsline {section}{\numberline {3.2}Chapter goals}{59}
\contentsline {section}{\numberline {3.3}Probability distributions: helping us explicitly state our ignorance}{60}
\contentsline {subsection}{\numberline {3.3.1}What makes a probability distribution \textit {valid}?}{60}
\contentsline {subsection}{\numberline {3.3.2}Probabilities vs probability density : interpreting discrete and continuous probability distributions}{62}
\contentsline {subsubsection}{A (wet) river crossing}{65}
\contentsline {subsubsection}{Probability zero vs impossibility}{68}
\contentsline {subsubsection}{The good news: Bayes' rule doesn't distinguish between probabilities and probability densities}{69}
\contentsline {subsection}{\numberline {3.3.3}Mean and variance of distributions}{70}
\contentsline {subsection}{\numberline {3.3.4}Generalising probability distributions to two dimensions}{73}
\contentsline {subsubsection}{Horses for courses: a 2-dimensional discrete probability example}{74}
\contentsline {subsubsection}{Foot length and literacy: a 2-dimensional continuous probability example}{76}
\contentsline {subsection}{\numberline {3.3.5}Marginal distributions}{76}
\contentsline {subsubsection}{Venn diagrams}{80}
\contentsline {subsection}{\numberline {3.3.6}Conditional distributions}{80}
\contentsline {section}{\numberline {3.4}Higher dimensional probability densities: no harder than 2-D, just looks it!}{83}
\contentsline {section}{\numberline {3.5}Independence}{86}
\contentsline {subsection}{\numberline {3.5.1}Conditional independence}{89}
\contentsline {section}{\numberline {3.6}Central Limit Theorems}{90}
\contentsline {section}{\numberline {3.7}The Bayesian formula}{93}
\contentsline {subsection}{\numberline {3.7.1}The intuition behind the formula}{94}
\contentsline {subsection}{\numberline {3.7.2}Breast cancer screeing}{95}
\contentsline {section}{\numberline {3.8}The Bayesian inference process from the Bayesian formula}{96}
\contentsline {section}{\numberline {3.9}Chapter summary}{97}
\contentsline {section}{\numberline {3.10}Chapter outcomes}{97}
\contentsline {section}{\numberline {3.11}Problem set}{98}
\contentsline {subsection}{\numberline {3.11.1}The expected returns of a derivative}{98}
\contentsline {subsubsection}{What are the expected returns of the stock?}{98}
\contentsline {subsubsection}{What are the variance in returns?}{98}
\contentsline {subsubsection}{Would you expect the return to be equal to the mean of the original stock squared? If not, why not?}{98}
\contentsline {subsubsection}{What would be a fair price to pay for this derivative?}{98}
\contentsline {subsubsection}{What is a fair price to pay for this stock?}{99}
\contentsline {subsubsection}{Which asset is more risky, $D_t$ or $R_t$?}{99}
\contentsline {subsubsection}{Use \textit {method of moments} to estimate $\mu $ and $\sigma $. Note: this requires use of a computer with mathematical software, as analytic solutions aren't possible.}{99}
\contentsline {subsubsection}{Compare the estimated model with the data.}{99}
\contentsline {subsubsection}{Is the model a reasonable approximation to the data generating process?}{99}
\contentsline {subsubsection}{If not, suggest a better alternative.}{99}
\contentsline {subsection}{\numberline {3.11.2}The boy or girl paradox\let \reserved@d =[\def \par }{99}
\contentsline {subsection}{\numberline {3.11.3}The Bayesian game show}{99}
\contentsline {subsection}{\numberline {3.11.4}Blood doping}{100}
\contentsline {subsubsection}{What is the probability that a professional cyclist wins a race?}{100}
\contentsline {subsubsection}{What is the probability that a cyclist wins a race given that they have cheated?}{100}
\contentsline {subsubsection}{What is the probability that a cyclist is cheating given that he wins?}{100}
\contentsline {part}{II\hspace {1em}Understanding the Bayesian formula}{103}
\contentsline {section}{\numberline {3.12}Part mission statement}{105}
\contentsline {section}{\numberline {3.13}Part goals}{105}
\contentsline {chapter}{\numberline {4}The posterior - the goal of Bayesian inference}{107}
\contentsline {section}{\numberline {4.1}Chapter Mission statement}{107}
\contentsline {section}{\numberline {4.2}Chapter goals}{107}
\contentsline {section}{\numberline {4.3}Expressing uncertainty through the posterior probability distribution}{108}
\contentsline {subsection}{\numberline {4.3.1}Bayesian coastguard: introducing the prior and the posterior}{111}
\contentsline {subsection}{\numberline {4.3.2}Bayesian statistics: updating our pre-analysis uncertainty}{112}
\contentsline {subsection}{\numberline {4.3.3}Do parameters actually exist and have a point value?}{114}
\contentsline {subsection}{\numberline {4.3.4}Failings of the Frequentist confidence interval}{115}
\contentsline {subsection}{\numberline {4.3.5}Credible intervals}{117}
\contentsline {subsubsection}{Treasure hunting: The central posterior and highest density intervals}{118}
\contentsline {subsection}{\numberline {4.3.6}Reconciling the difference between confidence and credible intervals}{119}
\contentsline {subsubsection}{The interval ENIGMA}{120}
\contentsline {section}{\numberline {4.4}Point parameter estimates}{123}
\contentsline {section}{\numberline {4.5}Prediction using predictive distributions}{126}
\contentsline {subsection}{\numberline {4.5.1}Example: number of Republican voters within a sample}{127}
\contentsline {subsection}{\numberline {4.5.2}Example: interest rate hedging}{131}
\contentsline {section}{\numberline {4.6}Model comparison using the posterior}{134}
\contentsline {subsection}{\numberline {4.6.1}Example: epidemiologist comparison}{137}
\contentsline {subsection}{\numberline {4.6.2}Example: customer footfall}{138}
\contentsline {section}{\numberline {4.7}Model comparison through posterior predictive checks}{142}
\contentsline {subsection}{\numberline {4.7.1}Example: stock returns}{142}
\contentsline {section}{\numberline {4.8}Chapter summary}{143}
\contentsline {section}{\numberline {4.9}Chapter outcomes}{144}
\contentsline {section}{\numberline {4.10}Problem set}{145}
\contentsline {subsection}{\numberline {4.10.1}The lesser evil}{145}
\contentsline {subsubsection}{Which of the above loss functions do you think is most appropriate, and why?}{146}
\contentsline {subsubsection}{Which loss function might you choose to be most robust to \textit {any} situation?}{146}
\contentsline {subsubsection}{Following from the previous point, which measure of posterior centrality might you choose?}{146}
\contentsline {subsection}{\numberline {4.10.2}Google word search prediction}{146}
\contentsline {subsubsection}{Find the minimum-coverage confidence intervals of topics that exceed 70\%.}{146}
\contentsline {subsubsection}{Find most narrow credible intervals for topics that exceed 70\%.}{146}
\contentsline {subsubsection}{Topic search volumes}{146}
\contentsline {subsubsection}{Three-letter search volumes}{147}
\contentsline {subsection}{\numberline {4.10.3}Prior and posterior predictive example (with PPCs maybe)}{147}
\contentsline {section}{\numberline {4.11}Appendix}{147}
\contentsline {subsection}{\numberline {4.11.1}The interval ENIGMA - explained in full}{147}
\contentsline {chapter}{\numberline {5}Likelihoods}{149}
\contentsline {section}{\numberline {5.1}Chapter Mission statement}{149}
\contentsline {section}{\numberline {5.2}Chapter goals}{149}
\contentsline {section}{\numberline {5.3}What is a likelihood?}{150}
\contentsline {section}{\numberline {5.4}Why use 'likelihood' rather than 'probability'?}{152}
\contentsline {section}{\numberline {5.5}What are models and why do we need them?}{155}
\contentsline {section}{\numberline {5.6}How to choose an appropriate likelihood?}{157}
\contentsline {subsection}{\numberline {5.6.1}A likelihood model for an individual's disease status}{158}
\contentsline {subsection}{\numberline {5.6.2}A likelihood model for disease prevalence of a group}{159}
\contentsline {subsection}{\numberline {5.6.3}The intelligence of a group of people}{166}
\contentsline {section}{\numberline {5.7}Exchangeability vs random sampling}{170}
\contentsline {section}{\numberline {5.8}The subjectivity of model choice}{172}
\contentsline {section}{\numberline {5.9}Maximum likelihood - a short introduction}{173}
\contentsline {subsection}{\numberline {5.9.1}Estimating disease prevalence}{173}
\contentsline {subsection}{\numberline {5.9.2}Estimating the mean and variance in intelligence scores}{176}
\contentsline {subsection}{\numberline {5.9.3}Maximum likelihood in simple steps}{177}
\contentsline {section}{\numberline {5.10}Frequentist inference in Maximum Likelihood}{178}
\contentsline {section}{\numberline {5.11}Chapter summary}{178}
\contentsline {section}{\numberline {5.12}Chapter outcomes}{180}
\contentsline {section}{\numberline {5.13}Problem set}{180}
\contentsline {subsection}{\numberline {5.13.1}Blog blues.}{180}
\contentsline {subsubsection}{What assumptions might you make about the first-time visits?}{181}
\contentsline {subsubsection}{What model might be appropriate to model the time between visits?}{181}
\contentsline {subsubsection}{Algebraically derive an estimate of the mean number of visits per hour}{181}
\contentsline {subsubsection}{Data analysis:}{181}
\contentsline {subsubsection}{Graph the log likelihood near your estimated value. What does this show? Why don't we plot the likelihood?}{181}
\contentsline {subsubsection}{Estimate confidence intervals around your parameter.}{181}
\contentsline {subsubsection}{What is the probability that you will wait:}{181}
\contentsline {subsubsection}{Evaluate your model.}{181}
\contentsline {subsubsection}{What alternative models might be useful here?}{181}
\contentsline {subsubsection}{What are the assumptions behind these models?}{181}
\contentsline {subsubsection}{Estimate the parameters of your new model.}{181}
\contentsline {subsubsection}{Use your new model to estimate the probability that you will wait:}{181}
\contentsline {subsubsection}{Hints: the exponential is to the poisson model, what the ? is to the negative binomial.}{182}
\contentsline {subsection}{\numberline {5.13.2}Violent crime counts in New York counties}{182}
\contentsline {subsubsection}{Graph the violent crime account against population. What type of relationship does this suggest?}{182}
\contentsline {subsubsection}{A simple model}{182}
\contentsline {subsubsection}{What are the assumptions of this model?}{183}
\contentsline {subsubsection}{Estimate the parameter $\theta $ from the data. What does this parameter represent?}{183}
\contentsline {subsubsection}{Do these assumptions seem realistic?}{183}
\contentsline {subsubsection}{Estimate a measure of uncertainty in your estimates.}{183}
\contentsline {subsubsection}{Evaluate the performance of your model.}{183}
\contentsline {subsubsection}{A better model}{183}
\contentsline {subsubsection}{Why is this model better?}{183}
\contentsline {subsubsection}{What factors might affect $\theta _i$?}{183}
\contentsline {subsubsection}{Write down a new model specification taking into account the previous point.}{183}
\contentsline {subsubsection}{Estimate the parameters of this new specification.}{183}
\contentsline {subsubsection}{How does this new model compare to the previous iteration?}{183}
\contentsline {subsubsection}{What alternative specifications might be worth attempting?}{183}
\contentsline {subsection}{\numberline {5.13.3}Monte Carlo evaluation of the performance of MLE in R}{183}
\contentsline {subsection}{\numberline {5.13.4}The sample mean as MLE}{184}
\contentsline {chapter}{\numberline {6}Priors}{185}
\contentsline {section}{\numberline {6.1}Chapter Mission statement}{185}
\contentsline {section}{\numberline {6.2}Chapter goals}{185}
\contentsline {section}{\numberline {6.3}What are priors, and what do they represent?}{186}
\contentsline {section}{\numberline {6.4}Why do we need priors at all?}{188}
\contentsline {section}{\numberline {6.5}Why don't we just normalise likelihood by choosing a unity prior?}{189}
\contentsline {section}{\numberline {6.6}The explicit subjectivity of priors}{192}
\contentsline {section}{\numberline {6.7}Combining a prior and likelihood to form a posterior}{192}
\contentsline {subsection}{\numberline {6.7.1}The Goldfish game}{192}
\contentsline {subsection}{\numberline {6.7.2}Disease proportions revisited}{195}
\contentsline {section}{\numberline {6.8}Constructing priors}{198}
\contentsline {subsection}{\numberline {6.8.1}Vague priors}{198}
\contentsline {subsection}{\numberline {6.8.2}Informative priors}{202}
\contentsline {subsection}{\numberline {6.8.3}The numerator of Bayes' rule determines the shape}{204}
\contentsline {subsection}{\numberline {6.8.4}Eliciting priors}{204}
\contentsline {section}{\numberline {6.9}A strong model is less sensitive to prior choice}{205}
\contentsline {subsection}{\numberline {6.9.1}Caveat: overly-zealous priors \textit {will} affect the posterior}{207}
\contentsline {section}{\numberline {6.10}Chapter summary}{208}
\contentsline {section}{\numberline {6.11}Chapter outcomes}{210}
\contentsline {section}{\numberline {6.12}Problem set}{210}
\contentsline {subsection}{\numberline {6.12.1}Counting sheep}{210}
\contentsline {subsubsection}{What likelihood model might you use here?}{211}
\contentsline {subsubsection}{What are the assumptions underpinning this model?}{211}
\contentsline {subsubsection}{Introducing a prior}{211}
\contentsline {subsubsection}{Which of the previous priors is most uninformative?}{211}
\contentsline {subsubsection}{Suppose that you observe 10 sheep jumping over the fence, calculate the posterior distribution for each of the different priors using your chosen likelihood.}{211}
\contentsline {subsubsection}{For what numbers of jumping sheep would one of your posteriors run into problems?}{211}
\contentsline {subsection}{\numberline {6.12.2}Investigating priors through US elections}{211}
\contentsline {subsection}{\numberline {6.12.3}Choosing prior distributions.}{212}
\contentsline {subsubsection}{If we choose a prior distribution that sets $p(log(\sigma ))=1$, what does this imply about the distribution of $p(\sigma )$?}{212}
\contentsline {subsubsection}{For a parameter $\theta $, which choice of prior will leave it invariant under transformations?}{212}
\contentsline {subsubsection}{Pre-experimental data prior setting}{212}
\contentsline {subsection}{\numberline {6.12.4}Expert data prior example}{212}
\contentsline {subsection}{\numberline {6.12.5}Data analysis example showing the declining importance of prior as data set increases in size}{212}
\contentsline {section}{\numberline {6.13}Appendix}{213}
\contentsline {subsection}{\numberline {6.13.1}Bayes' rule for the urn}{213}
\contentsline {subsection}{\numberline {6.13.2}The probabilities of having a disease}{213}
\contentsline {chapter}{\numberline {7}The devil's in the denominator}{215}
\contentsline {section}{\numberline {7.1}Chapter mission}{215}
\contentsline {section}{\numberline {7.2}Chapter goals}{215}
\contentsline {section}{\numberline {7.3}An introduction to the denominator}{216}
\contentsline {subsection}{\numberline {7.3.1}The denominator as a normalising factor}{216}
\contentsline {subsection}{\numberline {7.3.2}Example: disease}{217}
\contentsline {subsection}{\numberline {7.3.3}Example: the proportion of people who vote the Conservatives}{219}
\contentsline {subsection}{\numberline {7.3.4}The denominator as a probability}{221}
\contentsline {subsection}{\numberline {7.3.5}Using the denominator to choose between competing models}{223}
\contentsline {section}{\numberline {7.4}The difficulty with the denominator}{228}
\contentsline {subsection}{\numberline {7.4.1}Multi-parameter discrete model example: the comorbidity between depression and anxiety}{228}
\contentsline {subsection}{\numberline {7.4.2}Continuous multi-parameter example: mean and variance of IQ}{231}
\contentsline {section}{\numberline {7.5}How to dispense with the difficulty: Bayesian computation}{235}
\contentsline {section}{\numberline {7.6}Chapter summary}{237}
\contentsline {section}{\numberline {7.7}Chapter outcomes}{237}
\contentsline {section}{\numberline {7.8}Problem set}{238}
\contentsline {subsection}{\numberline {7.8.1}New disease cases}{238}
\contentsline {subsubsection}{What likelihood model might be appropriate here?}{238}
\contentsline {subsubsection}{What are the assumptions of this model? Are they appropriate here?}{238}
\contentsline {subsubsection}{A gamma prior}{238}
\contentsline {subsubsection}{Data}{238}
\contentsline {subsubsection}{Find the posterior distribution for the mean parameter $\lambda $}{239}
\contentsline {subsection}{\numberline {7.8.2}The comorbidity between depression, anxiety and psychosis}{239}
\contentsline {subsubsection}{Calculate the probability that a patient is depressed.}{239}
\contentsline {subsubsection}{Calculate the probability that a patient is psychotic.}{239}
\contentsline {subsubsection}{What is the probability that a patient is psychotic given that they are depressed, and anxious?}{239}
\contentsline {subsubsection}{What is the probability that a patient is not psychotic if they are not depressed?}{239}
\contentsline {subsection}{\numberline {7.8.3}Finding mosquito larvae after rain}{239}
\contentsline {subsubsection}{Find the mean of a $lognormal(d,1)$ distribution.}{240}
\contentsline {subsubsection}{Finding the mode}{240}
\contentsline {subsubsection}{Use a computer to graph the shape of the posterior distribution.}{240}
\contentsline {subsubsection}{Calculate the posterior (difficult).}{240}
\contentsline {section}{\numberline {7.9}Appendix}{240}
\contentsline {part}{III\hspace {1em}Analytic Bayesian methods}{241}
\contentsline {section}{\numberline {7.10}Part mission statement}{243}
\contentsline {section}{\numberline {7.11}Part goals}{243}
\contentsline {chapter}{\numberline {8}An introduction to distributions for the mathematically-un-inclined}{245}
\contentsline {section}{\numberline {8.1}Chapter mission statement}{245}
\contentsline {section}{\numberline {8.2}Chapter goals}{245}
\contentsline {section}{\numberline {8.3}The interrelation among distributions}{246}
\contentsline {section}{\numberline {8.4}Sampling distributions for likelihoods}{247}
\contentsline {subsection}{\numberline {8.4.1}Bernoulli}{248}
\contentsline {subsection}{\numberline {8.4.2}Binomial}{251}
\contentsline {subsection}{\numberline {8.4.3}Poisson}{254}
\contentsline {subsection}{\numberline {8.4.4}Negative binomial}{257}
\contentsline {subsection}{\numberline {8.4.5}Beta-binomial}{262}
\contentsline {subsection}{\numberline {8.4.6}Normal}{264}
\contentsline {subsection}{\numberline {8.4.7}Student t}{267}
\contentsline {subsection}{\numberline {8.4.8}Exponential}{272}
\contentsline {subsection}{\numberline {8.4.9}Gamma}{274}
\contentsline {subsection}{\numberline {8.4.10}Multinomial}{277}
\contentsline {subsection}{\numberline {8.4.11}Multivariate normal and multivariate t}{281}
\contentsline {section}{\numberline {8.5}Prior distributions}{284}
\contentsline {subsection}{\numberline {8.5.1}Distributions for probabilities, proportions and percentages}{285}
\contentsline {subsubsection}{Uniform}{285}
\contentsline {subsubsection}{Beta}{286}
\contentsline {subsubsection}{Logit-normal}{288}
\contentsline {subsubsection}{Dirichlet}{290}
\contentsline {subsection}{\numberline {8.5.2}Distributions for means and regression coefficients}{293}
\contentsline {subsubsection}{Normal}{294}
\contentsline {subsubsection}{Student t}{295}
\contentsline {subsubsection}{Cauchy}{296}
\contentsline {subsubsection}{Multivariate normal, and multivariate t}{298}
\contentsline {subsection}{\numberline {8.5.3}Distributions for non-negative parameters}{300}
\contentsline {subsubsection}{Gamma}{300}
\contentsline {subsubsection}{Half-Cauchy, inverse-gamma, inverse-$\chi ^2$ and uniform}{301}
\contentsline {subsubsection}{Log-normal}{304}
\contentsline {subsection}{\numberline {8.5.4}Distributions for covariance and correlation matrices}{306}
\contentsline {subsubsection}{LKJ}{307}
\contentsline {subsubsection}{Wishart and inverse-Wishart}{313}
\contentsline {section}{\numberline {8.6}Choosing a likelihood made easy}{316}
\contentsline {section}{\numberline {8.7}Table of common likelihoods, their uses, and reasonable priors}{316}
\contentsline {section}{\numberline {8.8}Distributions of distributions, and mixtures - link to website, and relevance}{316}
\contentsline {section}{\numberline {8.9}Chapter summary}{318}
\contentsline {section}{\numberline {8.10}Chapter outcomes}{318}
\contentsline {section}{\numberline {8.11}Problem set}{319}
\contentsline {subsection}{\numberline {8.11.1}Drug trials}{319}
\contentsline {subsection}{\numberline {8.11.2}100m results across countries}{320}
\contentsline {subsection}{\numberline {8.11.3}Triangular representation of simplexes}{320}
\contentsline {subsubsection}{Show that the triangular representation of a 3 parameter Dirichlet is correct.}{320}
\contentsline {subsection}{\numberline {8.11.4}Normal distribution with normal prior}{321}
\contentsline {subsubsection}{Prove that the mean of the lower distribution is the same as that of the prior}{321}
\contentsline {subsubsection}{Find the mean of the lower distribution}{321}
\contentsline {subsubsection}{Prove that the distribution of the lower distribution is unconditionally normal}{321}
\contentsline {chapter}{\numberline {9}Conjugate priors and their place in Bayesian analysis}{323}
\contentsline {section}{\numberline {9.1}Chapter mission statement}{323}
\contentsline {section}{\numberline {9.2}Chapter goals}{323}
\contentsline {section}{\numberline {9.3}What is a conjugate prior and why are they useful?}{324}
\contentsline {section}{\numberline {9.4}Gamma-poisson example}{328}
\contentsline {section}{\numberline {9.5}Normal example: extra}{329}
\contentsline {section}{\numberline {9.6}Table of conjugate priors}{332}
\contentsline {section}{\numberline {9.7}The lessons and limits of a conjugate analysis}{332}
\contentsline {section}{\numberline {9.8}Chapter summary}{334}
\contentsline {section}{\numberline {9.9}Chapter outcomes}{334}
\contentsline {chapter}{\numberline {10}Evaluation of model fit and hypothesis testing}{335}
\contentsline {section}{\numberline {10.1}Chapter mission statement}{335}
\contentsline {section}{\numberline {10.2}Chapter goals}{335}
\contentsline {section}{\numberline {10.3}Posterior predictive checks}{336}
\contentsline {subsection}{\numberline {10.3.1}Recap - posterior predictive distributions}{336}
\contentsline {subsection}{\numberline {10.3.2}Graphical PPCs and Bayesian p values}{338}
\contentsline {subsubsection}{Amazon staff hiring}{339}
\contentsline {subsubsection}{Mosquito lifetime}{342}
\contentsline {subsubsection}{Snow days}{343}
\contentsline {subsubsection}{Word distributions within topics}{344}
\contentsline {subsubsection}{Bacterial infection}{347}
\contentsline {section}{\numberline {10.4}Why do we call it a \textit {p value}?}{350}
\contentsline {section}{\numberline {10.5}Statistics measuring predictive accuracy: AIC, Deviance, WAIC and LOO}{351}
\contentsline {subsection}{\numberline {10.5.1}Independent evaluation of a model, and overfitting}{351}
\contentsline {subsection}{\numberline {10.5.2}How to measure a model's predictive capability?}{352}
\contentsline {subsection}{\numberline {10.5.3}The ideal measure of a model's predictive accuracy}{354}
\contentsline {subsection}{\numberline {10.5.4}Estimating out-of-sample predictive accuracy from in-sample data}{358}
\contentsline {subsection}{\numberline {10.5.5}AIC}{359}
\contentsline {subsection}{\numberline {10.5.6}DIC}{359}
\contentsline {subsection}{\numberline {10.5.7}WAIC}{360}
\contentsline {subsection}{\numberline {10.5.8}LOO}{365}
\contentsline {subsection}{\numberline {10.5.9}A practical and short summary of measures of predictive accuracy in simple terms}{366}
\contentsline {section}{\numberline {10.6}Choosing one model, or a number?}{368}
\contentsline {section}{\numberline {10.7}Sensitivity analysis}{368}
\contentsline {section}{\numberline {10.8}Chapter summary}{370}
\contentsline {section}{\numberline {10.9}Chapter outcomes}{371}
\contentsline {chapter}{\numberline {11}Making Bayesian analysis objective?}{373}
\contentsline {section}{\numberline {11.1}Chapter mission statement}{373}
\contentsline {section}{\numberline {11.2}Chapter goals}{373}
\contentsline {section}{\numberline {11.3}The illusion of the 'uninformative' uniform prior}{374}
\contentsline {section}{\numberline {11.4}Jeffreys' priors}{375}
\contentsline {subsection}{\numberline {11.4.1}Another definition of 'uninformative'}{375}
\contentsline {subsection}{\numberline {11.4.2}Being critical}{379}
\contentsline {section}{\numberline {11.5}Reference priors}{380}
\contentsline {section}{\numberline {11.6}Empirical Bayes}{382}
\contentsline {section}{\numberline {11.7}A move towards weakly informative priors}{384}
\contentsline {section}{\numberline {11.8}Chapter summary}{385}
\contentsline {section}{\numberline {11.9}Chapter outcomes}{386}
\contentsline {part}{IV\hspace {1em}A practical guide to doing real life Bayesian analysis: Computational Bayes}{387}
\contentsline {section}{\numberline {11.10}Part mission statement}{389}
\contentsline {section}{\numberline {11.11}Part goals}{389}
\contentsline {chapter}{\numberline {12}Leaving conjugates behind: Markov Chain Monte Carlo}{391}
\contentsline {section}{\numberline {12.1}Chapter mission statement}{391}
\contentsline {section}{\numberline {12.2}Chapter goals}{391}
\contentsline {section}{\numberline {12.3}The difficulty with real life Bayesian inference}{393}
\contentsline {section}{\numberline {12.4}Discrete approximation to continuous posteriors}{394}
\contentsline {section}{\numberline {12.5}The posterior through quadrature}{397}
\contentsline {section}{\numberline {12.6}Integrating using independent samples: an introduction to Monte Carlo}{399}
\contentsline {subsection}{\numberline {12.6.1}BSE revisited}{402}
\contentsline {section}{\numberline {12.7}Why is independent sampling easier said than done?}{404}
\contentsline {section}{\numberline {12.8}Ideal sampling from a posterior using only the unnormalised posterior}{406}
\contentsline {subsection}{\numberline {12.8.1}The unnormalised posterior - a window onto the real deal}{409}
\contentsline {section}{\numberline {12.9}Moving from independent to dependent sampling}{410}
\contentsline {subsection}{\numberline {12.9.1}An analogy to MCMC: mapping mountains}{412}
\contentsline {section}{\numberline {12.10}What's the catch with dependent samplers?}{413}
\contentsline {section}{\numberline {12.11}Chapter summary}{417}
\contentsline {section}{\numberline {12.12}Chapter outcomes}{418}
\contentsline {section}{\numberline {12.13}Problem set}{418}
\contentsline {subsection}{\numberline {12.13.1}Prove that the inverse-transform sampler works!}{418}
\contentsline {subsection}{\numberline {12.13.2}Computationally investigate the die example}{418}
\contentsline {subsection}{\numberline {12.13.3}Evaluate the mean and variance of the posterior for the BSE example}{418}
\contentsline {chapter}{\numberline {13}The Metropolis algorithm}{419}
\contentsline {section}{\numberline {13.1}Chapter mission statement}{419}
\contentsline {section}{\numberline {13.2}Chapter goals}{419}
\contentsline {section}{\numberline {13.3}Sustainable fishing}{421}
\contentsline {section}{\numberline {13.4}Prospecting for gold}{422}
\contentsline {section}{\numberline {13.5}Defining the Metropolis algorithm}{425}
\contentsline {section}{\numberline {13.6}When does Metropolis work?}{429}
\contentsline {subsection}{\numberline {13.6.1}Mathematical underpinnings of MCMC}{431}
\contentsline {subsection}{\numberline {13.6.2}Desirable qualities of a Markov Chain}{432}
\contentsline {subsection}{\numberline {13.6.3}Detailed balance}{432}
\contentsline {subsection}{\numberline {13.6.4}The intuition behind the acceptance/rejection rule of Metropolis, and detailed balance}{435}
\contentsline {subsection}{\numberline {13.6.5}Proving things we know to be true: Mathematics revisited}{437}
\contentsline {section}{\numberline {13.7}Efficiency of convergence: the importance of choosing the right proposal scale}{438}
\contentsline {subsection}{\numberline {13.7.1}MCMC as finding and exploring the typical set\let \reserved@d =[\def \par }{442}
\contentsline {subsection}{\numberline {13.7.2}Speeding up convergence: tuning the proposal distribution}{443}
\contentsline {subsection}{\numberline {13.7.3}Geometric convergence of Markov Chains}{445}
\contentsline {section}{\numberline {13.8}Judging convergence}{446}
\contentsline {subsection}{\numberline {13.8.1}Bob's bees in a house\let \reserved@d =[\def \par }{446}
\contentsline {subsection}{\numberline {13.8.2}Using multiple chains to monitor convergence}{449}
\contentsline {subsection}{\numberline {13.8.3}Using within- and between-chain variation to estimate convergence}{452}
\contentsline {subsection}{\numberline {13.8.4}Two types of non-convergence}{454}
\contentsline {subsection}{\numberline {13.8.5}Warm-up}{455}
\contentsline {section}{\numberline {13.9}Effective sample size revisited}{457}
\contentsline {subsection}{\numberline {13.9.1}Thinning samples to increase effective sample size}{459}
\contentsline {section}{\numberline {13.10}Chapter summary}{460}
\contentsline {section}{\numberline {13.11}Chapter outcomes}{462}
\contentsline {section}{\numberline {13.12}Problem set}{463}
\contentsline {subsection}{\numberline {13.12.1}Metropolis-Hastings example}{463}
\contentsline {subsection}{\numberline {13.12.2}Find the optimal rejection probability = Gelman says 0.44 for unimodal}{463}
\contentsline {subsection}{\numberline {13.12.3}Adaptive MCMC}{463}
\contentsline {chapter}{\numberline {14}Gibbs sampling}{465}
\contentsline {section}{\numberline {14.1}Chapter mission statement}{465}
\contentsline {section}{\numberline {14.2}Chapter goals}{465}
\contentsline {section}{\numberline {14.3}Back to prospecting for gold}{467}
\contentsline {section}{\numberline {14.4}Defining the Gibbs algorithm}{469}
\contentsline {subsection}{\numberline {14.4.1}Crime and punishment/unemployment}{472}
\contentsline {section}{\numberline {14.5}Gibbs' earth: the intuition behind the Gibbs algorithm}{472}
\contentsline {section}{\numberline {14.6}The benefits and problems with Gibbs and Random Walk Metropolis}{474}
\contentsline {section}{\numberline {14.7}A change of parameters to speed up exploration}{476}
\contentsline {section}{\numberline {14.8}Chapter summary}{477}
\contentsline {section}{\numberline {14.9}Chapter outcomes}{478}
\contentsline {section}{\numberline {14.10}Problem set}{478}
\contentsline {subsection}{\numberline {14.10.1}Prove that the Gibbs sampler can be viewed as a case of Metropolis-Hastings}{478}
\contentsline {subsection}{\numberline {14.10.2}Prove that a Gibbs sampler on bivariate normal has an effective sample size of half its true sample size}{478}
\contentsline {section}{\numberline {14.11}Appendix}{478}
\contentsline {subsection}{\numberline {14.11.1}Derivation of conditional distributions for multivariate normal}{478}
\contentsline {subsection}{\numberline {14.11.2}Derive uncorrelated normals from bivariate normal}{478}
\contentsline {chapter}{\numberline {15}Hamiltonian Monte Carlo}{479}
\contentsline {section}{\numberline {15.1}Chapter mission statement}{479}
\contentsline {section}{\numberline {15.2}Chapter goals}{479}
\contentsline {section}{\numberline {15.3}Hamiltonian Monte Carlo as a sledge}{480}
\contentsline {subsection}{\numberline {15.3.1}Rewriting our problem in the language of Statistical Mechanics\let \reserved@d =[\def \par }{482}
\contentsline {subsection}{\numberline {15.3.2}Choosing the potential and kinetic energy terms}{485}
\contentsline {subsection}{\numberline {15.3.3}Simulating sledge movement in NLP space using the Leapfrog algorithm}{487}
\contentsline {subsection}{\numberline {15.3.4}Approximating our path in a way that conserves volume}{489}
\contentsline {subsection}{\numberline {15.3.5}Revising our acceptance rule}{490}
\contentsline {subsection}{\numberline {15.3.6}Putting it all together: the full HMC algorithm}{490}
\contentsline {subsection}{\numberline {15.3.7}Competing with Random Walk Metropolis and Gibbs}{490}
\contentsline {section}{\numberline {15.4}Avoiding manual labour: the No-U-turn sampler}{490}
\contentsline {section}{\numberline {15.5}Riemannian MCMC}{490}
\contentsline {section}{\numberline {15.6}Multimodality}{490}
\contentsline {chapter}{\numberline {16}Stan and JAGS}{491}
\contentsline {section}{\numberline {16.1}Motivation for Stan and JAGS}{491}
\contentsline {subsection}{\numberline {16.1.1}Their similarities and differences}{491}
\contentsline {subsection}{\numberline {16.1.2}The future}{491}
\contentsline {subsection}{\numberline {16.1.3}The nuts and bolts of Stan and JAGS: access through R}{491}
\contentsline {subsubsection}{How to get packages}{491}
\contentsline {subsubsection}{How to package data}{491}
\contentsline {section}{\numberline {16.2}Stan}{491}
\contentsline {subsection}{\numberline {16.2.1}How to download, and install}{491}
\contentsline {subsection}{\numberline {16.2.2}A first program in Stan}{491}
\contentsline {subsection}{\numberline {16.2.3}The building blocks of a Stan program}{492}
\contentsline {subsubsection}{Parameters}{492}
\contentsline {subsubsection}{Model}{492}
\contentsline {subsubsection}{Transformed parameters}{492}
\contentsline {subsubsection}{Transformed data}{492}
\contentsline {subsubsection}{Generated quantities}{492}
\contentsline {subsection}{\numberline {16.2.4}A simple model with data}{492}
\contentsline {subsection}{\numberline {16.2.5}Diagnostics}{492}
\contentsline {subsubsection}{$\mathaccentV {hat}05E{R}$ and effective sample size}{492}
\contentsline {subsubsection}{Divergent iterations}{492}
\contentsline {subsubsection}{Tree depth exceeding maximum}{492}
\contentsline {subsubsection}{Leapfrog steps}{492}
\contentsline {subsection}{\numberline {16.2.6}More complex models with array indexing}{492}
\contentsline {subsection}{\numberline {16.2.7}Essential Stan reading}{492}
\contentsline {subsubsection}{The importance of log probability incrementing}{493}
\contentsline {subsubsection}{Marginalising of discrete parameters}{493}
\contentsline {subsection}{\numberline {16.2.8}Calculating LOO-CV and WAIC}{493}
\contentsline {section}{\numberline {16.3}JAGS}{493}
\contentsline {subsection}{\numberline {16.3.1}How to download, and install}{493}
\contentsline {subsection}{\numberline {16.3.2}A first program in JAGS}{493}
\contentsline {subsection}{\numberline {16.3.3}The literal meaning of a for loop}{494}
\contentsline {subsection}{\numberline {16.3.4}A simple model}{494}
\contentsline {subsection}{\numberline {16.3.5}Diagnosis}{494}
\contentsline {subsubsection}{In-built using vanilla R}{494}
\contentsline {subsubsection}{coda}{494}
\contentsline {subsubsection}{More complex model}{494}
\contentsline {subsection}{\numberline {16.3.6}Essential JAGS reading}{494}
\contentsline {subsubsection}{Controlling the flow without if conditions}{494}
\contentsline {subsubsection}{Storing values in matrices}{494}
\contentsline {part}{V\hspace {1em}Regression analysis and hierarchical models}{495}
\contentsline {section}{\numberline {16.4}Part mission statement}{497}
\contentsline {section}{\numberline {16.5}Part goals}{497}
\contentsline {chapter}{\numberline {17}Hierarchical models}{499}
\contentsline {section}{\numberline {17.1}The spectrum from pooled to heterogeneous}{499}
\contentsline {subsection}{\numberline {17.1.1}The logic and benefits of partial pooling}{499}
\contentsline {subsection}{\numberline {17.1.2}Shrinkage towards the mean}{499}
\contentsline {section}{\numberline {17.2}Meta analysis example: simple}{499}
\contentsline {section}{\numberline {17.3}The importance of fake data simulation for complex models}{499}
\contentsline {subsection}{\numberline {17.3.1}The importance of making 'good' fake data}{499}
\contentsline {chapter}{\numberline {18}Linear regression models}{501}
\contentsline {section}{\numberline {18.1}Choosing covariates: model averaging}{501}
\contentsline {chapter}{\numberline {19}Generalised linear models}{503}
\contentsline {section}{\numberline {19.1}Malarial example of complex meta-analysis}{503}
\contentsline {section}{\numberline {19.2}Practical computational inference}{503}
\contentsline {subsection}{\numberline {19.2.1}The importance of pre-simulation MLE}{503}
\contentsline {subsection}{\numberline {19.2.2}Fake data simulation}{503}
\contentsline {subsection}{\numberline {19.2.3}Poor convergence}{503}
\contentsline {section}{\numberline {19.3}Posterior predictive checks}{503}
