\BOOKMARK [0][-]{chapter.1}{How to best use this book}{}% 1
\BOOKMARK [1][-]{section.1.1}{The purpose of this book}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Who is this book for?}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Pre-requisites}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.4}{Book outline}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.5}{Route planner - suggested journeys through Bayesland}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.6}{Video}{chapter.1}% 7
\BOOKMARK [1][-]{section.1.7}{Interactive elements}{chapter.1}% 8
\BOOKMARK [1][-]{section.1.8}{Interactive problem sets}{chapter.1}% 9
\BOOKMARK [1][-]{section.1.9}{Code}{chapter.1}% 10
\BOOKMARK [1][-]{section.1.10}{R, Stan and JAGS}{chapter.1}% 11
\BOOKMARK [1][-]{section.1.11}{Why don't more people use Bayesian statistics?}{chapter.1}% 12
\BOOKMARK [1][-]{section.1.12}{What are the tangible \(non-academic\) benefits of Bayesian statistics?}{chapter.1}% 13
\BOOKMARK [1][-]{section.1.13}{Suggested further reading}{chapter.1}% 14
\BOOKMARK [-1][-]{part.1}{I An introduction to Bayesian inference}{}% 15
\BOOKMARK [0][-]{section.1.14}{Part mission statement}{part.1}% 16
\BOOKMARK [1][-]{section.1.15}{Part goals}{section.1.14}% 17
\BOOKMARK [0][-]{chapter.2}{The subjective worlds of Frequentist and Bayesian statistics}{part.1}% 18
\BOOKMARK [1][-]{section.2.1}{Chapter mission statement}{chapter.2}% 19
\BOOKMARK [1][-]{section.2.2}{Chapter goals}{chapter.2}% 20
\BOOKMARK [1][-]{section.2.3}{Bayes' rule - allowing us to go from the effect back to its cause}{chapter.2}% 21
\BOOKMARK [1][-]{section.2.4}{The purpose of statistical inference}{chapter.2}% 22
\BOOKMARK [1][-]{section.2.5}{The world according to Frequentists}{chapter.2}% 23
\BOOKMARK [1][-]{section.2.6}{The world according to Bayesians}{chapter.2}% 24
\BOOKMARK [1][-]{section.2.7}{Frequentist and Bayesian inference}{chapter.2}% 25
\BOOKMARK [2][-]{subsection.2.7.1}{The Frequentist and Bayesian murder trials}{section.2.7}% 26
\BOOKMARK [2][-]{subsection.2.7.2}{Radio control towers: example}{section.2.7}% 27
\BOOKMARK [1][-]{section.2.8}{Bayesian inference via Bayes' rule}{chapter.2}% 28
\BOOKMARK [2][-]{subsection.2.8.1}{Likelihoods}{section.2.8}% 29
\BOOKMARK [2][-]{subsection.2.8.2}{Priors}{section.2.8}% 30
\BOOKMARK [2][-]{subsection.2.8.3}{The denominator}{section.2.8}% 31
\BOOKMARK [2][-]{subsection.2.8.4}{Posteriors: the goal of Bayesian inference}{section.2.8}% 32
\BOOKMARK [1][-]{section.2.9}{Implicit vs Explicit subjectivity}{chapter.2}% 33
\BOOKMARK [1][-]{section.2.10}{Chapter summary}{chapter.2}% 34
\BOOKMARK [1][-]{section.2.11}{Chapter outcomes}{chapter.2}% 35
\BOOKMARK [1][-]{section.2.12}{Problem set}{chapter.2}% 36
\BOOKMARK [2][-]{subsection.2.12.1}{The deterministic nature of random coin throwing.}{section.2.12}% 37
\BOOKMARK [2][-]{subsection.2.12.2}{Model choice}{section.2.12}% 38
\BOOKMARK [1][-]{section.2.13}{Appendix}{chapter.2}% 39
\BOOKMARK [2][-]{subsection.2.13.1}{The Frequentist and Bayesian murder trial}{section.2.13}% 40
\BOOKMARK [0][-]{chapter.3}{Probability - the nuts and bolts of Bayesian inference}{part.1}% 41
\BOOKMARK [1][-]{section.3.1}{Chapter mission statement}{chapter.3}% 42
\BOOKMARK [1][-]{section.3.2}{Chapter goals}{chapter.3}% 43
\BOOKMARK [1][-]{section.3.3}{Probability distributions: helping us explicitly state our ignorance}{chapter.3}% 44
\BOOKMARK [2][-]{subsection.3.3.1}{What makes a probability distribution valid?}{section.3.3}% 45
\BOOKMARK [2][-]{subsection.3.3.2}{Probabilities vs probability density : interpreting discrete and continuous probability distributions}{section.3.3}% 46
\BOOKMARK [2][-]{subsection.3.3.3}{Mean and variance of distributions}{section.3.3}% 47
\BOOKMARK [2][-]{subsection.3.3.4}{Generalising probability distributions to two dimensions}{section.3.3}% 48
\BOOKMARK [2][-]{subsection.3.3.5}{Marginal distributions}{section.3.3}% 49
\BOOKMARK [2][-]{subsection.3.3.6}{Conditional distributions}{section.3.3}% 50
\BOOKMARK [1][-]{section.3.4}{Higher dimensional probability densities: no harder than 2-D, just looks it!}{chapter.3}% 51
\BOOKMARK [1][-]{section.3.5}{Independence}{chapter.3}% 52
\BOOKMARK [2][-]{subsection.3.5.1}{Conditional independence}{section.3.5}% 53
\BOOKMARK [1][-]{section.3.6}{Central Limit Theorems}{chapter.3}% 54
\BOOKMARK [1][-]{section.3.7}{The Bayesian formula}{chapter.3}% 55
\BOOKMARK [2][-]{subsection.3.7.1}{The intuition behind the formula}{section.3.7}% 56
\BOOKMARK [2][-]{subsection.3.7.2}{Breast cancer screeing}{section.3.7}% 57
\BOOKMARK [1][-]{section.3.8}{The Bayesian inference process from the Bayesian formula}{chapter.3}% 58
\BOOKMARK [1][-]{section.3.9}{Chapter summary}{chapter.3}% 59
\BOOKMARK [1][-]{section.3.10}{Chapter outcomes}{chapter.3}% 60
\BOOKMARK [1][-]{section.3.11}{Problem set}{chapter.3}% 61
\BOOKMARK [2][-]{subsection.3.11.1}{The expected returns of a derivative}{section.3.11}% 62
\BOOKMARK [2][-]{subsection.3.11.2}{The boy or girl paradox}{section.3.11}% 63
\BOOKMARK [2][-]{subsection.3.11.3}{The Bayesian game show}{section.3.11}% 64
\BOOKMARK [2][-]{subsection.3.11.4}{Blood doping}{section.3.11}% 65
\BOOKMARK [-1][-]{part.2}{II Understanding the Bayesian formula}{}% 66
\BOOKMARK [0][-]{section.3.12}{Part mission statement}{part.2}% 67
\BOOKMARK [1][-]{section.3.13}{Part goals}{section.3.12}% 68
\BOOKMARK [0][-]{chapter.4}{The posterior - the goal of Bayesian inference}{part.2}% 69
\BOOKMARK [1][-]{section.4.1}{Chapter Mission statement}{chapter.4}% 70
\BOOKMARK [1][-]{section.4.2}{Chapter goals}{chapter.4}% 71
\BOOKMARK [1][-]{section.4.3}{Expressing uncertainty through the posterior probability distribution}{chapter.4}% 72
\BOOKMARK [2][-]{subsection.4.3.1}{Bayesian coastguard: introducing the prior and the posterior}{section.4.3}% 73
\BOOKMARK [2][-]{subsection.4.3.2}{Bayesian statistics: updating our pre-analysis uncertainty}{section.4.3}% 74
\BOOKMARK [2][-]{subsection.4.3.3}{Do parameters actually exist and have a point value?}{section.4.3}% 75
\BOOKMARK [2][-]{subsection.4.3.4}{Failings of the Frequentist confidence interval}{section.4.3}% 76
\BOOKMARK [2][-]{subsection.4.3.5}{Credible intervals}{section.4.3}% 77
\BOOKMARK [2][-]{subsection.4.3.6}{Reconciling the difference between confidence and credible intervals}{section.4.3}% 78
\BOOKMARK [1][-]{section.4.4}{Point parameter estimates}{chapter.4}% 79
\BOOKMARK [1][-]{section.4.5}{Prediction using predictive distributions}{chapter.4}% 80
\BOOKMARK [2][-]{subsection.4.5.1}{Example: number of Republican voters within a sample}{section.4.5}% 81
\BOOKMARK [2][-]{subsection.4.5.2}{Example: interest rate hedging}{section.4.5}% 82
\BOOKMARK [1][-]{section.4.6}{Model comparison using the posterior}{chapter.4}% 83
\BOOKMARK [2][-]{subsection.4.6.1}{Example: epidemiologist comparison}{section.4.6}% 84
\BOOKMARK [2][-]{subsection.4.6.2}{Example: customer footfall}{section.4.6}% 85
\BOOKMARK [1][-]{section.4.7}{Model comparison through posterior predictive checks}{chapter.4}% 86
\BOOKMARK [2][-]{subsection.4.7.1}{Example: stock returns}{section.4.7}% 87
\BOOKMARK [1][-]{section.4.8}{Chapter summary}{chapter.4}% 88
\BOOKMARK [1][-]{section.4.9}{Chapter outcomes}{chapter.4}% 89
\BOOKMARK [1][-]{section.4.10}{Problem set}{chapter.4}% 90
\BOOKMARK [2][-]{subsection.4.10.1}{The lesser evil}{section.4.10}% 91
\BOOKMARK [2][-]{subsection.4.10.2}{Google word search prediction}{section.4.10}% 92
\BOOKMARK [2][-]{subsection.4.10.3}{Prior and posterior predictive example \(with PPCs maybe\)}{section.4.10}% 93
\BOOKMARK [1][-]{section.4.11}{Appendix}{chapter.4}% 94
\BOOKMARK [2][-]{subsection.4.11.1}{The interval ENIGMA - explained in full}{section.4.11}% 95
\BOOKMARK [0][-]{chapter.5}{Likelihoods}{part.2}% 96
\BOOKMARK [1][-]{section.5.1}{Chapter Mission statement}{chapter.5}% 97
\BOOKMARK [1][-]{section.5.2}{Chapter goals}{chapter.5}% 98
\BOOKMARK [1][-]{section.5.3}{What is a likelihood?}{chapter.5}% 99
\BOOKMARK [1][-]{section.5.4}{Why use 'likelihood' rather than 'probability'?}{chapter.5}% 100
\BOOKMARK [1][-]{section.5.5}{What are models and why do we need them?}{chapter.5}% 101
\BOOKMARK [1][-]{section.5.6}{How to choose an appropriate likelihood?}{chapter.5}% 102
\BOOKMARK [2][-]{subsection.5.6.1}{A likelihood model for an individual's disease status}{section.5.6}% 103
\BOOKMARK [2][-]{subsection.5.6.2}{A likelihood model for disease prevalence of a group}{section.5.6}% 104
\BOOKMARK [2][-]{subsection.5.6.3}{The intelligence of a group of people}{section.5.6}% 105
\BOOKMARK [1][-]{section.5.7}{Exchangeability vs random sampling}{chapter.5}% 106
\BOOKMARK [1][-]{section.5.8}{The subjectivity of model choice}{chapter.5}% 107
\BOOKMARK [1][-]{section.5.9}{Maximum likelihood - a short introduction}{chapter.5}% 108
\BOOKMARK [2][-]{subsection.5.9.1}{Estimating disease prevalence}{section.5.9}% 109
\BOOKMARK [2][-]{subsection.5.9.2}{Estimating the mean and variance in intelligence scores}{section.5.9}% 110
\BOOKMARK [2][-]{subsection.5.9.3}{Maximum likelihood in simple steps}{section.5.9}% 111
\BOOKMARK [1][-]{section.5.10}{Frequentist inference in Maximum Likelihood}{chapter.5}% 112
\BOOKMARK [1][-]{section.5.11}{Chapter summary}{chapter.5}% 113
\BOOKMARK [1][-]{section.5.12}{Chapter outcomes}{chapter.5}% 114
\BOOKMARK [1][-]{section.5.13}{Problem set}{chapter.5}% 115
\BOOKMARK [2][-]{subsection.5.13.1}{Blog blues.}{section.5.13}% 116
\BOOKMARK [2][-]{subsection.5.13.2}{Violent crime counts in New York counties}{section.5.13}% 117
\BOOKMARK [2][-]{subsection.5.13.3}{Monte Carlo evaluation of the performance of MLE in R}{section.5.13}% 118
\BOOKMARK [2][-]{subsection.5.13.4}{The sample mean as MLE}{section.5.13}% 119
\BOOKMARK [0][-]{chapter.6}{Priors}{part.2}% 120
\BOOKMARK [1][-]{section.6.1}{Chapter Mission statement}{chapter.6}% 121
\BOOKMARK [1][-]{section.6.2}{Chapter goals}{chapter.6}% 122
\BOOKMARK [1][-]{section.6.3}{What are priors, and what do they represent?}{chapter.6}% 123
\BOOKMARK [1][-]{section.6.4}{Why do we need priors at all?}{chapter.6}% 124
\BOOKMARK [1][-]{section.6.5}{Why don't we just normalise likelihood by choosing a unity prior?}{chapter.6}% 125
\BOOKMARK [1][-]{section.6.6}{The explicit subjectivity of priors}{chapter.6}% 126
\BOOKMARK [1][-]{section.6.7}{Combining a prior and likelihood to form a posterior}{chapter.6}% 127
\BOOKMARK [2][-]{subsection.6.7.1}{The Goldfish game}{section.6.7}% 128
\BOOKMARK [2][-]{subsection.6.7.2}{Disease proportions revisited}{section.6.7}% 129
\BOOKMARK [1][-]{section.6.8}{Constructing priors}{chapter.6}% 130
\BOOKMARK [2][-]{subsection.6.8.1}{Vague priors}{section.6.8}% 131
\BOOKMARK [2][-]{subsection.6.8.2}{Informative priors}{section.6.8}% 132
\BOOKMARK [2][-]{subsection.6.8.3}{The numerator of Bayes' rule determines the shape}{section.6.8}% 133
\BOOKMARK [2][-]{subsection.6.8.4}{Eliciting priors}{section.6.8}% 134
\BOOKMARK [1][-]{section.6.9}{A strong model is less sensitive to prior choice}{chapter.6}% 135
\BOOKMARK [2][-]{subsection.6.9.1}{Caveat: overly-zealous priors will affect the posterior}{section.6.9}% 136
\BOOKMARK [1][-]{section.6.10}{Chapter summary}{chapter.6}% 137
\BOOKMARK [1][-]{section.6.11}{Chapter outcomes}{chapter.6}% 138
\BOOKMARK [1][-]{section.6.12}{Problem set}{chapter.6}% 139
\BOOKMARK [2][-]{subsection.6.12.1}{Counting sheep}{section.6.12}% 140
\BOOKMARK [2][-]{subsection.6.12.2}{Investigating priors through US elections}{section.6.12}% 141
\BOOKMARK [2][-]{subsection.6.12.3}{Choosing prior distributions.}{section.6.12}% 142
\BOOKMARK [2][-]{subsection.6.12.4}{Expert data prior example}{section.6.12}% 143
\BOOKMARK [2][-]{subsection.6.12.5}{Data analysis example showing the declining importance of prior as data set increases in size}{section.6.12}% 144
\BOOKMARK [1][-]{section.6.13}{Appendix}{chapter.6}% 145
\BOOKMARK [2][-]{subsection.6.13.1}{Bayes' rule for the urn}{section.6.13}% 146
\BOOKMARK [2][-]{subsection.6.13.2}{The probabilities of having a disease}{section.6.13}% 147
\BOOKMARK [0][-]{chapter.7}{The devil's in the denominator}{part.2}% 148
\BOOKMARK [1][-]{section.7.1}{Chapter mission}{chapter.7}% 149
\BOOKMARK [1][-]{section.7.2}{Chapter goals}{chapter.7}% 150
\BOOKMARK [1][-]{section.7.3}{An introduction to the denominator}{chapter.7}% 151
\BOOKMARK [2][-]{subsection.7.3.1}{The denominator as a normalising factor}{section.7.3}% 152
\BOOKMARK [2][-]{subsection.7.3.2}{Example: disease}{section.7.3}% 153
\BOOKMARK [2][-]{subsection.7.3.3}{Example: the proportion of people who vote the Conservatives}{section.7.3}% 154
\BOOKMARK [2][-]{subsection.7.3.4}{The denominator as a probability}{section.7.3}% 155
\BOOKMARK [2][-]{subsection.7.3.5}{Using the denominator to choose between competing models}{section.7.3}% 156
\BOOKMARK [1][-]{section.7.4}{The difficulty with the denominator}{chapter.7}% 157
\BOOKMARK [2][-]{subsection.7.4.1}{Multi-parameter discrete model example: the comorbidity between depression and anxiety}{section.7.4}% 158
\BOOKMARK [2][-]{subsection.7.4.2}{Continuous multi-parameter example: mean and variance of IQ}{section.7.4}% 159
\BOOKMARK [1][-]{section.7.5}{How to dispense with the difficulty: Bayesian computation}{chapter.7}% 160
\BOOKMARK [1][-]{section.7.6}{Chapter summary}{chapter.7}% 161
\BOOKMARK [1][-]{section.7.7}{Chapter outcomes}{chapter.7}% 162
\BOOKMARK [1][-]{section.7.8}{Problem set}{chapter.7}% 163
\BOOKMARK [2][-]{subsection.7.8.1}{New disease cases}{section.7.8}% 164
\BOOKMARK [2][-]{subsection.7.8.2}{The comorbidity between depression, anxiety and psychosis}{section.7.8}% 165
\BOOKMARK [2][-]{subsection.7.8.3}{Finding mosquito larvae after rain}{section.7.8}% 166
\BOOKMARK [1][-]{section.7.9}{Appendix}{chapter.7}% 167
\BOOKMARK [-1][-]{part.3}{III Analytic Bayesian methods}{}% 168
\BOOKMARK [0][-]{section.7.10}{Part mission statement}{part.3}% 169
\BOOKMARK [1][-]{section.7.11}{Part goals}{section.7.10}% 170
\BOOKMARK [0][-]{chapter.8}{An introduction to distributions for the mathematically-un-inclined}{part.3}% 171
\BOOKMARK [1][-]{section.8.1}{Chapter mission statement}{chapter.8}% 172
\BOOKMARK [1][-]{section.8.2}{Chapter goals}{chapter.8}% 173
\BOOKMARK [1][-]{section.8.3}{The interrelation among distributions}{chapter.8}% 174
\BOOKMARK [1][-]{section.8.4}{Sampling distributions for likelihoods}{chapter.8}% 175
\BOOKMARK [2][-]{subsection.8.4.1}{Bernoulli}{section.8.4}% 176
\BOOKMARK [2][-]{subsection.8.4.2}{Binomial}{section.8.4}% 177
\BOOKMARK [2][-]{subsection.8.4.3}{Poisson}{section.8.4}% 178
\BOOKMARK [2][-]{subsection.8.4.4}{Negative binomial}{section.8.4}% 179
\BOOKMARK [2][-]{subsection.8.4.5}{Beta-binomial}{section.8.4}% 180
\BOOKMARK [2][-]{subsection.8.4.6}{Normal}{section.8.4}% 181
\BOOKMARK [2][-]{subsection.8.4.7}{Student t}{section.8.4}% 182
\BOOKMARK [2][-]{subsection.8.4.8}{Exponential}{section.8.4}% 183
\BOOKMARK [2][-]{subsection.8.4.9}{Gamma}{section.8.4}% 184
\BOOKMARK [2][-]{subsection.8.4.10}{Multinomial}{section.8.4}% 185
\BOOKMARK [2][-]{subsection.8.4.11}{Multivariate normal and multivariate t}{section.8.4}% 186
\BOOKMARK [1][-]{section.8.5}{Prior distributions}{chapter.8}% 187
\BOOKMARK [2][-]{subsection.8.5.1}{Distributions for probabilities, proportions and percentages}{section.8.5}% 188
\BOOKMARK [2][-]{subsection.8.5.2}{Distributions for means and regression coefficients}{section.8.5}% 189
\BOOKMARK [2][-]{subsection.8.5.3}{Distributions for non-negative parameters}{section.8.5}% 190
\BOOKMARK [2][-]{subsection.8.5.4}{Distributions for covariance and correlation matrices}{section.8.5}% 191
\BOOKMARK [1][-]{section.8.6}{Choosing a likelihood made easy}{chapter.8}% 192
\BOOKMARK [1][-]{section.8.7}{Table of common likelihoods, their uses, and reasonable priors}{chapter.8}% 193
\BOOKMARK [1][-]{section.8.8}{Distributions of distributions, and mixtures - link to website, and relevance}{chapter.8}% 194
\BOOKMARK [1][-]{section.8.9}{Chapter summary}{chapter.8}% 195
\BOOKMARK [1][-]{section.8.10}{Chapter outcomes}{chapter.8}% 196
\BOOKMARK [1][-]{section.8.11}{Problem set}{chapter.8}% 197
\BOOKMARK [2][-]{subsection.8.11.1}{Drug trials}{section.8.11}% 198
\BOOKMARK [2][-]{subsection.8.11.2}{100m results across countries}{section.8.11}% 199
\BOOKMARK [2][-]{subsection.8.11.3}{Triangular representation of simplexes}{section.8.11}% 200
\BOOKMARK [2][-]{subsection.8.11.4}{Normal distribution with normal prior}{section.8.11}% 201
\BOOKMARK [0][-]{chapter.9}{Conjugate priors and their place in Bayesian analysis}{part.3}% 202
\BOOKMARK [1][-]{section.9.1}{Chapter mission statement}{chapter.9}% 203
\BOOKMARK [1][-]{section.9.2}{Chapter goals}{chapter.9}% 204
\BOOKMARK [1][-]{section.9.3}{What is a conjugate prior and why are they useful?}{chapter.9}% 205
\BOOKMARK [1][-]{section.9.4}{Gamma-poisson example}{chapter.9}% 206
\BOOKMARK [1][-]{section.9.5}{Normal example: extra}{chapter.9}% 207
\BOOKMARK [1][-]{section.9.6}{Table of conjugate priors}{chapter.9}% 208
\BOOKMARK [1][-]{section.9.7}{The lessons and limits of a conjugate analysis}{chapter.9}% 209
\BOOKMARK [1][-]{section.9.8}{Chapter summary}{chapter.9}% 210
\BOOKMARK [1][-]{section.9.9}{Chapter outcomes}{chapter.9}% 211
\BOOKMARK [0][-]{chapter.10}{Evaluation of model fit and hypothesis testing}{part.3}% 212
\BOOKMARK [1][-]{section.10.1}{Chapter mission statement}{chapter.10}% 213
\BOOKMARK [1][-]{section.10.2}{Chapter goals}{chapter.10}% 214
\BOOKMARK [1][-]{section.10.3}{Posterior predictive checks}{chapter.10}% 215
\BOOKMARK [2][-]{subsection.10.3.1}{Recap - posterior predictive distributions}{section.10.3}% 216
\BOOKMARK [2][-]{subsection.10.3.2}{Graphical PPCs and Bayesian p values}{section.10.3}% 217
\BOOKMARK [1][-]{section.10.4}{Why do we call it a p value?}{chapter.10}% 218
\BOOKMARK [1][-]{section.10.5}{Statistics measuring predictive accuracy: AIC, Deviance, WAIC and LOO}{chapter.10}% 219
\BOOKMARK [2][-]{subsection.10.5.1}{Independent evaluation of a model, and overfitting}{section.10.5}% 220
\BOOKMARK [2][-]{subsection.10.5.2}{How to measure a model's predictive capability?}{section.10.5}% 221
\BOOKMARK [2][-]{subsection.10.5.3}{The ideal measure of a model's predictive accuracy}{section.10.5}% 222
\BOOKMARK [2][-]{subsection.10.5.4}{Estimating out-of-sample predictive accuracy from in-sample data}{section.10.5}% 223
\BOOKMARK [2][-]{subsection.10.5.5}{AIC}{section.10.5}% 224
\BOOKMARK [2][-]{subsection.10.5.6}{DIC}{section.10.5}% 225
\BOOKMARK [2][-]{subsection.10.5.7}{WAIC}{section.10.5}% 226
\BOOKMARK [2][-]{subsection.10.5.8}{LOO}{section.10.5}% 227
\BOOKMARK [2][-]{subsection.10.5.9}{A practical and short summary of measures of predictive accuracy in simple terms}{section.10.5}% 228
\BOOKMARK [1][-]{section.10.6}{Choosing one model, or a number?}{chapter.10}% 229
\BOOKMARK [1][-]{section.10.7}{Sensitivity analysis}{chapter.10}% 230
\BOOKMARK [1][-]{section.10.8}{Chapter summary}{chapter.10}% 231
\BOOKMARK [1][-]{section.10.9}{Chapter outcomes}{chapter.10}% 232
\BOOKMARK [0][-]{chapter.11}{Making Bayesian analysis objective?}{part.3}% 233
\BOOKMARK [1][-]{section.11.1}{Chapter mission statement}{chapter.11}% 234
\BOOKMARK [1][-]{section.11.2}{Chapter goals}{chapter.11}% 235
\BOOKMARK [1][-]{section.11.3}{The illusion of the 'uninformative' uniform prior}{chapter.11}% 236
\BOOKMARK [1][-]{section.11.4}{Jeffreys' priors}{chapter.11}% 237
\BOOKMARK [2][-]{subsection.11.4.1}{Another definition of 'uninformative'}{section.11.4}% 238
\BOOKMARK [2][-]{subsection.11.4.2}{Being critical}{section.11.4}% 239
\BOOKMARK [1][-]{section.11.5}{Reference priors}{chapter.11}% 240
\BOOKMARK [1][-]{section.11.6}{Empirical Bayes}{chapter.11}% 241
\BOOKMARK [1][-]{section.11.7}{A move towards weakly informative priors}{chapter.11}% 242
\BOOKMARK [1][-]{section.11.8}{Chapter summary}{chapter.11}% 243
\BOOKMARK [1][-]{section.11.9}{Chapter outcomes}{chapter.11}% 244
\BOOKMARK [-1][-]{part.4}{IV A practical guide to doing real life Bayesian analysis: Computational Bayes}{}% 245
\BOOKMARK [0][-]{section.11.10}{Part mission statement}{part.4}% 246
\BOOKMARK [1][-]{section.11.11}{Part goals}{section.11.10}% 247
\BOOKMARK [0][-]{chapter.12}{Leaving conjugates behind: Markov Chain Monte Carlo}{part.4}% 248
\BOOKMARK [1][-]{section.12.1}{Chapter mission statement}{chapter.12}% 249
\BOOKMARK [1][-]{section.12.2}{Chapter goals}{chapter.12}% 250
\BOOKMARK [1][-]{section.12.3}{The difficulty with real life Bayesian inference}{chapter.12}% 251
\BOOKMARK [1][-]{section.12.4}{Discrete approximation to continuous posteriors}{chapter.12}% 252
\BOOKMARK [1][-]{section.12.5}{The posterior through quadrature}{chapter.12}% 253
\BOOKMARK [1][-]{section.12.6}{Integrating using independent samples: an introduction to Monte Carlo}{chapter.12}% 254
\BOOKMARK [2][-]{subsection.12.6.1}{BSE revisited}{section.12.6}% 255
\BOOKMARK [1][-]{section.12.7}{Why is independent sampling easier said than done?}{chapter.12}% 256
\BOOKMARK [1][-]{section.12.8}{Ideal sampling from a posterior using only the unnormalised posterior}{chapter.12}% 257
\BOOKMARK [2][-]{subsection.12.8.1}{The unnormalised posterior - a window onto the real deal}{section.12.8}% 258
\BOOKMARK [1][-]{section.12.9}{Moving from independent to dependent sampling}{chapter.12}% 259
\BOOKMARK [2][-]{subsection.12.9.1}{An analogy to MCMC: mapping mountains}{section.12.9}% 260
\BOOKMARK [1][-]{section.12.10}{What's the catch with dependent samplers?}{chapter.12}% 261
\BOOKMARK [1][-]{section.12.11}{Chapter summary}{chapter.12}% 262
\BOOKMARK [1][-]{section.12.12}{Chapter outcomes}{chapter.12}% 263
\BOOKMARK [1][-]{section.12.13}{Problem set}{chapter.12}% 264
\BOOKMARK [2][-]{subsection.12.13.1}{Prove that the inverse-transform sampler works!}{section.12.13}% 265
\BOOKMARK [2][-]{subsection.12.13.2}{Computationally investigate the die example}{section.12.13}% 266
\BOOKMARK [2][-]{subsection.12.13.3}{Evaluate the mean and variance of the posterior for the BSE example}{section.12.13}% 267
\BOOKMARK [0][-]{chapter.13}{The Metropolis algorithm}{part.4}% 268
\BOOKMARK [1][-]{section.13.1}{Chapter mission statement}{chapter.13}% 269
\BOOKMARK [1][-]{section.13.2}{Chapter goals}{chapter.13}% 270
\BOOKMARK [1][-]{section.13.3}{Sustainable fishing}{chapter.13}% 271
\BOOKMARK [1][-]{section.13.4}{Prospecting for gold}{chapter.13}% 272
\BOOKMARK [1][-]{section.13.5}{Defining the Metropolis algorithm}{chapter.13}% 273
\BOOKMARK [1][-]{section.13.6}{When does Metropolis work?}{chapter.13}% 274
\BOOKMARK [2][-]{subsection.13.6.1}{Mathematical underpinnings of MCMC}{section.13.6}% 275
\BOOKMARK [2][-]{subsection.13.6.2}{Desirable qualities of a Markov Chain}{section.13.6}% 276
\BOOKMARK [2][-]{subsection.13.6.3}{Detailed balance}{section.13.6}% 277
\BOOKMARK [2][-]{subsection.13.6.4}{The intuition behind the acceptance/rejection rule of Metropolis, and detailed balance}{section.13.6}% 278
\BOOKMARK [2][-]{subsection.13.6.5}{Proving things we know to be true: Mathematics revisited}{section.13.6}% 279
\BOOKMARK [1][-]{section.13.7}{Efficiency of convergence: the importance of choosing the right proposal scale}{chapter.13}% 280
\BOOKMARK [2][-]{subsection.13.7.1}{MCMC as finding and exploring the typical set}{section.13.7}% 281
\BOOKMARK [2][-]{subsection.13.7.2}{Speeding up convergence: tuning the proposal distribution}{section.13.7}% 282
\BOOKMARK [2][-]{subsection.13.7.3}{Geometric convergence of Markov Chains}{section.13.7}% 283
\BOOKMARK [1][-]{section.13.8}{Judging convergence}{chapter.13}% 284
\BOOKMARK [2][-]{subsection.13.8.1}{Bob's bees in a house}{section.13.8}% 285
\BOOKMARK [2][-]{subsection.13.8.2}{Using multiple chains to monitor convergence}{section.13.8}% 286
\BOOKMARK [2][-]{subsection.13.8.3}{Using within- and between-chain variation to estimate convergence}{section.13.8}% 287
\BOOKMARK [2][-]{subsection.13.8.4}{Two types of non-convergence}{section.13.8}% 288
\BOOKMARK [2][-]{subsection.13.8.5}{Warm-up}{section.13.8}% 289
\BOOKMARK [1][-]{section.13.9}{Effective sample size revisited}{chapter.13}% 290
\BOOKMARK [2][-]{subsection.13.9.1}{Thinning samples to increase effective sample size}{section.13.9}% 291
\BOOKMARK [1][-]{section.13.10}{Chapter summary}{chapter.13}% 292
\BOOKMARK [1][-]{section.13.11}{Chapter outcomes}{chapter.13}% 293
\BOOKMARK [1][-]{section.13.12}{Problem set}{chapter.13}% 294
\BOOKMARK [2][-]{subsection.13.12.1}{Metropolis-Hastings example}{section.13.12}% 295
\BOOKMARK [2][-]{subsection.13.12.2}{Find the optimal rejection probability = Gelman says 0.44 for unimodal}{section.13.12}% 296
\BOOKMARK [2][-]{subsection.13.12.3}{Adaptive MCMC}{section.13.12}% 297
\BOOKMARK [0][-]{chapter.14}{Gibbs sampling}{part.4}% 298
\BOOKMARK [1][-]{section.14.1}{Chapter mission statement}{chapter.14}% 299
\BOOKMARK [1][-]{section.14.2}{Chapter goals}{chapter.14}% 300
\BOOKMARK [1][-]{section.14.3}{Back to prospecting for gold}{chapter.14}% 301
\BOOKMARK [1][-]{section.14.4}{Defining the Gibbs algorithm}{chapter.14}% 302
\BOOKMARK [2][-]{subsection.14.4.1}{Crime and punishment/unemployment}{section.14.4}% 303
\BOOKMARK [1][-]{section.14.5}{Gibbs' earth: the intuition behind the Gibbs algorithm}{chapter.14}% 304
\BOOKMARK [1][-]{section.14.6}{The benefits and problems with Gibbs and Random Walk Metropolis}{chapter.14}% 305
\BOOKMARK [1][-]{section.14.7}{A change of parameters to speed up exploration}{chapter.14}% 306
\BOOKMARK [1][-]{section.14.8}{Chapter summary}{chapter.14}% 307
\BOOKMARK [1][-]{section.14.9}{Chapter outcomes}{chapter.14}% 308
\BOOKMARK [1][-]{section.14.10}{Problem set}{chapter.14}% 309
\BOOKMARK [2][-]{subsection.14.10.1}{Prove that the Gibbs sampler can be viewed as a case of Metropolis-Hastings}{section.14.10}% 310
\BOOKMARK [2][-]{subsection.14.10.2}{Prove that a Gibbs sampler on bivariate normal has an effective sample size of half its true sample size}{section.14.10}% 311
\BOOKMARK [1][-]{section.14.11}{Appendix}{chapter.14}% 312
\BOOKMARK [2][-]{subsection.14.11.1}{Derivation of conditional distributions for multivariate normal}{section.14.11}% 313
\BOOKMARK [2][-]{subsection.14.11.2}{Derive uncorrelated normals from bivariate normal}{section.14.11}% 314
\BOOKMARK [0][-]{chapter.15}{Hamiltonian Monte Carlo}{part.4}% 315
\BOOKMARK [1][-]{section.15.1}{Chapter mission statement}{chapter.15}% 316
\BOOKMARK [1][-]{section.15.2}{Chapter goals}{chapter.15}% 317
\BOOKMARK [1][-]{section.15.3}{Hamiltonian Monte Carlo as a sledge}{chapter.15}% 318
\BOOKMARK [2][-]{subsection.15.3.1}{Rewriting our problem in the language of Statistical Mechanics}{section.15.3}% 319
\BOOKMARK [2][-]{subsection.15.3.2}{Choosing the potential and kinetic energy terms}{section.15.3}% 320
\BOOKMARK [2][-]{subsection.15.3.3}{Simulating sledge movement in NLP space using the Leapfrog algorithm}{section.15.3}% 321
\BOOKMARK [2][-]{subsection.15.3.4}{Approximating our path in a way that conserves volume}{section.15.3}% 322
\BOOKMARK [2][-]{subsection.15.3.5}{Revising our acceptance rule}{section.15.3}% 323
\BOOKMARK [2][-]{subsection.15.3.6}{Putting it all together: the full HMC algorithm}{section.15.3}% 324
\BOOKMARK [2][-]{subsection.15.3.7}{Competing with Random Walk Metropolis and Gibbs}{section.15.3}% 325
\BOOKMARK [1][-]{section.15.4}{Avoiding manual labour: the No-U-turn sampler}{chapter.15}% 326
\BOOKMARK [1][-]{section.15.5}{Riemannian MCMC}{chapter.15}% 327
\BOOKMARK [1][-]{section.15.6}{Multimodality}{chapter.15}% 328
\BOOKMARK [0][-]{chapter.16}{Stan}{part.4}% 329
\BOOKMARK [1][-]{section.16.1}{Chapter mission statement}{chapter.16}% 330
\BOOKMARK [1][-]{section.16.2}{Chapter goals}{chapter.16}% 331
\BOOKMARK [1][-]{section.16.3}{Why Stan, and how to get it.}{chapter.16}% 332
\BOOKMARK [2][-]{subsection.16.3.1}{When to use black-box MCMC rather than coding up the algorithms yourself}{section.16.3}% 333
\BOOKMARK [2][-]{subsection.16.3.2}{What is Stan?}{section.16.3}% 334
\BOOKMARK [2][-]{subsection.16.3.3}{Why choose Stan?}{section.16.3}% 335
\BOOKMARK [1][-]{section.16.4}{Getting setup with Stan using RStan}{chapter.16}% 336
\BOOKMARK [1][-]{section.16.5}{Our first words in Stan}{chapter.16}% 337
\BOOKMARK [2][-]{subsection.16.5.1}{The building blocks of a Stan program}{section.16.5}% 338
\BOOKMARK [2][-]{subsection.16.5.2}{Diagnostics}{section.16.5}% 339
\BOOKMARK [2][-]{subsection.16.5.3}{More complex models with array indexing}{section.16.5}% 340
\BOOKMARK [2][-]{subsection.16.5.4}{Essential Stan reading}{section.16.5}% 341
\BOOKMARK [1][-]{section.16.6}{What to do when things go wrong}{chapter.16}% 342
\BOOKMARK [-1][-]{part.5}{V Regression analysis and hierarchical models}{}% 343
\BOOKMARK [0][-]{section.16.7}{Part mission statement}{part.5}% 344
\BOOKMARK [1][-]{section.16.8}{Part goals}{section.16.7}% 345
\BOOKMARK [0][-]{chapter.17}{Hierarchical models}{part.5}% 346
\BOOKMARK [1][-]{section.17.1}{The spectrum from pooled to heterogeneous}{chapter.17}% 347
\BOOKMARK [2][-]{subsection.17.1.1}{The logic and benefits of partial pooling}{section.17.1}% 348
\BOOKMARK [2][-]{subsection.17.1.2}{Shrinkage towards the mean}{section.17.1}% 349
\BOOKMARK [1][-]{section.17.2}{Meta analysis example: simple}{chapter.17}% 350
\BOOKMARK [1][-]{section.17.3}{The importance of fake data simulation for complex models}{chapter.17}% 351
\BOOKMARK [2][-]{subsection.17.3.1}{The importance of making 'good' fake data}{section.17.3}% 352
\BOOKMARK [0][-]{chapter.18}{Linear regression models}{part.5}% 353
\BOOKMARK [1][-]{section.18.1}{Choosing covariates: model averaging}{chapter.18}% 354
\BOOKMARK [0][-]{chapter.19}{Generalised linear models}{part.5}% 355
\BOOKMARK [1][-]{section.19.1}{Malarial example of complex meta-analysis}{chapter.19}% 356
\BOOKMARK [1][-]{section.19.2}{Practical computational inference}{chapter.19}% 357
\BOOKMARK [2][-]{subsection.19.2.1}{The importance of pre-simulation MLE}{section.19.2}% 358
\BOOKMARK [2][-]{subsection.19.2.2}{Fake data simulation}{section.19.2}% 359
\BOOKMARK [2][-]{subsection.19.2.3}{Poor convergence}{section.19.2}% 360
\BOOKMARK [1][-]{section.19.3}{Posterior predictive checks}{chapter.19}% 361
