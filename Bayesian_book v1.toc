\contentsline {chapter}{\numberline {1}How to best use this book}{13}
\contentsline {section}{\numberline {1.1}The purpose of this book}{13}
\contentsline {section}{\numberline {1.2}Who is this book for?}{15}
\contentsline {section}{\numberline {1.3}Pre-requisites}{15}
\contentsline {section}{\numberline {1.4}Book outline}{16}
\contentsline {section}{\numberline {1.5}Route planner - suggested journeys through Bayesland}{18}
\contentsline {section}{\numberline {1.6}Video}{19}
\contentsline {section}{\numberline {1.7}Interactive elements}{20}
\contentsline {section}{\numberline {1.8}Interactive problem sets}{20}
\contentsline {section}{\numberline {1.9}Code}{20}
\contentsline {section}{\numberline {1.10}R, Stan and JAGS}{21}
\contentsline {section}{\numberline {1.11}Why don't more people use Bayesian statistics?}{22}
\contentsline {section}{\numberline {1.12}What are the tangible (non-academic) benefits of Bayesian statistics?}{23}
\contentsline {section}{\numberline {1.13}Suggested further reading}{24}
\contentsline {part}{I\hspace {1em}An introduction to Bayesian inference}{25}
\contentsline {section}{\numberline {1.14}Part mission statement}{27}
\contentsline {section}{\numberline {1.15}Part goals}{27}
\contentsline {chapter}{\numberline {2}The subjective worlds of Frequentist and Bayesian statistics}{29}
\contentsline {section}{\numberline {2.1}Chapter mission statement}{29}
\contentsline {section}{\numberline {2.2}Chapter goals}{29}
\contentsline {section}{\numberline {2.3}Bayes' rule - allowing us to go from the effect back to its cause}{30}
\contentsline {section}{\numberline {2.4}The purpose of statistical inference}{31}
\contentsline {section}{\numberline {2.5}The world according to Frequentists}{32}
\contentsline {section}{\numberline {2.6}The world according to Bayesians}{34}
\contentsline {section}{\numberline {2.7}Frequentist and Bayesian inference}{35}
\contentsline {subsection}{\numberline {2.7.1}The Frequentist and Bayesian murder trials}{36}
\contentsline {subsection}{\numberline {2.7.2}Radio control towers: example}{37}
\contentsline {section}{\numberline {2.8}Bayesian inference via Bayes' rule}{38}
\contentsline {subsection}{\numberline {2.8.1}Likelihoods}{39}
\contentsline {subsection}{\numberline {2.8.2}Priors}{40}
\contentsline {subsection}{\numberline {2.8.3}The denominator}{41}
\contentsline {subsection}{\numberline {2.8.4}Posteriors: the goal of Bayesian inference}{41}
\contentsline {section}{\numberline {2.9}Implicit vs Explicit subjectivity}{43}
\contentsline {section}{\numberline {2.10}Chapter summary}{45}
\contentsline {section}{\numberline {2.11}Chapter outcomes}{45}
\contentsline {section}{\numberline {2.12}Problem set}{45}
\contentsline {subsection}{\numberline {2.12.1}The deterministic nature of random coin throwing.}{45}
\contentsline {subsubsection}{Suppose that all combinations of angles and heights are equally likely to be chosen. What is the probability that the coin lands on heads?}{46}
\contentsline {subsubsection}{What are the new probabilities that the coin lands heads-up?}{47}
\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at an angle of 45 degrees. What is the probability that the coin lands heads-up?}{47}
\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at a height of 0.2m. What is the probability that the coin lands heads-up?}{47}
\contentsline {subsubsection}{If we constrained the angle and height to be fixed, what would happen in repetitions of the same experiment?}{47}
\contentsline {subsubsection}{In light of the previous question, comment on the Frequentist assumption of \textit {exact repetitions} of a given experiment.}{47}
\contentsline {subsection}{\numberline {2.12.2}Model choice}{47}
\contentsline {subsubsection}{Fit a linear regression model using classical least squares. How reasonable is the fit?}{48}
\contentsline {subsubsection}{Fit a quintic (powers up to the 5th) model to the data. How does its fit compare to that of the linear model?}{48}
\contentsline {subsubsection}{Fit a linear regression to each of the data sets, and similarly for the quintic model. Which of these performs best?}{48}
\contentsline {subsubsection}{Using the fits from the first part of this question, compare the performance of the linear regression model, with that of the quintic model.}{48}
\contentsline {subsubsection}{Which of the two models do you prefer, and why?}{48}
\contentsline {subsubsection}{If you then found out that the data were years of education (x), and salary in \$000s (y). Which model would you favour?}{48}
\contentsline {section}{\numberline {2.13}Appendix}{48}
\contentsline {subsection}{\numberline {2.13.1}The Frequentist and Bayesian murder trial}{48}
\contentsline {chapter}{\numberline {3}Probability - the nuts and bolts of Bayesian inference}{51}
\contentsline {section}{\numberline {3.1}Chapter mission statement}{51}
\contentsline {section}{\numberline {3.2}Chapter goals}{51}
\contentsline {section}{\numberline {3.3}Probability distributions: helping us explicitly state our ignorance}{52}
\contentsline {subsection}{\numberline {3.3.1}What make a probability distribution \textit {valid}?}{52}
\contentsline {subsection}{\numberline {3.3.2}Probabilities vs probability density : interpreting discrete and continuous probability distributions}{54}
\contentsline {subsubsection}{Analogy: stepping stones vs containers}{57}
\contentsline {subsubsection}{The good news: Bayes' rule doesn't distinguish between probabilities and probability densities}{57}
\contentsline {subsection}{\numberline {3.3.3}Mean and variance of distributions}{59}
\contentsline {subsection}{\numberline {3.3.4}Generalising probability distributions to two dimensions}{63}
\contentsline {subsubsection}{Horses for courses: a 2-dimensional discrete probability example}{64}
\contentsline {subsubsection}{Foot length and literacy: a 2-dimensional continuous probability example}{65}
\contentsline {subsection}{\numberline {3.3.5}Marginal distributions}{65}
\contentsline {subsubsection}{Venn diagrams}{68}
\contentsline {subsection}{\numberline {3.3.6}Conditional distributions}{70}
\contentsline {section}{\numberline {3.4}Higher dimensional probability densities: no harder than 2-D, just looks it!}{74}
\contentsline {section}{\numberline {3.5}Independence}{76}
\contentsline {subsection}{\numberline {3.5.1}Conditional independence}{79}
\contentsline {section}{\numberline {3.6}Central Limit Theorems}{80}
\contentsline {section}{\numberline {3.7}The Bayesian formula}{82}
\contentsline {subsection}{\numberline {3.7.1}The intuition behind the formula}{83}
\contentsline {subsection}{\numberline {3.7.2}Breast cancer screeing}{84}
\contentsline {section}{\numberline {3.8}The Bayesian inference process from the Bayesian formula}{85}
\contentsline {section}{\numberline {3.9}Chapter summary}{86}
\contentsline {section}{\numberline {3.10}Chapter outcomes}{86}
\contentsline {section}{\numberline {3.11}Problem set}{87}
\contentsline {subsection}{\numberline {3.11.1}The expected returns of a derivative}{87}
\contentsline {subsubsection}{What are the expected returns of the stock?}{87}
\contentsline {subsubsection}{What are the variance in returns?}{87}
\contentsline {subsubsection}{Would you expect the return to be equal to the mean of the original stock squared? If not, why not?}{87}
\contentsline {subsubsection}{What would be a fair price to pay for this derivative?}{88}
\contentsline {subsubsection}{What is a fair price to pay for this stock?}{88}
\contentsline {subsubsection}{Which asset is more risky, $D_t$ or $R_t$?}{88}
\contentsline {subsubsection}{Use \textit {method of moments} to estimate $\mu $ and $\sigma $. Note: this requires use of a computer with mathematical software, as analytic solutions aren't possible.}{88}
\contentsline {subsubsection}{Compare the estimated model with the data.}{88}
\contentsline {subsubsection}{Is the model a reasonable approximation to the data generating process?}{88}
\contentsline {subsubsection}{If not, suggest a better alternative.}{88}
\contentsline {subsection}{\numberline {3.11.2}The boy or girl paradox\let \reserved@d =[\def \par }{88}
\contentsline {subsection}{\numberline {3.11.3}The Bayesian game show}{89}
\contentsline {subsection}{\numberline {3.11.4}Blood doping}{89}
\contentsline {subsubsection}{What is the probability that a professional cyclist wins a race?}{90}
\contentsline {subsubsection}{What is the probability that a cyclist wins a race given that they have cheated?}{90}
\contentsline {subsubsection}{What is the probability that a cyclist is cheating given that he wins?}{90}
\contentsline {part}{II\hspace {1em}Understanding the Bayesian formula}{91}
\contentsline {section}{\numberline {3.12}Part mission statement}{93}
\contentsline {section}{\numberline {3.13}Part goals}{93}
\contentsline {chapter}{\numberline {4}The posterior - the goal of Bayesian inference}{95}
\contentsline {section}{\numberline {4.1}Chapter Mission statement}{95}
\contentsline {section}{\numberline {4.2}Chapter goals}{95}
\contentsline {section}{\numberline {4.3}Expressing uncertainty through the posterior probability distribution}{96}
\contentsline {subsection}{\numberline {4.3.1}Bayesian coastguard: introducing the prior and the posterior}{99}
\contentsline {subsection}{\numberline {4.3.2}Bayesian statistics: updating our pre-analysis uncertainty}{99}
\contentsline {subsection}{\numberline {4.3.3}Do parameters actually exist and have a point value?}{101}
\contentsline {subsection}{\numberline {4.3.4}Failings of the Frequentist confidence interval}{103}
\contentsline {subsection}{\numberline {4.3.5}Credible intervals}{105}
\contentsline {subsubsection}{Treasure hunting: The central posterior and highest density intervals}{106}
\contentsline {subsection}{\numberline {4.3.6}Reconciling the difference between confidence and credible intervals}{107}
\contentsline {subsubsection}{The interval ENIGMA}{108}
\contentsline {section}{\numberline {4.4}Point parameter estimates}{111}
\contentsline {section}{\numberline {4.5}Prediction using predictive distributions}{112}
\contentsline {subsection}{\numberline {4.5.1}Example: number of Republican voters within a sample}{113}
\contentsline {subsection}{\numberline {4.5.2}Example: interest rate hedging}{116}
\contentsline {section}{\numberline {4.6}Model comparison using the posterior}{120}
\contentsline {subsection}{\numberline {4.6.1}Example: epidemiologist comparison}{123}
\contentsline {subsection}{\numberline {4.6.2}Example: customer footfall}{124}
\contentsline {section}{\numberline {4.7}Model comparison through posterior predictive checks}{126}
\contentsline {subsection}{\numberline {4.7.1}Example: stock returns}{127}
\contentsline {section}{\numberline {4.8}Chapter summary}{128}
\contentsline {section}{\numberline {4.9}Chapter outcomes}{129}
\contentsline {section}{\numberline {4.10}Problem set}{129}
\contentsline {subsection}{\numberline {4.10.1}The lesser evil}{129}
\contentsline {subsubsection}{Which of the above loss functions do you think is most appropriate, and why?}{130}
\contentsline {subsubsection}{Which loss function might you choose to be most robust to \textit {any} situation?}{130}
\contentsline {subsubsection}{Following from the previous point, which measure of posterior centrality might you choose?}{130}
\contentsline {subsection}{\numberline {4.10.2}Google word search prediction}{130}
\contentsline {subsubsection}{Find the minimum-coverage confidence intervals of topics that exceed 70\%.}{131}
\contentsline {subsubsection}{Find most narrow credible intervals for topics that exceed 70\%.}{131}
\contentsline {subsubsection}{Topic search volumes}{131}
\contentsline {subsubsection}{Three-letter search volumes}{131}
\contentsline {subsection}{\numberline {4.10.3}Prior and posterior predictive example (with PPCs maybe)}{131}
\contentsline {section}{\numberline {4.11}Appendix}{132}
\contentsline {subsection}{\numberline {4.11.1}The interval ENIGMA - explained in full}{132}
\contentsline {chapter}{\numberline {5}Likelihoods}{133}
\contentsline {section}{\numberline {5.1}Chapter Mission statement}{133}
\contentsline {section}{\numberline {5.2}Chapter goals}{133}
\contentsline {section}{\numberline {5.3}What is a likelihood?}{134}
\contentsline {section}{\numberline {5.4}Why use 'likelihood' rather than 'probability'?}{136}
\contentsline {section}{\numberline {5.5}What are models and why do we need them?}{140}
\contentsline {section}{\numberline {5.6}How to choose an appropriate likelihood?}{141}
\contentsline {subsection}{\numberline {5.6.1}A likelihood model for an individual's disease status}{142}
\contentsline {subsection}{\numberline {5.6.2}A likelihood model for disease prevalence of a group}{144}
\contentsline {subsection}{\numberline {5.6.3}The intelligence of a group of people}{149}
\contentsline {section}{\numberline {5.7}Exchangeability vs random sampling}{151}
\contentsline {section}{\numberline {5.8}The subjectivity of model choice}{153}
\contentsline {section}{\numberline {5.9}Maximum likelihood - a short introduction}{154}
\contentsline {subsection}{\numberline {5.9.1}Estimating disease prevalence}{154}
\contentsline {subsection}{\numberline {5.9.2}Estimating the mean and variance in intelligence scores}{157}
\contentsline {section}{\numberline {5.10}Frequentist inference in Maximum Likelihood}{158}
\contentsline {section}{\numberline {5.11}Chapter summary}{160}
\contentsline {section}{\numberline {5.12}Chapter outcomes}{160}
\contentsline {section}{\numberline {5.13}Problem set}{160}
\contentsline {subsection}{\numberline {5.13.1}Blog blues.}{160}
\contentsline {subsubsection}{What assumptions might you make about the first-time visits?}{161}
\contentsline {subsubsection}{What model might be appropriate to model the time between visits?}{161}
\contentsline {subsubsection}{Algebraically derive an estimate of the mean number of visits per hour}{161}
\contentsline {subsubsection}{Data analysis:}{161}
\contentsline {subsubsection}{Graph the log likelihood near your estimated value. What does this show? Why don't we plot the likelihood?}{161}
\contentsline {subsubsection}{Estimate confidence intervals around your parameter.}{161}
\contentsline {subsubsection}{What is the probability that you will wait:}{161}
\contentsline {subsubsection}{Evaluate your model.}{162}
\contentsline {subsubsection}{What alternative models might be useful here?}{162}
\contentsline {subsubsection}{What are the assumptions behind these models?}{162}
\contentsline {subsubsection}{Estimate the parameters of your new model.}{162}
\contentsline {subsubsection}{Use your new model to estimate the probability that you will wait:}{162}
\contentsline {subsubsection}{Hints: the exponential is to the poisson model, what the ? is to the negative binomial.}{162}
\contentsline {subsection}{\numberline {5.13.2}Violent crime counts in New York counties}{162}
\contentsline {subsubsection}{Graph the violent crime account against population. What type of relationship does this suggest?}{162}
\contentsline {subsubsection}{A simple model}{162}
\contentsline {subsubsection}{What are the assumptions of this model?}{163}
\contentsline {subsubsection}{Estimate the parameter $\theta $ from the data. What does this parameter represent?}{163}
\contentsline {subsubsection}{Do these assumptions seem realistic?}{163}
\contentsline {subsubsection}{Estimate a measure of uncertainty in your estimates.}{163}
\contentsline {subsubsection}{Evaluate the performance of your model.}{163}
\contentsline {subsubsection}{A better model}{163}
\contentsline {subsubsection}{Why is this model better?}{163}
\contentsline {subsubsection}{What factors might affect $\theta _i$?}{163}
\contentsline {subsubsection}{Write down a new model specification taking into account the previous point.}{163}
\contentsline {subsubsection}{Estimate the parameters of this new specification.}{163}
\contentsline {subsubsection}{How does this new model compare to the previous iteration?}{163}
\contentsline {subsubsection}{What alternative specifications might be worth attempting?}{163}
\contentsline {subsection}{\numberline {5.13.3}Monte Carlo evaluation of the performance of MLE in R}{163}
\contentsline {subsection}{\numberline {5.13.4}The sample mean as MLE}{164}
\contentsline {chapter}{\numberline {6}Priors}{165}
\contentsline {section}{\numberline {6.1}Chapter Mission statement}{165}
\contentsline {section}{\numberline {6.2}Chapter goals}{165}
\contentsline {section}{\numberline {6.3}What are priors, and what do they represent?}{166}
\contentsline {section}{\numberline {6.4}Why do we need priors at all?}{168}
\contentsline {section}{\numberline {6.5}Why don't we just normalise likelihood by choosing a unity prior?}{169}
\contentsline {section}{\numberline {6.6}The explicit subjectivity of priors}{172}
\contentsline {section}{\numberline {6.7}Combining a prior and likelihood to form a posterior}{172}
\contentsline {subsection}{\numberline {6.7.1}The Goldfish game}{172}
\contentsline {subsection}{\numberline {6.7.2}Disease proportions revisited}{175}
\contentsline {subsubsection}{Interactive effect of the prior on the posterior}{177}
\contentsline {section}{\numberline {6.8}Constructing priors}{177}
\contentsline {subsection}{\numberline {6.8.1}Vague priors}{177}
\contentsline {subsection}{\numberline {6.8.2}Informative priors}{181}
\contentsline {subsection}{\numberline {6.8.3}The numerator of Bayes' rule determines the shape}{184}
\contentsline {subsection}{\numberline {6.8.4}Eliciting priors}{184}
\contentsline {section}{\numberline {6.9}A strong model is not heavily influenced by priors}{186}
\contentsline {section}{\numberline {6.10}Chapter summary}{188}
\contentsline {section}{\numberline {6.11}Chapter outcomes}{188}
\contentsline {section}{\numberline {6.12}Problem set}{189}
\contentsline {subsection}{\numberline {6.12.1}Counting sheep}{189}
\contentsline {subsubsection}{What likelihood model might you use here?}{189}
\contentsline {subsubsection}{What are the assumptions underpinning this model?}{189}
\contentsline {subsubsection}{Introducing a prior}{189}
\contentsline {subsubsection}{Which of the previous priors is most uninformative?}{190}
\contentsline {subsubsection}{Suppose that you observe 10 sheep jumping over the fence, calculate the posterior distribution for each of the different priors using your chosen likelihood.}{190}
\contentsline {subsubsection}{For what numbers of jumping sheep would one of your posteriors run into problems?}{190}
\contentsline {subsection}{\numberline {6.12.2}Investigating priors through US elections}{190}
\contentsline {subsection}{\numberline {6.12.3}Choosing prior distributions.}{191}
\contentsline {subsubsection}{If we choose a prior distribution that sets $p(log(\sigma ))=1$, what does this imply about the distribution of $p(\sigma )$?}{191}
\contentsline {subsubsection}{For a parameter $\theta $, which choice of prior will leave it invariant under transformations?}{191}
\contentsline {subsubsection}{Pre-experimental data prior setting}{191}
\contentsline {subsection}{\numberline {6.12.4}Expert data prior example}{191}
\contentsline {subsection}{\numberline {6.12.5}Data analysis example showing the declining importance of prior as data set increases in size}{191}
\contentsline {section}{\numberline {6.13}Appendix}{191}
\contentsline {subsection}{\numberline {6.13.1}Bayes' rule for the urn}{191}
\contentsline {subsection}{\numberline {6.13.2}The probabilities of having a disease}{192}
\contentsline {chapter}{\numberline {7}The devil's in the denominator}{193}
\contentsline {section}{\numberline {7.1}Chapter mission}{193}
\contentsline {section}{\numberline {7.2}Chapter goals}{193}
\contentsline {section}{\numberline {7.3}An introduction to the denominator}{194}
\contentsline {subsection}{\numberline {7.3.1}The denominator as a normalising factor}{194}
\contentsline {subsection}{\numberline {7.3.2}Example: disease}{195}
\contentsline {subsection}{\numberline {7.3.3}Example: the proportion of people who vote for conservatively}{197}
\contentsline {subsection}{\numberline {7.3.4}The denominator as a probability}{199}
\contentsline {subsection}{\numberline {7.3.5}Using the denominator to choose between competing models}{201}
\contentsline {section}{\numberline {7.4}The difficulty with the denominator}{206}
\contentsline {subsection}{\numberline {7.4.1}Multi-parameter discrete model example: the comorbidity between depression and anxiety}{206}
\contentsline {subsection}{\numberline {7.4.2}Continuous multi-parameter example: mean and variance of IQ}{209}
\contentsline {section}{\numberline {7.5}How to dispense with the difficulty: Bayesian computation}{213}
\contentsline {section}{\numberline {7.6}Chapter summary}{215}
\contentsline {section}{\numberline {7.7}Chapter outcomes}{215}
\contentsline {section}{\numberline {7.8}Problem set}{216}
\contentsline {subsection}{\numberline {7.8.1}New disease cases}{216}
\contentsline {subsubsection}{What likelihood model might be appropriate here?}{216}
\contentsline {subsubsection}{What are the assumptions of this model? Are they appropriate here?}{216}
\contentsline {subsubsection}{A gamma prior}{216}
\contentsline {subsubsection}{Data}{216}
\contentsline {subsubsection}{Find the posterior distribution for the mean parameter $\lambda $}{217}
\contentsline {subsection}{\numberline {7.8.2}The comorbidity between depression, anxiety and psychosis}{217}
\contentsline {subsubsection}{Calculate the probability that a patient is depressed.}{217}
\contentsline {subsubsection}{Calculate the probability that a patient is psychotic.}{217}
\contentsline {subsubsection}{What is the probability that a patient is psychotic given that they are depressed, and anxious?}{217}
\contentsline {subsubsection}{What is the probability that a patient is not psychotic if they are not depressed?}{217}
\contentsline {subsection}{\numberline {7.8.3}Finding mosquito larvae after rain}{217}
\contentsline {subsubsection}{Find the mean of a $lognormal(d,1)$ distribution.}{218}
\contentsline {subsubsection}{Finding the mode}{218}
\contentsline {subsubsection}{Use a computer to graph the shape of the posterior distribution.}{218}
\contentsline {subsubsection}{Calculate the posterior (difficult).}{218}
\contentsline {section}{\numberline {7.9}Appendix}{218}
\contentsline {part}{III\hspace {1em}Analytic Bayesian methods}{219}
\contentsline {section}{\numberline {7.10}Part mission statement}{221}
\contentsline {section}{\numberline {7.11}Part goals}{221}
\contentsline {chapter}{\numberline {8}An introduction to distributions for the mathematically-un-inclined}{223}
\contentsline {section}{\numberline {8.1}Chapter mission statement}{223}
\contentsline {section}{\numberline {8.2}Chapter goals}{223}
\contentsline {section}{\numberline {8.3}Sampling distributions for likelihoods}{223}
\contentsline {subsubsection}{Bernoulli}{223}
\contentsline {subsubsection}{Binomial}{226}
\contentsline {subsubsection}{Normal}{229}
\contentsline {subsubsection}{Poisson}{229}
\contentsline {subsubsection}{Negative binomial}{229}
\contentsline {subsubsection}{Logistic}{229}
\contentsline {section}{\numberline {8.4}Prior distributions}{229}
\contentsline {subsubsection}{Distributions for probabilities, proportions and percentages}{229}
\contentsline {subsubsection}{Uniform}{229}
\contentsline {subsubsection}{Beta}{229}
\contentsline {subsubsection}{Dirichlet}{229}
\contentsline {section}{\numberline {8.5}Table of distributions, their uses, and reasonable priors}{229}
\contentsline {subsection}{\numberline {8.5.1}Distributions for means and medians}{229}
\contentsline {subsubsection}{Normal}{229}
\contentsline {subsubsection}{Student t}{229}
\contentsline {subsection}{\numberline {8.5.2}Distributions for variances, and shape parameters}{229}
\contentsline {subsubsection}{Gamma}{229}
\contentsline {subsubsection}{Half-Cauchy}{229}
\contentsline {subsubsection}{Inverse Gamma}{229}
\contentsline {subsubsection}{Inverse chi}{229}
\contentsline {subsection}{\numberline {8.5.3}Multinomial - or other regression}{229}
\contentsline {subsection}{\numberline {8.5.4}LBG prior - see Michael Betancourt video and Stan doc}{229}
\contentsline {subsection}{\numberline {8.5.5}Wishart}{230}
\contentsline {subsection}{\numberline {8.5.6}Distributions for categories}{230}
\contentsline {subsubsection}{Categorical}{230}
\contentsline {section}{\numberline {8.6}Chapter summary}{230}
\contentsline {chapter}{\numberline {9}Conjugate priors and their place in Bayesian analysis}{231}
\contentsline {chapter}{\numberline {10}Objective Bayesian analysis}{233}
\contentsline {part}{IV\hspace {1em}A practical guide to doing real life Bayesian analysis: Computational Bayes}{235}
\contentsline {part}{V\hspace {1em}Regression analysis and hierarchical models}{239}
\contentsline {section}{\numberline {10.1}Part mission statement}{241}
\contentsline {section}{\numberline {10.2}Part goals}{241}
\contentsline {chapter}{\numberline {11}Hierarchical models}{243}
\contentsline {chapter}{\numberline {12}Hypothesis testing I: Classical Frequentist vs Bayesian approaches}{245}
\contentsline {chapter}{\numberline {13}Evaluation of model fit}{247}
