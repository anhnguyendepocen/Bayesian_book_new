\contentsline {chapter}{\numberline {1}How to best use this book}{21}{chapter.1}
\contentsline {section}{\numberline {1.1}The purpose of this book}{21}{section.1.1}
\contentsline {section}{\numberline {1.2}Who is this book for?}{23}{section.1.2}
\contentsline {section}{\numberline {1.3}Pre-requisites}{23}{section.1.3}
\contentsline {section}{\numberline {1.4}Book outline}{24}{section.1.4}
\contentsline {section}{\numberline {1.5}Route planner - suggested journeys through Bayesland}{26}{section.1.5}
\contentsline {section}{\numberline {1.6}Video}{27}{section.1.6}
\contentsline {section}{\numberline {1.7}Interactive elements}{28}{section.1.7}
\contentsline {section}{\numberline {1.8}Interactive problem sets}{28}{section.1.8}
\contentsline {section}{\numberline {1.9}Code}{28}{section.1.9}
\contentsline {section}{\numberline {1.10}R, Stan and JAGS}{29}{section.1.10}
\contentsline {section}{\numberline {1.11}Why don't more people use Bayesian statistics?}{30}{section.1.11}
\contentsline {section}{\numberline {1.12}What are the tangible (non-academic) benefits of Bayesian statistics?}{31}{section.1.12}
\contentsline {section}{\numberline {1.13}Suggested further reading}{32}{section.1.13}
\contentsline {part}{I\hspace {1em}An introduction to Bayesian inference}{33}{part.1}
\contentsline {section}{\numberline {1.14}Part mission statement}{35}{section.1.14}
\contentsline {section}{\numberline {1.15}Part goals}{35}{section.1.15}
\contentsline {chapter}{\numberline {2}The subjective worlds of Frequentist and Bayesian statistics}{37}{chapter.2}
\contentsline {section}{\numberline {2.1}Chapter mission statement}{37}{section.2.1}
\contentsline {section}{\numberline {2.2}Chapter goals}{37}{section.2.2}
\contentsline {section}{\numberline {2.3}Bayes' rule - allowing us to go from the effect back to its cause}{38}{section.2.3}
\contentsline {section}{\numberline {2.4}The purpose of statistical inference}{39}{section.2.4}
\contentsline {section}{\numberline {2.5}The world according to Frequentists}{40}{section.2.5}
\contentsline {section}{\numberline {2.6}The world according to Bayesians}{42}{section.2.6}
\contentsline {section}{\numberline {2.7}Frequentist and Bayesian inference}{43}{section.2.7}
\contentsline {subsection}{\numberline {2.7.1}The Frequentist and Bayesian murder trials}{44}{subsection.2.7.1}
\contentsline {subsection}{\numberline {2.7.2}Radio control towers: example}{45}{subsection.2.7.2}
\contentsline {section}{\numberline {2.8}Bayesian inference via Bayes' rule}{46}{section.2.8}
\contentsline {subsection}{\numberline {2.8.1}Likelihoods}{47}{subsection.2.8.1}
\contentsline {subsection}{\numberline {2.8.2}Priors}{48}{subsection.2.8.2}
\contentsline {subsection}{\numberline {2.8.3}The denominator}{49}{subsection.2.8.3}
\contentsline {subsection}{\numberline {2.8.4}Posteriors: the goal of Bayesian inference}{49}{subsection.2.8.4}
\contentsline {section}{\numberline {2.9}Implicit vs Explicit subjectivity}{51}{section.2.9}
\contentsline {section}{\numberline {2.10}Chapter summary}{53}{section.2.10}
\contentsline {section}{\numberline {2.11}Chapter outcomes}{53}{section.2.11}
\contentsline {section}{\numberline {2.12}Problem set}{53}{section.2.12}
\contentsline {subsection}{\numberline {2.12.1}The deterministic nature of random coin throwing.}{53}{subsection.2.12.1}
\contentsline {subsubsection}{Suppose that all combinations of angles and heights are equally likely to be chosen. What is the probability that the coin lands on heads?}{54}{section*.9}
\contentsline {subsubsection}{What are the new probabilities that the coin lands heads-up?}{55}{section*.10}
\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at an angle of 45 degrees. What is the probability that the coin lands heads-up?}{55}{section*.11}
\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at a height of 0.2m. What is the probability that the coin lands heads-up?}{55}{section*.12}
\contentsline {subsubsection}{If we constrained the angle and height to be fixed, what would happen in repetitions of the same experiment?}{55}{section*.13}
\contentsline {subsubsection}{In light of the previous question, comment on the Frequentist assumption of \textit {exact repetitions} of a given experiment.}{55}{section*.14}
\contentsline {subsection}{\numberline {2.12.2}Model choice}{55}{subsection.2.12.2}
\contentsline {subsubsection}{Fit a linear regression model using classical least squares. How reasonable is the fit?}{56}{section*.16}
\contentsline {subsubsection}{Fit a quintic (powers up to the 5th) model to the data. How does its fit compare to that of the linear model?}{56}{section*.17}
\contentsline {subsubsection}{Fit a linear regression to each of the data sets, and similarly for the quintic model. Which of these performs best?}{56}{section*.18}
\contentsline {subsubsection}{Using the fits from the first part of this question, compare the performance of the linear regression model, with that of the quintic model.}{56}{section*.19}
\contentsline {subsubsection}{Which of the two models do you prefer, and why?}{56}{section*.20}
\contentsline {subsubsection}{If you then found out that the data were years of education (x), and salary in \$000s (y). Which model would you favour?}{56}{section*.21}
\contentsline {section}{\numberline {2.13}Appendix}{56}{section.2.13}
\contentsline {subsection}{\numberline {2.13.1}The Frequentist and Bayesian murder trial}{56}{subsection.2.13.1}
\contentsline {chapter}{\numberline {3}Probability - the nuts and bolts of Bayesian inference}{59}{chapter.3}
\contentsline {section}{\numberline {3.1}Chapter mission statement}{59}{section.3.1}
\contentsline {section}{\numberline {3.2}Chapter goals}{59}{section.3.2}
\contentsline {section}{\numberline {3.3}Probability distributions: helping us explicitly state our ignorance}{60}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}What makes a probability distribution \textit {valid}?}{60}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Probabilities vs probability density : interpreting discrete and continuous probability distributions}{62}{subsection.3.3.2}
\contentsline {subsubsection}{A (wet) river crossing}{65}{section*.24}
\contentsline {subsubsection}{Probability zero vs impossibility}{68}{section*.26}
\contentsline {subsubsection}{The good news: Bayes' rule doesn't distinguish between probabilities and probability densities}{69}{section*.27}
\contentsline {subsection}{\numberline {3.3.3}Mean and variance of distributions}{70}{subsection.3.3.3}
\contentsline {subsection}{\numberline {3.3.4}Generalising probability distributions to two dimensions}{74}{subsection.3.3.4}
\contentsline {subsubsection}{Horses for courses: a 2-dimensional discrete probability example}{74}{section*.31}
\contentsline {subsubsection}{Foot length and literacy: a 2-dimensional continuous probability example}{76}{section*.33}
\contentsline {subsection}{\numberline {3.3.5}Marginal distributions}{76}{subsection.3.3.5}
\contentsline {subsubsection}{Venn diagrams}{80}{section*.36}
\contentsline {subsection}{\numberline {3.3.6}Conditional distributions}{80}{subsection.3.3.6}
\contentsline {section}{\numberline {3.4}Higher dimensional probability densities: no harder than 2-D, just looks it!}{83}{section.3.4}
\contentsline {section}{\numberline {3.5}Independence}{86}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Conditional independence}{89}{subsection.3.5.1}
\contentsline {section}{\numberline {3.6}Central Limit Theorems}{90}{section.3.6}
\contentsline {section}{\numberline {3.7}The Bayesian formula}{93}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}The intuition behind the formula}{94}{subsection.3.7.1}
\contentsline {subsection}{\numberline {3.7.2}Breast cancer screeing}{94}{subsection.3.7.2}
\contentsline {section}{\numberline {3.8}The Bayesian inference process from the Bayesian formula}{95}{section.3.8}
\contentsline {section}{\numberline {3.9}Chapter summary}{96}{section.3.9}
\contentsline {section}{\numberline {3.10}Chapter outcomes}{97}{section.3.10}
\contentsline {section}{\numberline {3.11}Problem set}{97}{section.3.11}
\contentsline {subsection}{\numberline {3.11.1}The expected returns of a derivative}{97}{subsection.3.11.1}
\contentsline {subsubsection}{What are the expected returns of the stock?}{97}{section*.46}
\contentsline {subsubsection}{What are the variance in returns?}{97}{section*.47}
\contentsline {subsubsection}{Would you expect the return to be equal to the mean of the original stock squared? If not, why not?}{98}{section*.48}
\contentsline {subsubsection}{What would be a fair price to pay for this derivative?}{98}{section*.49}
\contentsline {subsubsection}{What is a fair price to pay for this stock?}{98}{section*.50}
\contentsline {subsubsection}{Which asset is more risky, $D_t$ or $R_t$?}{98}{section*.51}
\contentsline {subsubsection}{Use \textit {method of moments} to estimate $\mu $ and $\sigma $. Note: this requires use of a computer with mathematical software, as analytic solutions aren't possible.}{98}{section*.52}
\contentsline {subsubsection}{Compare the estimated model with the data.}{98}{section*.53}
\contentsline {subsubsection}{Is the model a reasonable approximation to the data generating process?}{98}{section*.54}
\contentsline {subsubsection}{If not, suggest a better alternative.}{98}{section*.55}
\contentsline {subsection}{\numberline {3.11.2}The boy or girl paradox}{98}{subsection.3.11.2}
\contentsline {subsection}{\numberline {3.11.3}The Bayesian game show}{99}{subsection.3.11.3}
\contentsline {subsection}{\numberline {3.11.4}Blood doping}{99}{subsection.3.11.4}
\contentsline {subsubsection}{What is the probability that a professional cyclist wins a race?}{100}{section*.56}
\contentsline {subsubsection}{What is the probability that a cyclist wins a race given that they have cheated?}{100}{section*.57}
\contentsline {subsubsection}{What is the probability that a cyclist is cheating given that he wins?}{100}{section*.58}
\contentsline {part}{II\hspace {1em}Understanding the Bayesian formula}{101}{part.2}
\contentsline {section}{\numberline {3.12}Part mission statement}{103}{section.3.12}
\contentsline {section}{\numberline {3.13}Part goals}{103}{section.3.13}
\contentsline {chapter}{\numberline {4}The posterior - the goal of Bayesian inference}{105}{chapter.4}
\contentsline {section}{\numberline {4.1}Chapter Mission statement}{105}{section.4.1}
\contentsline {section}{\numberline {4.2}Chapter goals}{105}{section.4.2}
\contentsline {section}{\numberline {4.3}Expressing uncertainty through the posterior probability distribution}{106}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Bayesian coastguard: introducing the prior and the posterior}{109}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}Bayesian statistics: updating our pre-analysis uncertainty}{109}{subsection.4.3.2}
\contentsline {subsection}{\numberline {4.3.3}Do parameters actually exist and have a point value?}{111}{subsection.4.3.3}
\contentsline {subsection}{\numberline {4.3.4}Failings of the Frequentist confidence interval}{112}{subsection.4.3.4}
\contentsline {subsection}{\numberline {4.3.5}Credible intervals}{115}{subsection.4.3.5}
\contentsline {subsubsection}{Treasure hunting: The central posterior and highest density intervals}{116}{section*.66}
\contentsline {subsection}{\numberline {4.3.6}Reconciling the difference between confidence and credible intervals}{117}{subsection.4.3.6}
\contentsline {subsubsection}{The interval ENIGMA}{118}{section*.68}
\contentsline {section}{\numberline {4.4}Point parameter estimates}{121}{section.4.4}
\contentsline {section}{\numberline {4.5}Prediction using predictive distributions}{122}{section.4.5}
\contentsline {subsection}{\numberline {4.5.1}Example: number of Republican voters within a sample}{123}{subsection.4.5.1}
\contentsline {subsection}{\numberline {4.5.2}Example: interest rate hedging}{126}{subsection.4.5.2}
\contentsline {section}{\numberline {4.6}Model comparison using the posterior}{130}{section.4.6}
\contentsline {subsection}{\numberline {4.6.1}Example: epidemiologist comparison}{133}{subsection.4.6.1}
\contentsline {subsection}{\numberline {4.6.2}Example: customer footfall}{134}{subsection.4.6.2}
\contentsline {section}{\numberline {4.7}Model comparison through posterior predictive checks}{136}{section.4.7}
\contentsline {subsection}{\numberline {4.7.1}Example: stock returns}{137}{subsection.4.7.1}
\contentsline {section}{\numberline {4.8}Chapter summary}{138}{section.4.8}
\contentsline {section}{\numberline {4.9}Chapter outcomes}{139}{section.4.9}
\contentsline {section}{\numberline {4.10}Problem set}{139}{section.4.10}
\contentsline {subsection}{\numberline {4.10.1}The lesser evil}{139}{subsection.4.10.1}
\contentsline {subsubsection}{Which of the above loss functions do you think is most appropriate, and why?}{140}{section*.81}
\contentsline {subsubsection}{Which loss function might you choose to be most robust to \textit {any} situation?}{140}{section*.82}
\contentsline {subsubsection}{Following from the previous point, which measure of posterior centrality might you choose?}{140}{section*.83}
\contentsline {subsection}{\numberline {4.10.2}Google word search prediction}{140}{subsection.4.10.2}
\contentsline {subsubsection}{Find the minimum-coverage confidence intervals of topics that exceed 70\%.}{141}{section*.85}
\contentsline {subsubsection}{Find most narrow credible intervals for topics that exceed 70\%.}{141}{section*.86}
\contentsline {subsubsection}{Topic search volumes}{141}{section*.87}
\contentsline {subsubsection}{Three-letter search volumes}{141}{section*.89}
\contentsline {subsection}{\numberline {4.10.3}Prior and posterior predictive example (with PPCs maybe)}{141}{subsection.4.10.3}
\contentsline {section}{\numberline {4.11}Appendix}{142}{section.4.11}
\contentsline {subsection}{\numberline {4.11.1}The interval ENIGMA - explained in full}{142}{subsection.4.11.1}
\contentsline {chapter}{\numberline {5}Likelihoods}{143}{chapter.5}
\contentsline {section}{\numberline {5.1}Chapter Mission statement}{143}{section.5.1}
\contentsline {section}{\numberline {5.2}Chapter goals}{143}{section.5.2}
\contentsline {section}{\numberline {5.3}What is a likelihood?}{144}{section.5.3}
\contentsline {section}{\numberline {5.4}Why use 'likelihood' rather than 'probability'?}{146}{section.5.4}
\contentsline {section}{\numberline {5.5}What are models and why do we need them?}{150}{section.5.5}
\contentsline {section}{\numberline {5.6}How to choose an appropriate likelihood?}{151}{section.5.6}
\contentsline {subsection}{\numberline {5.6.1}A likelihood model for an individual's disease status}{152}{subsection.5.6.1}
\contentsline {subsection}{\numberline {5.6.2}A likelihood model for disease prevalence of a group}{154}{subsection.5.6.2}
\contentsline {subsection}{\numberline {5.6.3}The intelligence of a group of people}{159}{subsection.5.6.3}
\contentsline {section}{\numberline {5.7}Exchangeability vs random sampling}{161}{section.5.7}
\contentsline {section}{\numberline {5.8}The subjectivity of model choice}{163}{section.5.8}
\contentsline {section}{\numberline {5.9}Maximum likelihood - a short introduction}{163}{section.5.9}
\contentsline {subsection}{\numberline {5.9.1}Estimating disease prevalence}{164}{subsection.5.9.1}
\contentsline {subsection}{\numberline {5.9.2}Estimating the mean and variance in intelligence scores}{167}{subsection.5.9.2}
\contentsline {subsection}{\numberline {5.9.3}Maximum likelihood in simple steps}{168}{subsection.5.9.3}
\contentsline {section}{\numberline {5.10}Frequentist inference in Maximum Likelihood}{169}{section.5.10}
\contentsline {section}{\numberline {5.11}Chapter summary}{169}{section.5.11}
\contentsline {section}{\numberline {5.12}Chapter outcomes}{171}{section.5.12}
\contentsline {section}{\numberline {5.13}Problem set}{171}{section.5.13}
\contentsline {subsection}{\numberline {5.13.1}Blog blues.}{171}{subsection.5.13.1}
\contentsline {subsubsection}{What assumptions might you make about the first-time visits?}{172}{section*.101}
\contentsline {subsubsection}{What model might be appropriate to model the time between visits?}{172}{section*.102}
\contentsline {subsubsection}{Algebraically derive an estimate of the mean number of visits per hour}{172}{section*.103}
\contentsline {subsubsection}{Data analysis:}{172}{section*.104}
\contentsline {subsubsection}{Graph the log likelihood near your estimated value. What does this show? Why don't we plot the likelihood?}{172}{section*.105}
\contentsline {subsubsection}{Estimate confidence intervals around your parameter.}{172}{section*.106}
\contentsline {subsubsection}{What is the probability that you will wait:}{172}{section*.107}
\contentsline {subsubsection}{Evaluate your model.}{172}{section*.108}
\contentsline {subsubsection}{What alternative models might be useful here?}{172}{section*.109}
\contentsline {subsubsection}{What are the assumptions behind these models?}{172}{section*.110}
\contentsline {subsubsection}{Estimate the parameters of your new model.}{172}{section*.111}
\contentsline {subsubsection}{Use your new model to estimate the probability that you will wait:}{172}{section*.112}
\contentsline {subsubsection}{Hints: the exponential is to the poisson model, what the ? is to the negative binomial.}{173}{section*.113}
\contentsline {subsection}{\numberline {5.13.2}Violent crime counts in New York counties}{173}{subsection.5.13.2}
\contentsline {subsubsection}{Graph the violent crime account against population. What type of relationship does this suggest?}{173}{section*.114}
\contentsline {subsubsection}{A simple model}{173}{section*.115}
\contentsline {subsubsection}{What are the assumptions of this model?}{174}{section*.116}
\contentsline {subsubsection}{Estimate the parameter $\theta $ from the data. What does this parameter represent?}{174}{section*.117}
\contentsline {subsubsection}{Do these assumptions seem realistic?}{174}{section*.118}
\contentsline {subsubsection}{Estimate a measure of uncertainty in your estimates.}{174}{section*.119}
\contentsline {subsubsection}{Evaluate the performance of your model.}{174}{section*.120}
\contentsline {subsubsection}{A better model}{174}{section*.121}
\contentsline {subsubsection}{Why is this model better?}{174}{section*.122}
\contentsline {subsubsection}{What factors might affect $\theta _i$?}{174}{section*.123}
\contentsline {subsubsection}{Write down a new model specification taking into account the previous point.}{174}{section*.124}
\contentsline {subsubsection}{Estimate the parameters of this new specification.}{174}{section*.125}
\contentsline {subsubsection}{How does this new model compare to the previous iteration?}{174}{section*.126}
\contentsline {subsubsection}{What alternative specifications might be worth attempting?}{174}{section*.127}
\contentsline {subsection}{\numberline {5.13.3}Monte Carlo evaluation of the performance of MLE in R}{174}{subsection.5.13.3}
\contentsline {subsection}{\numberline {5.13.4}The sample mean as MLE}{175}{subsection.5.13.4}
\contentsline {chapter}{\numberline {6}Priors}{177}{chapter.6}
\contentsline {section}{\numberline {6.1}Chapter Mission statement}{177}{section.6.1}
\contentsline {section}{\numberline {6.2}Chapter goals}{177}{section.6.2}
\contentsline {section}{\numberline {6.3}What are priors, and what do they represent?}{178}{section.6.3}
\contentsline {section}{\numberline {6.4}Why do we need priors at all?}{180}{section.6.4}
\contentsline {section}{\numberline {6.5}Why don't we just normalise likelihood by choosing a unity prior?}{181}{section.6.5}
\contentsline {section}{\numberline {6.6}The explicit subjectivity of priors}{184}{section.6.6}
\contentsline {section}{\numberline {6.7}Combining a prior and likelihood to form a posterior}{184}{section.6.7}
\contentsline {subsection}{\numberline {6.7.1}The Goldfish game}{184}{subsection.6.7.1}
\contentsline {subsection}{\numberline {6.7.2}Disease proportions revisited}{187}{subsection.6.7.2}
\contentsline {section}{\numberline {6.8}Constructing priors}{190}{section.6.8}
\contentsline {subsection}{\numberline {6.8.1}Vague priors}{190}{subsection.6.8.1}
\contentsline {subsection}{\numberline {6.8.2}Informative priors}{194}{subsection.6.8.2}
\contentsline {subsection}{\numberline {6.8.3}The numerator of Bayes' rule determines the shape}{196}{subsection.6.8.3}
\contentsline {subsection}{\numberline {6.8.4}Eliciting priors}{196}{subsection.6.8.4}
\contentsline {section}{\numberline {6.9}A strong model is less sensitive to prior choice}{197}{section.6.9}
\contentsline {subsection}{\numberline {6.9.1}Caveat: overly-zealous priors \textit {will} affect the posterior}{199}{subsection.6.9.1}
\contentsline {section}{\numberline {6.10}Chapter summary}{200}{section.6.10}
\contentsline {section}{\numberline {6.11}Chapter outcomes}{202}{section.6.11}
\contentsline {section}{\numberline {6.12}Problem set}{203}{section.6.12}
\contentsline {subsection}{\numberline {6.12.1}Counting sheep}{203}{subsection.6.12.1}
\contentsline {subsubsection}{What likelihood model might you use here?}{203}{section*.142}
\contentsline {subsubsection}{What are the assumptions underpinning this model?}{203}{section*.143}
\contentsline {subsubsection}{Introducing a prior}{203}{section*.144}
\contentsline {subsubsection}{Which of the previous priors is most uninformative?}{204}{section*.145}
\contentsline {subsubsection}{Suppose that you observe 10 sheep jumping over the fence, calculate the posterior distribution for each of the different priors using your chosen likelihood.}{204}{section*.146}
\contentsline {subsubsection}{For what numbers of jumping sheep would one of your posteriors run into problems?}{204}{section*.147}
\contentsline {subsection}{\numberline {6.12.2}Investigating priors through US elections}{204}{subsection.6.12.2}
\contentsline {subsection}{\numberline {6.12.3}Choosing prior distributions.}{205}{subsection.6.12.3}
\contentsline {subsubsection}{If we choose a prior distribution that sets $p(log(\sigma ))=1$, what does this imply about the distribution of $p(\sigma )$?}{205}{section*.148}
\contentsline {subsubsection}{For a parameter $\theta $, which choice of prior will leave it invariant under transformations?}{205}{section*.149}
\contentsline {subsubsection}{Pre-experimental data prior setting}{205}{section*.150}
\contentsline {subsection}{\numberline {6.12.4}Expert data prior example}{205}{subsection.6.12.4}
\contentsline {subsection}{\numberline {6.12.5}Data analysis example showing the declining importance of prior as data set increases in size}{205}{subsection.6.12.5}
\contentsline {section}{\numberline {6.13}Appendix}{205}{section.6.13}
\contentsline {subsection}{\numberline {6.13.1}Bayes' rule for the urn}{205}{subsection.6.13.1}
\contentsline {subsection}{\numberline {6.13.2}The probabilities of having a disease}{206}{subsection.6.13.2}
\contentsline {chapter}{\numberline {7}The devil's in the denominator}{207}{chapter.7}
\contentsline {section}{\numberline {7.1}Chapter mission}{207}{section.7.1}
\contentsline {section}{\numberline {7.2}Chapter goals}{207}{section.7.2}
\contentsline {section}{\numberline {7.3}An introduction to the denominator}{208}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}The denominator as a normalising factor}{208}{subsection.7.3.1}
\contentsline {subsection}{\numberline {7.3.2}Example: disease}{209}{subsection.7.3.2}
\contentsline {subsection}{\numberline {7.3.3}Example: the proportion of people who vote the Conservatives}{211}{subsection.7.3.3}
\contentsline {subsection}{\numberline {7.3.4}The denominator as a probability}{213}{subsection.7.3.4}
\contentsline {subsection}{\numberline {7.3.5}Using the denominator to choose between competing models}{215}{subsection.7.3.5}
\contentsline {section}{\numberline {7.4}The difficulty with the denominator}{220}{section.7.4}
\contentsline {subsection}{\numberline {7.4.1}Multi-parameter discrete model example: the comorbidity between depression and anxiety}{220}{subsection.7.4.1}
\contentsline {subsection}{\numberline {7.4.2}Continuous multi-parameter example: mean and variance of IQ}{223}{subsection.7.4.2}
\contentsline {section}{\numberline {7.5}How to dispense with the difficulty: Bayesian computation}{227}{section.7.5}
\contentsline {section}{\numberline {7.6}Chapter summary}{229}{section.7.6}
\contentsline {section}{\numberline {7.7}Chapter outcomes}{229}{section.7.7}
\contentsline {section}{\numberline {7.8}Problem set}{230}{section.7.8}
\contentsline {subsection}{\numberline {7.8.1}New disease cases}{230}{subsection.7.8.1}
\contentsline {subsubsection}{What likelihood model might be appropriate here?}{230}{section*.158}
\contentsline {subsubsection}{What are the assumptions of this model? Are they appropriate here?}{230}{section*.159}
\contentsline {subsubsection}{A gamma prior}{230}{section*.160}
\contentsline {subsubsection}{Data}{230}{section*.161}
\contentsline {subsubsection}{Find the posterior distribution for the mean parameter $\lambda $}{231}{section*.162}
\contentsline {subsection}{\numberline {7.8.2}The comorbidity between depression, anxiety and psychosis}{231}{subsection.7.8.2}
\contentsline {subsubsection}{Calculate the probability that a patient is depressed.}{231}{section*.163}
\contentsline {subsubsection}{Calculate the probability that a patient is psychotic.}{231}{section*.164}
\contentsline {subsubsection}{What is the probability that a patient is psychotic given that they are depressed, and anxious?}{231}{section*.165}
\contentsline {subsubsection}{What is the probability that a patient is not psychotic if they are not depressed?}{231}{section*.166}
\contentsline {subsection}{\numberline {7.8.3}Finding mosquito larvae after rain}{231}{subsection.7.8.3}
\contentsline {subsubsection}{Find the mean of a $lognormal(d,1)$ distribution.}{232}{section*.168}
\contentsline {subsubsection}{Finding the mode}{232}{section*.169}
\contentsline {subsubsection}{Use a computer to graph the shape of the posterior distribution.}{232}{section*.170}
\contentsline {subsubsection}{Calculate the posterior (difficult).}{232}{section*.171}
\contentsline {section}{\numberline {7.9}Appendix}{232}{section.7.9}
\contentsline {part}{III\hspace {1em}Analytic Bayesian methods}{233}{part.3}
\contentsline {section}{\numberline {7.10}Part mission statement}{235}{section.7.10}
\contentsline {section}{\numberline {7.11}Part goals}{235}{section.7.11}
\contentsline {chapter}{\numberline {8}An introduction to distributions for the mathematically-un-inclined}{237}{chapter.8}
\contentsline {section}{\numberline {8.1}Chapter mission statement}{237}{section.8.1}
\contentsline {section}{\numberline {8.2}Chapter goals}{237}{section.8.2}
\contentsline {section}{\numberline {8.3}The interrelation among distributions}{238}{section.8.3}
\contentsline {section}{\numberline {8.4}Sampling distributions for likelihoods}{239}{section.8.4}
\contentsline {subsection}{\numberline {8.4.1}Bernoulli}{240}{subsection.8.4.1}
\contentsline {subsection}{\numberline {8.4.2}Binomial}{242}{subsection.8.4.2}
\contentsline {subsection}{\numberline {8.4.3}Poisson}{245}{subsection.8.4.3}
\contentsline {subsection}{\numberline {8.4.4}Negative binomial}{248}{subsection.8.4.4}
\contentsline {subsection}{\numberline {8.4.5}Beta-binomial}{252}{subsection.8.4.5}
\contentsline {subsection}{\numberline {8.4.6}Normal}{256}{subsection.8.4.6}
\contentsline {subsection}{\numberline {8.4.7}Student t}{258}{subsection.8.4.7}
\contentsline {subsection}{\numberline {8.4.8}Exponential}{261}{subsection.8.4.8}
\contentsline {subsection}{\numberline {8.4.9}Gamma}{264}{subsection.8.4.9}
\contentsline {subsection}{\numberline {8.4.10}Multinomial}{267}{subsection.8.4.10}
\contentsline {subsection}{\numberline {8.4.11}Multivariate normal and multivariate t}{271}{subsection.8.4.11}
\contentsline {section}{\numberline {8.5}Prior distributions}{275}{section.8.5}
\contentsline {subsection}{\numberline {8.5.1}Distributions for probabilities, proportions and percentages}{276}{subsection.8.5.1}
\contentsline {subsubsection}{Uniform}{276}{section*.207}
\contentsline {subsubsection}{Beta}{277}{section*.210}
\contentsline {subsubsection}{Logit-normal}{279}{section*.213}
\contentsline {subsubsection}{Dirichlet}{280}{section*.216}
\contentsline {subsection}{\numberline {8.5.2}Distributions for means and regression coefficients}{284}{subsection.8.5.2}
\contentsline {subsubsection}{Normal}{285}{section*.220}
\contentsline {subsubsection}{Student t}{286}{section*.222}
\contentsline {subsubsection}{Cauchy}{287}{section*.224}
\contentsline {subsubsection}{Multivariate normal, and multivariate t}{289}{section*.227}
\contentsline {subsection}{\numberline {8.5.3}Distributions for non-negative parameters}{291}{subsection.8.5.3}
\contentsline {subsubsection}{Gamma}{291}{section*.228}
\contentsline {subsubsection}{Half-Cauchy, inverse-gamma, inverse-$\chi ^2$ and uniform}{292}{section*.230}
\contentsline {subsubsection}{Log-normal}{295}{section*.233}
\contentsline {subsection}{\numberline {8.5.4}Distributions for covariance and correlation matrices}{297}{subsection.8.5.4}
\contentsline {subsubsection}{LKJ}{298}{section*.236}
\contentsline {subsubsection}{Wishart and inverse-Wishart}{304}{section*.240}
\contentsline {section}{\numberline {8.6}Choosing a likelihood made easy}{306}{section.8.6}
\contentsline {section}{\numberline {8.7}Table of common likelihoods, their uses, and reasonable priors}{307}{section.8.7}
\contentsline {section}{\numberline {8.8}Distributions of distributions, and mixtures - link to website, and relevance}{307}{section.8.8}
\contentsline {section}{\numberline {8.9}Chapter summary}{307}{section.8.9}
\contentsline {section}{\numberline {8.10}Chapter outcomes}{309}{section.8.10}
\contentsline {section}{\numberline {8.11}Problem set}{310}{section.8.11}
\contentsline {subsection}{\numberline {8.11.1}Drug trials}{310}{subsection.8.11.1}
\contentsline {subsection}{\numberline {8.11.2}100m results across countries}{311}{subsection.8.11.2}
\contentsline {subsection}{\numberline {8.11.3}Triangular representation of simplexes}{311}{subsection.8.11.3}
\contentsline {subsubsection}{Show that the triangular representation of a 3 parameter Dirichlet is correct.}{311}{section*.244}
\contentsline {subsection}{\numberline {8.11.4}Normal distribution with normal prior}{311}{subsection.8.11.4}
\contentsline {subsubsection}{Prove that the mean of the lower distribution is the same as that of the prior}{311}{section*.245}
\contentsline {subsubsection}{Find the mean of the lower distribution}{311}{section*.246}
\contentsline {subsubsection}{Prove that the distribution of the lower distribution is unconditionally normal}{311}{section*.247}
\contentsline {chapter}{\numberline {9}Conjugate priors and their place in Bayesian analysis}{313}{chapter.9}
\contentsline {section}{\numberline {9.1}Chapter mission statement}{313}{section.9.1}
\contentsline {section}{\numberline {9.2}Chapter goals}{313}{section.9.2}
\contentsline {section}{\numberline {9.3}What is a conjugate prior and why are they useful?}{314}{section.9.3}
\contentsline {section}{\numberline {9.4}Gamma-poisson example}{318}{section.9.4}
\contentsline {section}{\numberline {9.5}Normal example: extra}{319}{section.9.5}
\contentsline {section}{\numberline {9.6}Table of conjugate priors}{322}{section.9.6}
\contentsline {section}{\numberline {9.7}The lessons and limits of a conjugate analysis}{322}{section.9.7}
\contentsline {section}{\numberline {9.8}Chapter summary}{324}{section.9.8}
\contentsline {section}{\numberline {9.9}Chapter outcomes}{324}{section.9.9}
\contentsline {chapter}{\numberline {10}Evaluation of model fit and hypothesis testing}{327}{chapter.10}
\contentsline {section}{\numberline {10.1}Chapter mission statement}{327}{section.10.1}
\contentsline {section}{\numberline {10.2}Chapter goals}{327}{section.10.2}
\contentsline {section}{\numberline {10.3}Posterior predictive checks}{328}{section.10.3}
\contentsline {subsection}{\numberline {10.3.1}Recap - posterior predictive distributions}{328}{subsection.10.3.1}
\contentsline {subsection}{\numberline {10.3.2}Graphical PPCs and Bayesian p values}{330}{subsection.10.3.2}
\contentsline {subsubsection}{Amazon staff hiring}{331}{section*.252}
\contentsline {subsubsection}{Mosquito lifetime}{334}{section*.254}
\contentsline {subsubsection}{Snow days}{335}{section*.256}
\contentsline {subsubsection}{Word distributions within topics}{336}{section*.258}
\contentsline {subsubsection}{Bacterial infection}{339}{section*.260}
\contentsline {section}{\numberline {10.4}Why do we call it a \textit {p value}?}{342}{section.10.4}
\contentsline {section}{\numberline {10.5}Statistics measuring predictive accuracy: AIC, Deviance, WAIC and LOO}{343}{section.10.5}
\contentsline {subsection}{\numberline {10.5.1}Independent evaluation of a model, and overfitting}{343}{subsection.10.5.1}
\contentsline {subsection}{\numberline {10.5.2}How to measure a model's predictive capability?}{344}{subsection.10.5.2}
\contentsline {subsection}{\numberline {10.5.3}The ideal measure of a model's predictive accuracy}{346}{subsection.10.5.3}
\contentsline {subsection}{\numberline {10.5.4}Estimating out-of-sample predictive accuracy from in-sample data}{350}{subsection.10.5.4}
\contentsline {subsection}{\numberline {10.5.5}AIC}{351}{subsection.10.5.5}
\contentsline {subsection}{\numberline {10.5.6}DIC}{351}{subsection.10.5.6}
\contentsline {subsection}{\numberline {10.5.7}WAIC}{352}{subsection.10.5.7}
\contentsline {subsection}{\numberline {10.5.8}LOO}{357}{subsection.10.5.8}
\contentsline {subsection}{\numberline {10.5.9}A practical and short summary of measures of predictive accuracy in simple terms}{358}{subsection.10.5.9}
\contentsline {section}{\numberline {10.6}Choosing one model, or a number?}{360}{section.10.6}
\contentsline {section}{\numberline {10.7}Sensitivity analysis}{360}{section.10.7}
\contentsline {section}{\numberline {10.8}Chapter summary}{362}{section.10.8}
\contentsline {section}{\numberline {10.9}Chapter outcomes}{363}{section.10.9}
\contentsline {chapter}{\numberline {11}Making Bayesian analysis objective?}{365}{chapter.11}
\contentsline {section}{\numberline {11.1}Chapter mission statement}{365}{section.11.1}
\contentsline {section}{\numberline {11.2}Chapter goals}{365}{section.11.2}
\contentsline {section}{\numberline {11.3}The illusion of the 'uninformative' uniform prior}{366}{section.11.3}
\contentsline {section}{\numberline {11.4}Jeffreys' priors}{367}{section.11.4}
\contentsline {subsection}{\numberline {11.4.1}Another definition of 'uninformative'}{367}{subsection.11.4.1}
\contentsline {subsection}{\numberline {11.4.2}Being critical}{371}{subsection.11.4.2}
\contentsline {section}{\numberline {11.5}Reference priors}{372}{section.11.5}
\contentsline {section}{\numberline {11.6}Empirical Bayes}{374}{section.11.6}
\contentsline {section}{\numberline {11.7}A move towards weakly informative priors}{376}{section.11.7}
\contentsline {section}{\numberline {11.8}Chapter summary}{377}{section.11.8}
\contentsline {section}{\numberline {11.9}Chapter outcomes}{378}{section.11.9}
\contentsline {part}{IV\hspace {1em}A practical guide to doing real life Bayesian analysis: Computational Bayes}{381}{part.4}
\contentsline {section}{\numberline {11.10}Part mission statement}{383}{section.11.10}
\contentsline {section}{\numberline {11.11}Part goals}{383}{section.11.11}
\contentsline {chapter}{\numberline {12}Leaving conjugates behind: Markov Chain Monte Carlo}{385}{chapter.12}
\contentsline {section}{\numberline {12.1}Chapter mission statement}{385}{section.12.1}
\contentsline {section}{\numberline {12.2}Chapter goals}{385}{section.12.2}
\contentsline {section}{\numberline {12.3}The difficulty with real life Bayesian inference}{387}{section.12.3}
\contentsline {section}{\numberline {12.4}Discrete approximation to continuous posteriors}{388}{section.12.4}
\contentsline {section}{\numberline {12.5}The posterior through quadrature}{391}{section.12.5}
\contentsline {section}{\numberline {12.6}Integrating using independent samples: an introduction to Monte Carlo}{393}{section.12.6}
\contentsline {subsection}{\numberline {12.6.1}BSE revisited}{396}{subsection.12.6.1}
\contentsline {section}{\numberline {12.7}Why is independent sampling easier said than done?}{398}{section.12.7}
\contentsline {section}{\numberline {12.8}Ideal sampling from a posterior using only the unnormalised posterior}{401}{section.12.8}
\contentsline {subsection}{\numberline {12.8.1}The unnormalised posterior - a window onto the real deal}{403}{subsection.12.8.1}
\contentsline {section}{\numberline {12.9}Moving from independent to dependent sampling}{404}{section.12.9}
\contentsline {subsection}{\numberline {12.9.1}An analogy to MCMC: mapping mountains}{406}{subsection.12.9.1}
\contentsline {section}{\numberline {12.10}What's the catch with dependent samplers?}{407}{section.12.10}
\contentsline {section}{\numberline {12.11}Chapter summary}{410}{section.12.11}
\contentsline {section}{\numberline {12.12}Chapter outcomes}{412}{section.12.12}
\contentsline {section}{\numberline {12.13}Problem set}{412}{section.12.13}
\contentsline {subsection}{\numberline {12.13.1}Prove that the inverse-transform sampler works!}{412}{subsection.12.13.1}
\contentsline {subsection}{\numberline {12.13.2}Computationally investigate the die example}{412}{subsection.12.13.2}
\contentsline {subsection}{\numberline {12.13.3}Evaluate the mean and variance of the posterior for the BSE example}{412}{subsection.12.13.3}
\contentsline {chapter}{\numberline {13}The Metropolis algorithm}{413}{chapter.13}
\contentsline {section}{\numberline {13.1}Chapter mission statement}{413}{section.13.1}
\contentsline {section}{\numberline {13.2}Chapter goals}{413}{section.13.2}
\contentsline {section}{\numberline {13.3}Sustainable fishing}{415}{section.13.3}
\contentsline {section}{\numberline {13.4}Prospecting for gold}{416}{section.13.4}
\contentsline {section}{\numberline {13.5}Defining the Metropolis algorithm}{419}{section.13.5}
\contentsline {section}{\numberline {13.6}When does Metropolis work?}{423}{section.13.6}
\contentsline {subsection}{\numberline {13.6.1}Mathematical underpinnings of MCMC}{425}{subsection.13.6.1}
\contentsline {subsection}{\numberline {13.6.2}Desirable qualities of a Markov Chain}{426}{subsection.13.6.2}
\contentsline {subsection}{\numberline {13.6.3}Detailed balance}{426}{subsection.13.6.3}
\contentsline {subsection}{\numberline {13.6.4}The intuition behind the acceptance/rejection rule of Metropolis, and detailed balance}{429}{subsection.13.6.4}
\contentsline {subsection}{\numberline {13.6.5}Proving things we know to be true: Mathematics revisited}{431}{subsection.13.6.5}
\contentsline {section}{\numberline {13.7}Efficiency of convergence: the importance of choosing the right proposal scale}{432}{section.13.7}
\contentsline {subsection}{\numberline {13.7.1}MCMC as finding and exploring the typical set}{436}{subsection.13.7.1}
\contentsline {subsection}{\numberline {13.7.2}Speeding up convergence: tuning the proposal distribution}{437}{subsection.13.7.2}
\contentsline {subsection}{\numberline {13.7.3}Geometric convergence of Markov Chains}{439}{subsection.13.7.3}
\contentsline {section}{\numberline {13.8}Judging convergence}{440}{section.13.8}
\contentsline {subsection}{\numberline {13.8.1}Bob's bees in a house}{440}{subsection.13.8.1}
\contentsline {subsection}{\numberline {13.8.2}Using multiple chains to monitor convergence}{443}{subsection.13.8.2}
\contentsline {subsection}{\numberline {13.8.3}Using within- and between-chain variation to estimate convergence}{446}{subsection.13.8.3}
\contentsline {subsection}{\numberline {13.8.4}Two types of non-convergence}{448}{subsection.13.8.4}
\contentsline {subsection}{\numberline {13.8.5}Warm-up}{449}{subsection.13.8.5}
\contentsline {section}{\numberline {13.9}Effective sample size revisited}{451}{section.13.9}
\contentsline {subsection}{\numberline {13.9.1}Thinning samples to increase effective sample size}{453}{subsection.13.9.1}
\contentsline {section}{\numberline {13.10}Chapter summary}{454}{section.13.10}
\contentsline {section}{\numberline {13.11}Chapter outcomes}{456}{section.13.11}
\contentsline {section}{\numberline {13.12}Problem set}{457}{section.13.12}
\contentsline {subsection}{\numberline {13.12.1}Metropolis-Hastings example}{457}{subsection.13.12.1}
\contentsline {subsection}{\numberline {13.12.2}Find the optimal rejection probability = Gelman says 0.44 for unimodal}{457}{subsection.13.12.2}
\contentsline {subsection}{\numberline {13.12.3}Adaptive MCMC}{457}{subsection.13.12.3}
\contentsline {chapter}{\numberline {14}Gibbs sampling}{459}{chapter.14}
\contentsline {section}{\numberline {14.1}Chapter mission statement}{459}{section.14.1}
\contentsline {section}{\numberline {14.2}Chapter goals}{459}{section.14.2}
\contentsline {section}{\numberline {14.3}Back to prospecting for gold}{461}{section.14.3}
\contentsline {section}{\numberline {14.4}Defining the Gibbs algorithm}{463}{section.14.4}
\contentsline {subsection}{\numberline {14.4.1}Crime and punishment/unemployment}{466}{subsection.14.4.1}
\contentsline {section}{\numberline {14.5}Gibbs' earth: the intuition behind the Gibbs algorithm}{466}{section.14.5}
\contentsline {section}{\numberline {14.6}The benefits and problems with Gibbs and Random Walk Metropolis}{468}{section.14.6}
\contentsline {section}{\numberline {14.7}A change of parameters to speed up exploration}{470}{section.14.7}
\contentsline {section}{\numberline {14.8}Chapter summary}{471}{section.14.8}
\contentsline {section}{\numberline {14.9}Chapter outcomes}{472}{section.14.9}
\contentsline {section}{\numberline {14.10}Problem set}{472}{section.14.10}
\contentsline {subsection}{\numberline {14.10.1}Prove that the Gibbs sampler can be viewed as a case of Metropolis-Hastings}{472}{subsection.14.10.1}
\contentsline {subsection}{\numberline {14.10.2}Prove that a Gibbs sampler on bivariate normal has an effective sample size of half its true sample size}{472}{subsection.14.10.2}
\contentsline {section}{\numberline {14.11}Appendix}{472}{section.14.11}
\contentsline {subsection}{\numberline {14.11.1}Derivation of conditional distributions for multivariate normal}{472}{subsection.14.11.1}
\contentsline {subsection}{\numberline {14.11.2}Derive uncorrelated normals from bivariate normal}{472}{subsection.14.11.2}
\contentsline {chapter}{\numberline {15}Hamiltonian Monte Carlo}{473}{chapter.15}
\contentsline {section}{\numberline {15.1}Chapter mission statement}{473}{section.15.1}
\contentsline {section}{\numberline {15.2}Chapter goals}{473}{section.15.2}
\contentsline {section}{\numberline {15.3}Hamiltonian Monte Carlo as a sledge}{474}{section.15.3}
\contentsline {subsection}{\numberline {15.3.1}Rewriting our problem in the language of Statistical Mechanics}{476}{subsection.15.3.1}
\contentsline {subsection}{\numberline {15.3.2}Choosing the potential and kinetic energy terms}{479}{subsection.15.3.2}
\contentsline {subsection}{\numberline {15.3.3}Simulating sledge movement in NLP space using the Leapfrog algorithm}{481}{subsection.15.3.3}
\contentsline {subsection}{\numberline {15.3.4}Approximating our path in a way that conserves volume}{483}{subsection.15.3.4}
\contentsline {subsection}{\numberline {15.3.5}Revising our acceptance rule}{484}{subsection.15.3.5}
\contentsline {subsection}{\numberline {15.3.6}Putting it all together: the full HMC algorithm}{484}{subsection.15.3.6}
\contentsline {subsection}{\numberline {15.3.7}Competing with Random Walk Metropolis and Gibbs}{484}{subsection.15.3.7}
\contentsline {section}{\numberline {15.4}Avoiding manual labour: the No-U-turn sampler}{484}{section.15.4}
\contentsline {section}{\numberline {15.5}Riemannian MCMC}{484}{section.15.5}
\contentsline {section}{\numberline {15.6}Multimodality}{484}{section.15.6}
\contentsline {chapter}{\numberline {16}Stan}{485}{chapter.16}
\contentsline {section}{\numberline {16.1}Chapter mission statement}{485}{section.16.1}
\contentsline {section}{\numberline {16.2}Chapter goals}{485}{section.16.2}
\contentsline {section}{\numberline {16.3}Why Stan, and how to get it.}{487}{section.16.3}
\contentsline {subsection}{\numberline {16.3.1}When to use black-box MCMC rather than coding up the algorithms yourself}{487}{subsection.16.3.1}
\contentsline {subsection}{\numberline {16.3.2}What is Stan?}{488}{subsection.16.3.2}
\contentsline {subsection}{\numberline {16.3.3}Why choose Stan?}{489}{subsection.16.3.3}
\contentsline {section}{\numberline {16.4}Getting setup with Stan using RStan}{490}{section.16.4}
\contentsline {paragraph}{R}{491}{section*.318}
\contentsline {paragraph}{R Studio}{491}{section*.319}
\contentsline {paragraph}{Toolchain}{491}{section*.320}
\contentsline {paragraph}{RStan}{491}{section*.321}
\contentsline {section}{\numberline {16.5}Our first words in Stan}{492}{section.16.5}
\contentsline {subsection}{\numberline {16.5.1}The building blocks of a Stan program}{492}{subsection.16.5.1}
\contentsline {subsubsection}{Writing a Stan programme}{493}{section*.322}
\contentsline {subsubsection}{data}{493}{section*.323}
\contentsline {subsubsection}{parameters}{495}{section*.324}
\contentsline {subsubsection}{model}{496}{section*.325}
\contentsline {paragraph}{Sampling statements.}{496}{section*.326}
\contentsline {paragraph}{Vectorisation.}{498}{section*.327}
\contentsline {paragraph}{Probability distributions.}{498}{section*.328}
\contentsline {subsection}{\numberline {16.5.2}Executing a Stan programme in R}{499}{subsection.16.5.2}
\contentsline {subsection}{\numberline {16.5.3}Interpreting the results of a Stan MCMC run}{500}{subsection.16.5.3}
\contentsline {paragraph}{Shiny Stan.}{502}{section*.329}
\contentsline {subsection}{\numberline {16.5.4}The other blocks}{503}{subsection.16.5.4}
\contentsline {subsubsection}{generated quantities}{503}{section*.330}
\contentsline {subsubsection}{functions}{505}{section*.331}
\contentsline {subsubsection}{transformed parameters}{508}{section*.332}
\contentsline {subsubsection}{transformed data}{509}{section*.333}
\contentsline {subsection}{\numberline {16.5.5}Efficiency and memory considerations for each block}{509}{subsection.16.5.5}
\contentsline {subsection}{\numberline {16.5.6}Loops and conditional statements}{512}{subsection.16.5.6}
\contentsline {subsection}{\numberline {16.5.7}Essential Stan reading}{513}{subsection.16.5.7}
\contentsline {subsubsection}{Accessing elements using indices}{513}{section*.334}
\contentsline {subsubsection}{Passing ragged arrays to Stan}{514}{section*.335}
\contentsline {subsubsection}{Using Stan to generate independent samples}{516}{section*.336}
\contentsline {subsubsection}{Translate and compile a model for use later on}{518}{section*.337}
\contentsline {subsubsection}{Custom PDFs: no problem with Stan}{519}{section*.338}
\contentsline {subsubsection}{Calculating WAIC, LOO-CV and other measures}{522}{section*.339}
\contentsline {paragraph}{WAIC and LOO-CV.}{523}{section*.340}
\contentsline {paragraph}{Explicit cross-validation.}{526}{section*.341}
\contentsline {subsubsection}{Jacobians: when do we need these?}{529}{section*.342}
\contentsline {subsubsection}{Marginalising out discrete parameters}{534}{section*.343}
\contentsline {section}{\numberline {16.6}What to do when things go wrong}{539}{section.16.6}
\contentsline {subsection}{\numberline {16.6.1}Coding errors}{540}{subsection.16.6.1}
\contentsline {subsubsection}{Debugging through fake data simulations}{542}{section*.344}
\contentsline {subsubsection}{Debugging by print}{544}{section*.345}
\contentsline {subsection}{\numberline {16.6.2}Sampling errors}{544}{subsection.16.6.2}
\contentsline {subsubsection}{The real power of fake data}{545}{section*.346}
\contentsline {subsubsection}{Slow convergence indicated by $\mathaccentV {hat}05E{R} > 1.1$.}{545}{section*.347}
\contentsline {subsubsection}{Divergent iterations}{548}{section*.348}
\contentsline {subsubsection}{Tree depth exceeding maximum}{552}{section*.349}
\contentsline {section}{\numberline {16.7}How to get further help, if you need it}{552}{section.16.7}
\contentsline {section}{\numberline {16.8}Chapter summary}{553}{section.16.8}
\contentsline {section}{\numberline {16.9}Chapter outcomes}{554}{section.16.9}
\contentsline {part}{V\hspace {1em}Regression analysis and hierarchical models}{555}{part.5}
\contentsline {section}{\numberline {16.10}Part mission statement}{557}{section.16.10}
\contentsline {section}{\numberline {16.11}Part goals}{557}{section.16.11}
\contentsline {chapter}{\numberline {17}Hierarchical models}{559}{chapter.17}
\contentsline {section}{\numberline {17.1}The spectrum from pooled to heterogeneous}{559}{section.17.1}
\contentsline {subsection}{\numberline {17.1.1}The logic and benefits of partial pooling}{559}{subsection.17.1.1}
\contentsline {subsection}{\numberline {17.1.2}Shrinkage towards the mean}{559}{subsection.17.1.2}
\contentsline {section}{\numberline {17.2}Meta analysis example: simple}{559}{section.17.2}
\contentsline {section}{\numberline {17.3}The importance of fake data simulation for complex models}{559}{section.17.3}
\contentsline {subsection}{\numberline {17.3.1}The importance of making 'good' fake data}{559}{subsection.17.3.1}
\contentsline {subsubsection}{Non-centred parameterisations}{559}{section*.350}
\contentsline {chapter}{\numberline {18}Linear regression models}{561}{chapter.18}
\contentsline {section}{\numberline {18.1}Choosing covariates: model averaging}{561}{section.18.1}
\contentsline {chapter}{\numberline {19}Generalised linear models}{563}{chapter.19}
\contentsline {section}{\numberline {19.1}Malarial example of complex meta-analysis}{563}{section.19.1}
\contentsline {section}{\numberline {19.2}Practical computational inference}{563}{section.19.2}
\contentsline {subsection}{\numberline {19.2.1}The importance of pre-simulation MLE}{563}{subsection.19.2.1}
\contentsline {subsection}{\numberline {19.2.2}Fake data simulation}{563}{subsection.19.2.2}
\contentsline {subsection}{\numberline {19.2.3}Poor convergence}{563}{subsection.19.2.3}
\contentsline {section}{\numberline {19.3}Posterior predictive checks}{563}{section.19.3}
