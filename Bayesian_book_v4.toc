\contentsline {chapter}{\numberline {1}How to best use this book}{19}{chapter.1}
\contentsline {section}{\numberline {1.1}The purpose of this book}{19}{section.1.1}
\contentsline {section}{\numberline {1.2}Who is this book for?}{21}{section.1.2}
\contentsline {section}{\numberline {1.3}Pre-requisites}{21}{section.1.3}
\contentsline {section}{\numberline {1.4}Book outline}{22}{section.1.4}
\contentsline {section}{\numberline {1.5}Route planner - suggested journeys through Bayesland}{24}{section.1.5}
\contentsline {section}{\numberline {1.6}Video}{25}{section.1.6}
\contentsline {section}{\numberline {1.7}Interactive elements}{26}{section.1.7}
\contentsline {section}{\numberline {1.8}Interactive problem sets}{26}{section.1.8}
\contentsline {section}{\numberline {1.9}Code}{26}{section.1.9}
\contentsline {section}{\numberline {1.10}R, Stan and JAGS}{27}{section.1.10}
\contentsline {section}{\numberline {1.11}Why don't more people use Bayesian statistics?}{28}{section.1.11}
\contentsline {section}{\numberline {1.12}What are the tangible (non-academic) benefits of Bayesian statistics?}{29}{section.1.12}
\contentsline {section}{\numberline {1.13}Suggested further reading}{30}{section.1.13}
\contentsline {part}{I\hspace {1em}An introduction to Bayesian inference}{31}{part.1}
\contentsline {section}{\numberline {1.14}Part mission statement}{33}{section.1.14}
\contentsline {section}{\numberline {1.15}Part goals}{33}{section.1.15}
\contentsline {chapter}{\numberline {2}The subjective worlds of Frequentist and Bayesian statistics}{35}{chapter.2}
\contentsline {section}{\numberline {2.1}Chapter mission statement}{35}{section.2.1}
\contentsline {section}{\numberline {2.2}Chapter goals}{35}{section.2.2}
\contentsline {section}{\numberline {2.3}Bayes' rule - allowing us to go from the effect back to its cause}{36}{section.2.3}
\contentsline {section}{\numberline {2.4}The purpose of statistical inference}{37}{section.2.4}
\contentsline {section}{\numberline {2.5}The world according to Frequentists}{38}{section.2.5}
\contentsline {section}{\numberline {2.6}The world according to Bayesians}{40}{section.2.6}
\contentsline {section}{\numberline {2.7}Frequentist and Bayesian inference}{41}{section.2.7}
\contentsline {subsection}{\numberline {2.7.1}The Frequentist and Bayesian murder trials}{42}{subsection.2.7.1}
\contentsline {subsection}{\numberline {2.7.2}Radio control towers: example}{43}{subsection.2.7.2}
\contentsline {section}{\numberline {2.8}Bayesian inference via Bayes' rule}{44}{section.2.8}
\contentsline {subsection}{\numberline {2.8.1}Likelihoods}{45}{subsection.2.8.1}
\contentsline {subsection}{\numberline {2.8.2}Priors}{46}{subsection.2.8.2}
\contentsline {subsection}{\numberline {2.8.3}The denominator}{47}{subsection.2.8.3}
\contentsline {subsection}{\numberline {2.8.4}Posteriors: the goal of Bayesian inference}{47}{subsection.2.8.4}
\contentsline {section}{\numberline {2.9}Implicit vs Explicit subjectivity}{49}{section.2.9}
\contentsline {section}{\numberline {2.10}Chapter summary}{51}{section.2.10}
\contentsline {section}{\numberline {2.11}Chapter outcomes}{51}{section.2.11}
\contentsline {section}{\numberline {2.12}Problem set}{51}{section.2.12}
\contentsline {subsection}{\numberline {2.12.1}The deterministic nature of random coin throwing.}{51}{subsection.2.12.1}
\contentsline {subsubsection}{Suppose that all combinations of angles and heights are equally likely to be chosen. What is the probability that the coin lands on heads?}{52}{section*.9}
\contentsline {subsubsection}{What are the new probabilities that the coin lands heads-up?}{53}{section*.10}
\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at an angle of 45 degrees. What is the probability that the coin lands heads-up?}{53}{section*.11}
\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at a height of 0.2m. What is the probability that the coin lands heads-up?}{53}{section*.12}
\contentsline {subsubsection}{If we constrained the angle and height to be fixed, what would happen in repetitions of the same experiment?}{53}{section*.13}
\contentsline {subsubsection}{In light of the previous question, comment on the Frequentist assumption of \textit {exact repetitions} of a given experiment.}{53}{section*.14}
\contentsline {subsection}{\numberline {2.12.2}Model choice}{53}{subsection.2.12.2}
\contentsline {subsubsection}{Fit a linear regression model using classical least squares. How reasonable is the fit?}{54}{section*.16}
\contentsline {subsubsection}{Fit a quintic (powers up to the 5th) model to the data. How does its fit compare to that of the linear model?}{54}{section*.17}
\contentsline {subsubsection}{Fit a linear regression to each of the data sets, and similarly for the quintic model. Which of these performs best?}{54}{section*.18}
\contentsline {subsubsection}{Using the fits from the first part of this question, compare the performance of the linear regression model, with that of the quintic model.}{54}{section*.19}
\contentsline {subsubsection}{Which of the two models do you prefer, and why?}{54}{section*.20}
\contentsline {subsubsection}{If you then found out that the data were years of education (x), and salary in \$000s (y). Which model would you favour?}{54}{section*.21}
\contentsline {section}{\numberline {2.13}Appendix}{54}{section.2.13}
\contentsline {subsection}{\numberline {2.13.1}The Frequentist and Bayesian murder trial}{54}{subsection.2.13.1}
\contentsline {chapter}{\numberline {3}Probability - the nuts and bolts of Bayesian inference}{57}{chapter.3}
\contentsline {section}{\numberline {3.1}Chapter mission statement}{57}{section.3.1}
\contentsline {section}{\numberline {3.2}Chapter goals}{57}{section.3.2}
\contentsline {section}{\numberline {3.3}Probability distributions: helping us explicitly state our ignorance}{58}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}What makes a probability distribution \textit {valid}?}{58}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Probabilities vs probability density : interpreting discrete and continuous probability distributions}{60}{subsection.3.3.2}
\contentsline {subsubsection}{A (wet) river crossing}{63}{section*.24}
\contentsline {subsubsection}{Probability zero vs impossibility}{66}{section*.26}
\contentsline {subsubsection}{The good news: Bayes' rule doesn't distinguish between probabilities and probability densities}{67}{section*.27}
\contentsline {subsection}{\numberline {3.3.3}Mean and variance of distributions}{68}{subsection.3.3.3}
\contentsline {subsection}{\numberline {3.3.4}Generalising probability distributions to two dimensions}{72}{subsection.3.3.4}
\contentsline {subsubsection}{Horses for courses: a 2-dimensional discrete probability example}{72}{section*.31}
\contentsline {subsubsection}{Foot length and literacy: a 2-dimensional continuous probability example}{74}{section*.33}
\contentsline {subsection}{\numberline {3.3.5}Marginal distributions}{74}{subsection.3.3.5}
\contentsline {subsubsection}{Venn diagrams}{78}{section*.36}
\contentsline {subsection}{\numberline {3.3.6}Conditional distributions}{78}{subsection.3.3.6}
\contentsline {section}{\numberline {3.4}Higher dimensional probability densities: no harder than 2-D, just looks it!}{81}{section.3.4}
\contentsline {section}{\numberline {3.5}Independence}{84}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Conditional independence}{87}{subsection.3.5.1}
\contentsline {section}{\numberline {3.6}Central Limit Theorems}{88}{section.3.6}
\contentsline {section}{\numberline {3.7}The Bayesian formula}{91}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}The intuition behind the formula}{92}{subsection.3.7.1}
\contentsline {subsection}{\numberline {3.7.2}Breast cancer screeing}{92}{subsection.3.7.2}
\contentsline {section}{\numberline {3.8}The Bayesian inference process from the Bayesian formula}{93}{section.3.8}
\contentsline {section}{\numberline {3.9}Chapter summary}{94}{section.3.9}
\contentsline {section}{\numberline {3.10}Chapter outcomes}{95}{section.3.10}
\contentsline {section}{\numberline {3.11}Problem set}{95}{section.3.11}
\contentsline {subsection}{\numberline {3.11.1}The expected returns of a derivative}{95}{subsection.3.11.1}
\contentsline {subsubsection}{What are the expected returns of the stock?}{95}{section*.46}
\contentsline {subsubsection}{What are the variance in returns?}{95}{section*.47}
\contentsline {subsubsection}{Would you expect the return to be equal to the mean of the original stock squared? If not, why not?}{96}{section*.48}
\contentsline {subsubsection}{What would be a fair price to pay for this derivative?}{96}{section*.49}
\contentsline {subsubsection}{What is a fair price to pay for this stock?}{96}{section*.50}
\contentsline {subsubsection}{Which asset is more risky, $D_t$ or $R_t$?}{96}{section*.51}
\contentsline {subsubsection}{Use \textit {method of moments} to estimate $\mu $ and $\sigma $. Note: this requires use of a computer with mathematical software, as analytic solutions aren't possible.}{96}{section*.52}
\contentsline {subsubsection}{Compare the estimated model with the data.}{96}{section*.53}
\contentsline {subsubsection}{Is the model a reasonable approximation to the data generating process?}{96}{section*.54}
\contentsline {subsubsection}{If not, suggest a better alternative.}{96}{section*.55}
\contentsline {subsection}{\numberline {3.11.2}The boy or girl paradox}{96}{subsection.3.11.2}
\contentsline {subsection}{\numberline {3.11.3}The Bayesian game show}{97}{subsection.3.11.3}
\contentsline {subsection}{\numberline {3.11.4}Blood doping}{97}{subsection.3.11.4}
\contentsline {subsubsection}{What is the probability that a professional cyclist wins a race?}{98}{section*.56}
\contentsline {subsubsection}{What is the probability that a cyclist wins a race given that they have cheated?}{98}{section*.57}
\contentsline {subsubsection}{What is the probability that a cyclist is cheating given that he wins?}{98}{section*.58}
\contentsline {part}{II\hspace {1em}Understanding the Bayesian formula}{99}{part.2}
\contentsline {section}{\numberline {3.12}Part mission statement}{101}{section.3.12}
\contentsline {section}{\numberline {3.13}Part goals}{101}{section.3.13}
\contentsline {chapter}{\numberline {4}The posterior - the goal of Bayesian inference}{103}{chapter.4}
\contentsline {section}{\numberline {4.1}Chapter Mission statement}{103}{section.4.1}
\contentsline {section}{\numberline {4.2}Chapter goals}{103}{section.4.2}
\contentsline {section}{\numberline {4.3}Expressing uncertainty through the posterior probability distribution}{104}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Bayesian coastguard: introducing the prior and the posterior}{107}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}Bayesian statistics: updating our pre-analysis uncertainty}{107}{subsection.4.3.2}
\contentsline {subsection}{\numberline {4.3.3}Do parameters actually exist and have a point value?}{109}{subsection.4.3.3}
\contentsline {subsection}{\numberline {4.3.4}Failings of the Frequentist confidence interval}{110}{subsection.4.3.4}
\contentsline {subsection}{\numberline {4.3.5}Credible intervals}{113}{subsection.4.3.5}
\contentsline {subsubsection}{Treasure hunting: The central posterior and highest density intervals}{114}{section*.66}
\contentsline {subsection}{\numberline {4.3.6}Reconciling the difference between confidence and credible intervals}{115}{subsection.4.3.6}
\contentsline {subsubsection}{The interval ENIGMA}{116}{section*.68}
\contentsline {section}{\numberline {4.4}Point parameter estimates}{119}{section.4.4}
\contentsline {section}{\numberline {4.5}Prediction using predictive distributions}{120}{section.4.5}
\contentsline {subsection}{\numberline {4.5.1}Example: number of Republican voters within a sample}{121}{subsection.4.5.1}
\contentsline {subsection}{\numberline {4.5.2}Example: interest rate hedging}{124}{subsection.4.5.2}
\contentsline {section}{\numberline {4.6}Model comparison using the posterior}{128}{section.4.6}
\contentsline {subsection}{\numberline {4.6.1}Example: epidemiologist comparison}{131}{subsection.4.6.1}
\contentsline {subsection}{\numberline {4.6.2}Example: customer footfall}{132}{subsection.4.6.2}
\contentsline {section}{\numberline {4.7}Model comparison through posterior predictive checks}{134}{section.4.7}
\contentsline {subsection}{\numberline {4.7.1}Example: stock returns}{135}{subsection.4.7.1}
\contentsline {section}{\numberline {4.8}Chapter summary}{136}{section.4.8}
\contentsline {section}{\numberline {4.9}Chapter outcomes}{137}{section.4.9}
\contentsline {section}{\numberline {4.10}Problem set}{137}{section.4.10}
\contentsline {subsection}{\numberline {4.10.1}The lesser evil}{137}{subsection.4.10.1}
\contentsline {subsubsection}{Which of the above loss functions do you think is most appropriate, and why?}{138}{section*.81}
\contentsline {subsubsection}{Which loss function might you choose to be most robust to \textit {any} situation?}{138}{section*.82}
\contentsline {subsubsection}{Following from the previous point, which measure of posterior centrality might you choose?}{138}{section*.83}
\contentsline {subsection}{\numberline {4.10.2}Google word search prediction}{138}{subsection.4.10.2}
\contentsline {subsubsection}{Find the minimum-coverage confidence intervals of topics that exceed 70\%.}{139}{section*.85}
\contentsline {subsubsection}{Find most narrow credible intervals for topics that exceed 70\%.}{139}{section*.86}
\contentsline {subsubsection}{Topic search volumes}{139}{section*.87}
\contentsline {subsubsection}{Three-letter search volumes}{139}{section*.89}
\contentsline {subsection}{\numberline {4.10.3}Prior and posterior predictive example (with PPCs maybe)}{139}{subsection.4.10.3}
\contentsline {section}{\numberline {4.11}Appendix}{140}{section.4.11}
\contentsline {subsection}{\numberline {4.11.1}The interval ENIGMA - explained in full}{140}{subsection.4.11.1}
\contentsline {chapter}{\numberline {5}Likelihoods}{141}{chapter.5}
\contentsline {section}{\numberline {5.1}Chapter Mission statement}{141}{section.5.1}
\contentsline {section}{\numberline {5.2}Chapter goals}{141}{section.5.2}
\contentsline {section}{\numberline {5.3}What is a likelihood?}{142}{section.5.3}
\contentsline {section}{\numberline {5.4}Why use 'likelihood' rather than 'probability'?}{144}{section.5.4}
\contentsline {section}{\numberline {5.5}What are models and why do we need them?}{148}{section.5.5}
\contentsline {section}{\numberline {5.6}How to choose an appropriate likelihood?}{149}{section.5.6}
\contentsline {subsection}{\numberline {5.6.1}A likelihood model for an individual's disease status}{150}{subsection.5.6.1}
\contentsline {subsection}{\numberline {5.6.2}A likelihood model for disease prevalence of a group}{152}{subsection.5.6.2}
\contentsline {subsection}{\numberline {5.6.3}The intelligence of a group of people}{157}{subsection.5.6.3}
\contentsline {section}{\numberline {5.7}Exchangeability vs random sampling}{159}{section.5.7}
\contentsline {section}{\numberline {5.8}The subjectivity of model choice}{161}{section.5.8}
\contentsline {section}{\numberline {5.9}Maximum likelihood - a short introduction}{161}{section.5.9}
\contentsline {subsection}{\numberline {5.9.1}Estimating disease prevalence}{162}{subsection.5.9.1}
\contentsline {subsection}{\numberline {5.9.2}Estimating the mean and variance in intelligence scores}{165}{subsection.5.9.2}
\contentsline {subsection}{\numberline {5.9.3}Maximum likelihood in simple steps}{166}{subsection.5.9.3}
\contentsline {section}{\numberline {5.10}Frequentist inference in Maximum Likelihood}{167}{section.5.10}
\contentsline {section}{\numberline {5.11}Chapter summary}{167}{section.5.11}
\contentsline {section}{\numberline {5.12}Chapter outcomes}{169}{section.5.12}
\contentsline {section}{\numberline {5.13}Problem set}{169}{section.5.13}
\contentsline {subsection}{\numberline {5.13.1}Blog blues.}{169}{subsection.5.13.1}
\contentsline {subsubsection}{What assumptions might you make about the first-time visits?}{170}{section*.101}
\contentsline {subsubsection}{What model might be appropriate to model the time between visits?}{170}{section*.102}
\contentsline {subsubsection}{Algebraically derive an estimate of the mean number of visits per hour}{170}{section*.103}
\contentsline {subsubsection}{Data analysis:}{170}{section*.104}
\contentsline {subsubsection}{Graph the log likelihood near your estimated value. What does this show? Why don't we plot the likelihood?}{170}{section*.105}
\contentsline {subsubsection}{Estimate confidence intervals around your parameter.}{170}{section*.106}
\contentsline {subsubsection}{What is the probability that you will wait:}{170}{section*.107}
\contentsline {subsubsection}{Evaluate your model.}{170}{section*.108}
\contentsline {subsubsection}{What alternative models might be useful here?}{170}{section*.109}
\contentsline {subsubsection}{What are the assumptions behind these models?}{170}{section*.110}
\contentsline {subsubsection}{Estimate the parameters of your new model.}{170}{section*.111}
\contentsline {subsubsection}{Use your new model to estimate the probability that you will wait:}{170}{section*.112}
\contentsline {subsubsection}{Hints: the exponential is to the poisson model, what the ? is to the negative binomial.}{171}{section*.113}
\contentsline {subsection}{\numberline {5.13.2}Violent crime counts in New York counties}{171}{subsection.5.13.2}
\contentsline {subsubsection}{Graph the violent crime account against population. What type of relationship does this suggest?}{171}{section*.114}
\contentsline {subsubsection}{A simple model}{171}{section*.115}
\contentsline {subsubsection}{What are the assumptions of this model?}{172}{section*.116}
\contentsline {subsubsection}{Estimate the parameter $\theta $ from the data. What does this parameter represent?}{172}{section*.117}
\contentsline {subsubsection}{Do these assumptions seem realistic?}{172}{section*.118}
\contentsline {subsubsection}{Estimate a measure of uncertainty in your estimates.}{172}{section*.119}
\contentsline {subsubsection}{Evaluate the performance of your model.}{172}{section*.120}
\contentsline {subsubsection}{A better model}{172}{section*.121}
\contentsline {subsubsection}{Why is this model better?}{172}{section*.122}
\contentsline {subsubsection}{What factors might affect $\theta _i$?}{172}{section*.123}
\contentsline {subsubsection}{Write down a new model specification taking into account the previous point.}{172}{section*.124}
\contentsline {subsubsection}{Estimate the parameters of this new specification.}{172}{section*.125}
\contentsline {subsubsection}{How does this new model compare to the previous iteration?}{172}{section*.126}
\contentsline {subsubsection}{What alternative specifications might be worth attempting?}{172}{section*.127}
\contentsline {subsection}{\numberline {5.13.3}Monte Carlo evaluation of the performance of MLE in R}{172}{subsection.5.13.3}
\contentsline {subsection}{\numberline {5.13.4}The sample mean as MLE}{173}{subsection.5.13.4}
\contentsline {chapter}{\numberline {6}Priors}{175}{chapter.6}
\contentsline {section}{\numberline {6.1}Chapter Mission statement}{175}{section.6.1}
\contentsline {section}{\numberline {6.2}Chapter goals}{175}{section.6.2}
\contentsline {section}{\numberline {6.3}What are priors, and what do they represent?}{176}{section.6.3}
\contentsline {section}{\numberline {6.4}Why do we need priors at all?}{178}{section.6.4}
\contentsline {section}{\numberline {6.5}Why don't we just normalise likelihood by choosing a unity prior?}{179}{section.6.5}
\contentsline {section}{\numberline {6.6}The explicit subjectivity of priors}{182}{section.6.6}
\contentsline {section}{\numberline {6.7}Combining a prior and likelihood to form a posterior}{182}{section.6.7}
\contentsline {subsection}{\numberline {6.7.1}The Goldfish game}{182}{subsection.6.7.1}
\contentsline {subsection}{\numberline {6.7.2}Disease proportions revisited}{185}{subsection.6.7.2}
\contentsline {section}{\numberline {6.8}Constructing priors}{188}{section.6.8}
\contentsline {subsection}{\numberline {6.8.1}Vague priors}{188}{subsection.6.8.1}
\contentsline {subsection}{\numberline {6.8.2}Informative priors}{192}{subsection.6.8.2}
\contentsline {subsection}{\numberline {6.8.3}The numerator of Bayes' rule determines the shape}{194}{subsection.6.8.3}
\contentsline {subsection}{\numberline {6.8.4}Eliciting priors}{194}{subsection.6.8.4}
\contentsline {section}{\numberline {6.9}A strong model is less sensitive to prior choice}{195}{section.6.9}
\contentsline {subsection}{\numberline {6.9.1}Caveat: overly-zealous priors \textit {will} affect the posterior}{197}{subsection.6.9.1}
\contentsline {section}{\numberline {6.10}Chapter summary}{198}{section.6.10}
\contentsline {section}{\numberline {6.11}Chapter outcomes}{200}{section.6.11}
\contentsline {section}{\numberline {6.12}Problem set}{201}{section.6.12}
\contentsline {subsection}{\numberline {6.12.1}Counting sheep}{201}{subsection.6.12.1}
\contentsline {subsubsection}{What likelihood model might you use here?}{201}{section*.142}
\contentsline {subsubsection}{What are the assumptions underpinning this model?}{201}{section*.143}
\contentsline {subsubsection}{Introducing a prior}{201}{section*.144}
\contentsline {subsubsection}{Which of the previous priors is most uninformative?}{202}{section*.145}
\contentsline {subsubsection}{Suppose that you observe 10 sheep jumping over the fence, calculate the posterior distribution for each of the different priors using your chosen likelihood.}{202}{section*.146}
\contentsline {subsubsection}{For what numbers of jumping sheep would one of your posteriors run into problems?}{202}{section*.147}
\contentsline {subsection}{\numberline {6.12.2}Investigating priors through US elections}{202}{subsection.6.12.2}
\contentsline {subsection}{\numberline {6.12.3}Choosing prior distributions.}{203}{subsection.6.12.3}
\contentsline {subsubsection}{If we choose a prior distribution that sets $p(log(\sigma ))=1$, what does this imply about the distribution of $p(\sigma )$?}{203}{section*.148}
\contentsline {subsubsection}{For a parameter $\theta $, which choice of prior will leave it invariant under transformations?}{203}{section*.149}
\contentsline {subsubsection}{Pre-experimental data prior setting}{203}{section*.150}
\contentsline {subsection}{\numberline {6.12.4}Expert data prior example}{203}{subsection.6.12.4}
\contentsline {subsection}{\numberline {6.12.5}Data analysis example showing the declining importance of prior as data set increases in size}{203}{subsection.6.12.5}
\contentsline {section}{\numberline {6.13}Appendix}{203}{section.6.13}
\contentsline {subsection}{\numberline {6.13.1}Bayes' rule for the urn}{203}{subsection.6.13.1}
\contentsline {subsection}{\numberline {6.13.2}The probabilities of having a disease}{204}{subsection.6.13.2}
\contentsline {chapter}{\numberline {7}The devil's in the denominator}{205}{chapter.7}
\contentsline {section}{\numberline {7.1}Chapter mission}{205}{section.7.1}
\contentsline {section}{\numberline {7.2}Chapter goals}{205}{section.7.2}
\contentsline {section}{\numberline {7.3}An introduction to the denominator}{206}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}The denominator as a normalising factor}{206}{subsection.7.3.1}
\contentsline {subsection}{\numberline {7.3.2}Example: disease}{207}{subsection.7.3.2}
\contentsline {subsection}{\numberline {7.3.3}Example: the proportion of people who vote the Conservatives}{209}{subsection.7.3.3}
\contentsline {subsection}{\numberline {7.3.4}The denominator as a probability}{211}{subsection.7.3.4}
\contentsline {subsection}{\numberline {7.3.5}Using the denominator to choose between competing models}{213}{subsection.7.3.5}
\contentsline {section}{\numberline {7.4}The difficulty with the denominator}{218}{section.7.4}
\contentsline {subsection}{\numberline {7.4.1}Multi-parameter discrete model example: the comorbidity between depression and anxiety}{218}{subsection.7.4.1}
\contentsline {subsection}{\numberline {7.4.2}Continuous multi-parameter example: mean and variance of IQ}{221}{subsection.7.4.2}
\contentsline {section}{\numberline {7.5}How to dispense with the difficulty: Bayesian computation}{225}{section.7.5}
\contentsline {section}{\numberline {7.6}Chapter summary}{227}{section.7.6}
\contentsline {section}{\numberline {7.7}Chapter outcomes}{227}{section.7.7}
\contentsline {section}{\numberline {7.8}Problem set}{228}{section.7.8}
\contentsline {subsection}{\numberline {7.8.1}New disease cases}{228}{subsection.7.8.1}
\contentsline {subsubsection}{What likelihood model might be appropriate here?}{228}{section*.158}
\contentsline {subsubsection}{What are the assumptions of this model? Are they appropriate here?}{228}{section*.159}
\contentsline {subsubsection}{A gamma prior}{228}{section*.160}
\contentsline {subsubsection}{Data}{228}{section*.161}
\contentsline {subsubsection}{Find the posterior distribution for the mean parameter $\lambda $}{229}{section*.162}
\contentsline {subsection}{\numberline {7.8.2}The comorbidity between depression, anxiety and psychosis}{229}{subsection.7.8.2}
\contentsline {subsubsection}{Calculate the probability that a patient is depressed.}{229}{section*.163}
\contentsline {subsubsection}{Calculate the probability that a patient is psychotic.}{229}{section*.164}
\contentsline {subsubsection}{What is the probability that a patient is psychotic given that they are depressed, and anxious?}{229}{section*.165}
\contentsline {subsubsection}{What is the probability that a patient is not psychotic if they are not depressed?}{229}{section*.166}
\contentsline {subsection}{\numberline {7.8.3}Finding mosquito larvae after rain}{229}{subsection.7.8.3}
\contentsline {subsubsection}{Find the mean of a $lognormal(d,1)$ distribution.}{230}{section*.168}
\contentsline {subsubsection}{Finding the mode}{230}{section*.169}
\contentsline {subsubsection}{Use a computer to graph the shape of the posterior distribution.}{230}{section*.170}
\contentsline {subsubsection}{Calculate the posterior (difficult).}{230}{section*.171}
\contentsline {section}{\numberline {7.9}Appendix}{230}{section.7.9}
\contentsline {part}{III\hspace {1em}Analytic Bayesian methods}{231}{part.3}
\contentsline {section}{\numberline {7.10}Part mission statement}{233}{section.7.10}
\contentsline {section}{\numberline {7.11}Part goals}{233}{section.7.11}
\contentsline {chapter}{\numberline {8}An introduction to distributions for the mathematically-un-inclined}{235}{chapter.8}
\contentsline {section}{\numberline {8.1}Chapter mission statement}{235}{section.8.1}
\contentsline {section}{\numberline {8.2}Chapter goals}{235}{section.8.2}
\contentsline {section}{\numberline {8.3}The interrelation among distributions}{236}{section.8.3}
\contentsline {section}{\numberline {8.4}Sampling distributions for likelihoods}{237}{section.8.4}
\contentsline {subsection}{\numberline {8.4.1}Bernoulli}{238}{subsection.8.4.1}
\contentsline {subsection}{\numberline {8.4.2}Binomial}{240}{subsection.8.4.2}
\contentsline {subsection}{\numberline {8.4.3}Poisson}{243}{subsection.8.4.3}
\contentsline {subsection}{\numberline {8.4.4}Negative binomial}{246}{subsection.8.4.4}
\contentsline {subsection}{\numberline {8.4.5}Beta-binomial}{250}{subsection.8.4.5}
\contentsline {subsection}{\numberline {8.4.6}Normal}{254}{subsection.8.4.6}
\contentsline {subsection}{\numberline {8.4.7}Student t}{256}{subsection.8.4.7}
\contentsline {subsection}{\numberline {8.4.8}Exponential}{259}{subsection.8.4.8}
\contentsline {subsection}{\numberline {8.4.9}Gamma}{262}{subsection.8.4.9}
\contentsline {subsection}{\numberline {8.4.10}Multinomial}{265}{subsection.8.4.10}
\contentsline {subsection}{\numberline {8.4.11}Multivariate normal and multivariate t}{269}{subsection.8.4.11}
\contentsline {section}{\numberline {8.5}Prior distributions}{273}{section.8.5}
\contentsline {subsection}{\numberline {8.5.1}Distributions for probabilities, proportions and percentages}{274}{subsection.8.5.1}
\contentsline {subsubsection}{Uniform}{274}{section*.207}
\contentsline {subsubsection}{Beta}{275}{section*.210}
\contentsline {subsubsection}{Logit-normal}{277}{section*.213}
\contentsline {subsubsection}{Dirichlet}{278}{section*.216}
\contentsline {subsection}{\numberline {8.5.2}Distributions for means and regression coefficients}{282}{subsection.8.5.2}
\contentsline {subsubsection}{Normal}{283}{section*.220}
\contentsline {subsubsection}{Student t}{284}{section*.222}
\contentsline {subsubsection}{Cauchy}{285}{section*.224}
\contentsline {subsubsection}{Multivariate normal, and multivariate t}{287}{section*.227}
\contentsline {subsection}{\numberline {8.5.3}Distributions for non-negative parameters}{289}{subsection.8.5.3}
\contentsline {subsubsection}{Gamma}{289}{section*.228}
\contentsline {subsubsection}{Half-Cauchy, inverse-gamma, inverse-$\chi ^2$ and uniform}{290}{section*.230}
\contentsline {subsubsection}{Log-normal}{293}{section*.233}
\contentsline {subsection}{\numberline {8.5.4}Distributions for covariance and correlation matrices}{295}{subsection.8.5.4}
\contentsline {subsubsection}{LKJ}{296}{section*.236}
\contentsline {subsubsection}{Wishart and inverse-Wishart}{302}{section*.240}
\contentsline {section}{\numberline {8.6}Choosing a likelihood made easy}{304}{section.8.6}
\contentsline {section}{\numberline {8.7}Table of common likelihoods, their uses, and reasonable priors}{305}{section.8.7}
\contentsline {section}{\numberline {8.8}Distributions of distributions, and mixtures - link to website, and relevance}{305}{section.8.8}
\contentsline {section}{\numberline {8.9}Chapter summary}{305}{section.8.9}
\contentsline {section}{\numberline {8.10}Chapter outcomes}{307}{section.8.10}
\contentsline {section}{\numberline {8.11}Problem set}{308}{section.8.11}
\contentsline {subsection}{\numberline {8.11.1}Drug trials}{308}{subsection.8.11.1}
\contentsline {subsection}{\numberline {8.11.2}100m results across countries}{309}{subsection.8.11.2}
\contentsline {subsection}{\numberline {8.11.3}Triangular representation of simplexes}{309}{subsection.8.11.3}
\contentsline {subsubsection}{Show that the triangular representation of a 3 parameter Dirichlet is correct.}{309}{section*.244}
\contentsline {subsection}{\numberline {8.11.4}Normal distribution with normal prior}{309}{subsection.8.11.4}
\contentsline {subsubsection}{Prove that the mean of the lower distribution is the same as that of the prior}{309}{section*.245}
\contentsline {subsubsection}{Find the mean of the lower distribution}{309}{section*.246}
\contentsline {subsubsection}{Prove that the distribution of the lower distribution is unconditionally normal}{309}{section*.247}
\contentsline {chapter}{\numberline {9}Conjugate priors and their place in Bayesian analysis}{311}{chapter.9}
\contentsline {section}{\numberline {9.1}Chapter mission statement}{311}{section.9.1}
\contentsline {section}{\numberline {9.2}Chapter goals}{311}{section.9.2}
\contentsline {section}{\numberline {9.3}What is a conjugate prior and why are they useful?}{312}{section.9.3}
\contentsline {section}{\numberline {9.4}Gamma-poisson example}{316}{section.9.4}
\contentsline {section}{\numberline {9.5}Normal example: extra}{317}{section.9.5}
\contentsline {section}{\numberline {9.6}Table of conjugate priors}{320}{section.9.6}
\contentsline {section}{\numberline {9.7}The lessons and limits of a conjugate analysis}{320}{section.9.7}
\contentsline {section}{\numberline {9.8}Chapter summary}{322}{section.9.8}
\contentsline {section}{\numberline {9.9}Chapter outcomes}{322}{section.9.9}
\contentsline {chapter}{\numberline {10}Evaluation of model fit and hypothesis testing}{325}{chapter.10}
\contentsline {section}{\numberline {10.1}Chapter mission statement}{325}{section.10.1}
\contentsline {section}{\numberline {10.2}Chapter goals}{325}{section.10.2}
\contentsline {section}{\numberline {10.3}Posterior predictive checks}{326}{section.10.3}
\contentsline {subsection}{\numberline {10.3.1}Recap - posterior predictive distributions}{326}{subsection.10.3.1}
\contentsline {subsection}{\numberline {10.3.2}Graphical PPCs and Bayesian p values}{328}{subsection.10.3.2}
\contentsline {subsubsection}{Amazon staff hiring}{329}{section*.252}
\contentsline {subsubsection}{Mosquito lifetime}{332}{section*.254}
\contentsline {subsubsection}{Snow days}{333}{section*.256}
\contentsline {subsubsection}{Word distributions within topics}{334}{section*.258}
\contentsline {subsubsection}{Bacterial infection}{337}{section*.260}
\contentsline {section}{\numberline {10.4}Why do we call it a \textit {p value}?}{340}{section.10.4}
\contentsline {section}{\numberline {10.5}Statistics measuring predictive accuracy: AIC, Deviance, WAIC and LOO}{341}{section.10.5}
\contentsline {subsection}{\numberline {10.5.1}Independent evaluation of a model, and overfitting}{341}{subsection.10.5.1}
\contentsline {subsection}{\numberline {10.5.2}How to measure a model's predictive capability?}{342}{subsection.10.5.2}
\contentsline {subsection}{\numberline {10.5.3}The ideal measure of a model's predictive accuracy}{344}{subsection.10.5.3}
\contentsline {subsection}{\numberline {10.5.4}Estimating out-of-sample predictive accuracy from in-sample data}{348}{subsection.10.5.4}
\contentsline {subsection}{\numberline {10.5.5}AIC}{349}{subsection.10.5.5}
\contentsline {subsection}{\numberline {10.5.6}DIC}{349}{subsection.10.5.6}
\contentsline {subsection}{\numberline {10.5.7}WAIC}{350}{subsection.10.5.7}
\contentsline {subsection}{\numberline {10.5.8}LOO}{355}{subsection.10.5.8}
\contentsline {subsection}{\numberline {10.5.9}A practical and short summary of measures of predictive accuracy in simple terms}{356}{subsection.10.5.9}
\contentsline {section}{\numberline {10.6}Choosing one model, or a number?}{358}{section.10.6}
\contentsline {section}{\numberline {10.7}Sensitivity analysis}{358}{section.10.7}
\contentsline {section}{\numberline {10.8}Chapter summary}{360}{section.10.8}
\contentsline {section}{\numberline {10.9}Chapter outcomes}{361}{section.10.9}
\contentsline {chapter}{\numberline {11}Making Bayesian analysis objective?}{363}{chapter.11}
\contentsline {section}{\numberline {11.1}Chapter mission statement}{363}{section.11.1}
\contentsline {section}{\numberline {11.2}Chapter goals}{363}{section.11.2}
\contentsline {section}{\numberline {11.3}The illusion of the 'uninformative' uniform prior}{364}{section.11.3}
\contentsline {section}{\numberline {11.4}Jeffreys' priors}{365}{section.11.4}
\contentsline {subsection}{\numberline {11.4.1}Another definition of 'uninformative'}{365}{subsection.11.4.1}
\contentsline {subsection}{\numberline {11.4.2}Being critical}{369}{subsection.11.4.2}
\contentsline {section}{\numberline {11.5}Reference priors}{370}{section.11.5}
\contentsline {section}{\numberline {11.6}Empirical Bayes}{372}{section.11.6}
\contentsline {section}{\numberline {11.7}A move towards weakly informative priors}{374}{section.11.7}
\contentsline {section}{\numberline {11.8}Chapter summary}{375}{section.11.8}
\contentsline {section}{\numberline {11.9}Chapter outcomes}{376}{section.11.9}
\contentsline {part}{IV\hspace {1em}A practical guide to doing real life Bayesian analysis: Computational Bayes}{379}{part.4}
\contentsline {section}{\numberline {11.10}Part mission statement}{381}{section.11.10}
\contentsline {section}{\numberline {11.11}Part goals}{381}{section.11.11}
\contentsline {chapter}{\numberline {12}Leaving conjugates behind: Markov Chain Monte Carlo}{383}{chapter.12}
\contentsline {section}{\numberline {12.1}Chapter mission statement}{383}{section.12.1}
\contentsline {section}{\numberline {12.2}Chapter goals}{383}{section.12.2}
\contentsline {section}{\numberline {12.3}The difficulty with real life Bayesian inference}{385}{section.12.3}
\contentsline {section}{\numberline {12.4}Discrete approximation to continuous posteriors}{386}{section.12.4}
\contentsline {section}{\numberline {12.5}The posterior through quadrature}{389}{section.12.5}
\contentsline {section}{\numberline {12.6}Integrating using independent samples: an introduction to Monte Carlo}{391}{section.12.6}
\contentsline {subsection}{\numberline {12.6.1}BSE revisited}{394}{subsection.12.6.1}
\contentsline {section}{\numberline {12.7}Why is independent sampling easier said than done?}{396}{section.12.7}
\contentsline {section}{\numberline {12.8}Ideal sampling from a posterior using only the unnormalised posterior}{399}{section.12.8}
\contentsline {subsection}{\numberline {12.8.1}The unnormalised posterior - a window onto the real deal}{401}{subsection.12.8.1}
\contentsline {section}{\numberline {12.9}Moving from independent to dependent sampling}{402}{section.12.9}
\contentsline {subsection}{\numberline {12.9.1}An analogy to MCMC: mapping mountains}{404}{subsection.12.9.1}
\contentsline {section}{\numberline {12.10}What's the catch with dependent samplers?}{405}{section.12.10}
\contentsline {section}{\numberline {12.11}Chapter summary}{408}{section.12.11}
\contentsline {section}{\numberline {12.12}Chapter outcomes}{410}{section.12.12}
\contentsline {section}{\numberline {12.13}Problem set}{410}{section.12.13}
\contentsline {subsection}{\numberline {12.13.1}Prove that the inverse-transform sampler works!}{410}{subsection.12.13.1}
\contentsline {subsection}{\numberline {12.13.2}Computationally investigate the die example}{410}{subsection.12.13.2}
\contentsline {subsection}{\numberline {12.13.3}Evaluate the mean and variance of the posterior for the BSE example}{410}{subsection.12.13.3}
\contentsline {chapter}{\numberline {13}The Metropolis algorithm}{411}{chapter.13}
\contentsline {section}{\numberline {13.1}Chapter mission statement}{411}{section.13.1}
\contentsline {section}{\numberline {13.2}Chapter goals}{411}{section.13.2}
\contentsline {section}{\numberline {13.3}Sustainable fishing}{413}{section.13.3}
\contentsline {section}{\numberline {13.4}Prospecting for gold}{414}{section.13.4}
\contentsline {section}{\numberline {13.5}Defining the Metropolis algorithm}{417}{section.13.5}
\contentsline {section}{\numberline {13.6}When does Metropolis work?}{421}{section.13.6}
\contentsline {subsection}{\numberline {13.6.1}Mathematical underpinnings of MCMC}{423}{subsection.13.6.1}
\contentsline {subsection}{\numberline {13.6.2}Desirable qualities of a Markov Chain}{424}{subsection.13.6.2}
\contentsline {subsection}{\numberline {13.6.3}Detailed balance}{424}{subsection.13.6.3}
\contentsline {subsection}{\numberline {13.6.4}The intuition behind the acceptance/rejection rule of Metropolis, and detailed balance}{427}{subsection.13.6.4}
\contentsline {subsection}{\numberline {13.6.5}Proving things we know to be true: Mathematics revisited}{429}{subsection.13.6.5}
\contentsline {section}{\numberline {13.7}Efficiency of convergence: the importance of choosing the right proposal scale}{430}{section.13.7}
\contentsline {subsection}{\numberline {13.7.1}MCMC as finding and exploring the typical set}{434}{subsection.13.7.1}
\contentsline {subsection}{\numberline {13.7.2}Speeding up convergence: tuning the proposal distribution}{435}{subsection.13.7.2}
\contentsline {subsection}{\numberline {13.7.3}Geometric convergence of Markov Chains}{437}{subsection.13.7.3}
\contentsline {section}{\numberline {13.8}Judging convergence}{438}{section.13.8}
\contentsline {subsection}{\numberline {13.8.1}Bob's bees in a house}{438}{subsection.13.8.1}
\contentsline {subsection}{\numberline {13.8.2}Using multiple chains to monitor convergence}{441}{subsection.13.8.2}
\contentsline {subsection}{\numberline {13.8.3}Using within- and between-chain variation to estimate convergence}{444}{subsection.13.8.3}
\contentsline {subsection}{\numberline {13.8.4}Two types of non-convergence}{446}{subsection.13.8.4}
\contentsline {subsection}{\numberline {13.8.5}Warm-up}{447}{subsection.13.8.5}
\contentsline {section}{\numberline {13.9}Effective sample size revisited}{449}{section.13.9}
\contentsline {subsection}{\numberline {13.9.1}Thinning samples to increase effective sample size}{451}{subsection.13.9.1}
\contentsline {section}{\numberline {13.10}Chapter summary}{452}{section.13.10}
\contentsline {section}{\numberline {13.11}Chapter outcomes}{454}{section.13.11}
\contentsline {section}{\numberline {13.12}Problem set}{455}{section.13.12}
\contentsline {subsection}{\numberline {13.12.1}Metropolis-Hastings example}{455}{subsection.13.12.1}
\contentsline {subsection}{\numberline {13.12.2}Find the optimal rejection probability = Gelman says 0.44 for unimodal}{455}{subsection.13.12.2}
\contentsline {subsection}{\numberline {13.12.3}Adaptive MCMC}{455}{subsection.13.12.3}
\contentsline {chapter}{\numberline {14}Gibbs sampling}{457}{chapter.14}
\contentsline {section}{\numberline {14.1}Chapter mission statement}{457}{section.14.1}
\contentsline {section}{\numberline {14.2}Chapter goals}{457}{section.14.2}
\contentsline {section}{\numberline {14.3}Back to prospecting for gold}{459}{section.14.3}
\contentsline {section}{\numberline {14.4}Defining the Gibbs algorithm}{461}{section.14.4}
\contentsline {subsection}{\numberline {14.4.1}Crime and punishment/unemployment}{464}{subsection.14.4.1}
\contentsline {section}{\numberline {14.5}Gibbs' earth: the intuition behind the Gibbs algorithm}{464}{section.14.5}
\contentsline {section}{\numberline {14.6}The benefits and problems with Gibbs and Random Walk Metropolis}{466}{section.14.6}
\contentsline {section}{\numberline {14.7}A change of parameters to speed up exploration}{468}{section.14.7}
\contentsline {section}{\numberline {14.8}Chapter summary}{469}{section.14.8}
\contentsline {section}{\numberline {14.9}Chapter outcomes}{470}{section.14.9}
\contentsline {section}{\numberline {14.10}Problem set}{470}{section.14.10}
\contentsline {subsection}{\numberline {14.10.1}Prove that the Gibbs sampler can be viewed as a case of Metropolis-Hastings}{470}{subsection.14.10.1}
\contentsline {subsection}{\numberline {14.10.2}Prove that a Gibbs sampler on bivariate normal has an effective sample size of half its true sample size}{470}{subsection.14.10.2}
\contentsline {section}{\numberline {14.11}Appendix}{470}{section.14.11}
\contentsline {subsection}{\numberline {14.11.1}Derivation of conditional distributions for multivariate normal}{470}{subsection.14.11.1}
\contentsline {subsection}{\numberline {14.11.2}Derive uncorrelated normals from bivariate normal}{470}{subsection.14.11.2}
\contentsline {chapter}{\numberline {15}Hamiltonian Monte Carlo}{471}{chapter.15}
\contentsline {section}{\numberline {15.1}Chapter mission statement}{471}{section.15.1}
\contentsline {section}{\numberline {15.2}Chapter goals}{471}{section.15.2}
\contentsline {section}{\numberline {15.3}Hamiltonian Monte Carlo as a sledge}{472}{section.15.3}
\contentsline {subsection}{\numberline {15.3.1}Rewriting our problem in the language of Statistical Mechanics}{474}{subsection.15.3.1}
\contentsline {subsection}{\numberline {15.3.2}Choosing the potential and kinetic energy terms}{477}{subsection.15.3.2}
\contentsline {subsection}{\numberline {15.3.3}Simulating sledge movement in NLP space using the Leapfrog algorithm}{479}{subsection.15.3.3}
\contentsline {subsection}{\numberline {15.3.4}Approximating our path in a way that conserves volume}{481}{subsection.15.3.4}
\contentsline {subsection}{\numberline {15.3.5}Revising our acceptance rule}{482}{subsection.15.3.5}
\contentsline {subsection}{\numberline {15.3.6}Putting it all together: the full HMC algorithm}{482}{subsection.15.3.6}
\contentsline {subsection}{\numberline {15.3.7}Competing with Random Walk Metropolis and Gibbs}{482}{subsection.15.3.7}
\contentsline {section}{\numberline {15.4}Avoiding manual labour: the No-U-turn sampler}{482}{section.15.4}
\contentsline {section}{\numberline {15.5}Riemannian MCMC}{482}{section.15.5}
\contentsline {section}{\numberline {15.6}Multimodality}{482}{section.15.6}
\contentsline {chapter}{\numberline {16}Stan}{483}{chapter.16}
\contentsline {section}{\numberline {16.1}Chapter mission statement}{483}{section.16.1}
\contentsline {section}{\numberline {16.2}Chapter goals}{483}{section.16.2}
\contentsline {section}{\numberline {16.3}Why Stan, and how to get it.}{485}{section.16.3}
\contentsline {subsection}{\numberline {16.3.1}When to use black-box MCMC rather than coding up the algorithms yourself}{485}{subsection.16.3.1}
\contentsline {subsection}{\numberline {16.3.2}What is Stan?}{486}{subsection.16.3.2}
\contentsline {subsection}{\numberline {16.3.3}Why choose Stan?}{487}{subsection.16.3.3}
\contentsline {section}{\numberline {16.4}Getting setup with Stan using RStan}{488}{section.16.4}
\contentsline {paragraph}{R}{489}{section*.318}
\contentsline {paragraph}{R Studio}{489}{section*.319}
\contentsline {paragraph}{Toolchain}{489}{section*.320}
\contentsline {paragraph}{RStan}{489}{section*.321}
\contentsline {section}{\numberline {16.5}Our first words in Stan}{490}{section.16.5}
\contentsline {subsection}{\numberline {16.5.1}The building blocks of a Stan program}{490}{subsection.16.5.1}
\contentsline {subsubsection}{Writing and executing a Stan programme}{491}{section*.322}
\contentsline {subsubsection}{data}{491}{section*.323}
\contentsline {subsubsection}{parameters}{492}{section*.324}
\contentsline {subsubsection}{model}{493}{section*.325}
\contentsline {subsubsection}{generated quantities}{494}{section*.326}
\contentsline {subsubsection}{functions}{494}{section*.327}
\contentsline {subsubsection}{transformed parameters}{494}{section*.328}
\contentsline {subsubsection}{transformed data}{494}{section*.329}
\contentsline {subsection}{\numberline {16.5.2}Diagnostics}{494}{subsection.16.5.2}
\contentsline {subsubsection}{$\mathaccentV {hat}05E{R}$ and effective sample size}{494}{section*.330}
\contentsline {subsubsection}{Divergent iterations}{494}{section*.331}
\contentsline {subsubsection}{Tree depth exceeding maximum}{494}{section*.332}
\contentsline {subsubsection}{Leapfrog steps}{494}{section*.333}
\contentsline {subsection}{\numberline {16.5.3}More complex models with array indexing}{494}{subsection.16.5.3}
\contentsline {subsection}{\numberline {16.5.4}Essential Stan reading}{494}{subsection.16.5.4}
\contentsline {subsubsection}{Packaging data}{495}{section*.334}
\contentsline {subsubsection}{Marginalising out discrete parameters}{495}{section*.335}
\contentsline {subsubsection}{Custom PDFs: no problem with Stan}{495}{section*.336}
\contentsline {subsubsection}{Calculating LOO-CV and WAIC}{495}{section*.337}
\contentsline {section}{\numberline {16.6}What to do when things go wrong}{495}{section.16.6}
\contentsline {part}{V\hspace {1em}Regression analysis and hierarchical models}{497}{part.5}
\contentsline {section}{\numberline {16.7}Part mission statement}{499}{section.16.7}
\contentsline {section}{\numberline {16.8}Part goals}{499}{section.16.8}
\contentsline {chapter}{\numberline {17}Hierarchical models}{501}{chapter.17}
\contentsline {section}{\numberline {17.1}The spectrum from pooled to heterogeneous}{501}{section.17.1}
\contentsline {subsection}{\numberline {17.1.1}The logic and benefits of partial pooling}{501}{subsection.17.1.1}
\contentsline {subsection}{\numberline {17.1.2}Shrinkage towards the mean}{501}{subsection.17.1.2}
\contentsline {section}{\numberline {17.2}Meta analysis example: simple}{501}{section.17.2}
\contentsline {section}{\numberline {17.3}The importance of fake data simulation for complex models}{501}{section.17.3}
\contentsline {subsection}{\numberline {17.3.1}The importance of making 'good' fake data}{501}{subsection.17.3.1}
\contentsline {chapter}{\numberline {18}Linear regression models}{503}{chapter.18}
\contentsline {section}{\numberline {18.1}Choosing covariates: model averaging}{503}{section.18.1}
\contentsline {chapter}{\numberline {19}Generalised linear models}{505}{chapter.19}
\contentsline {section}{\numberline {19.1}Malarial example of complex meta-analysis}{505}{section.19.1}
\contentsline {section}{\numberline {19.2}Practical computational inference}{505}{section.19.2}
\contentsline {subsection}{\numberline {19.2.1}The importance of pre-simulation MLE}{505}{subsection.19.2.1}
\contentsline {subsection}{\numberline {19.2.2}Fake data simulation}{505}{subsection.19.2.2}
\contentsline {subsection}{\numberline {19.2.3}Poor convergence}{505}{subsection.19.2.3}
\contentsline {section}{\numberline {19.3}Posterior predictive checks}{505}{section.19.3}
