\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}How to best use this book}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The purpose of this book}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The aim of this book.\relax }}{22}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:HowToUse_existingLiterature}{{1.1}{22}}
\citation{RLanguage}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Who is this book for?}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Pre-requisites}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Book outline}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Route planner - suggested journeys through Bayesland}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Video}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Interactive elements}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Interactive problem sets}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Code}{28}}
\citation{stan-software:2014}
\citation{plummer2003jags}
\@writefile{toc}{\contentsline {section}{\numberline {1.10}R, Stan and JAGS}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Why don't more people use Bayesian statistics?}{30}}
\citation{silver2012signal}
\@writefile{toc}{\contentsline {section}{\numberline {1.12}What are the tangible (non-academic) benefits of Bayesian statistics?}{31}}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {1.13}Suggested further reading}{32}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}An introduction to Bayesian inference}{33}}
\newlabel{part:BayesianInferenceIntro}{{I}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {1.14}Part mission statement}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {1.15}Part goals}{35}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The subjective worlds of Frequentist and Bayesian statistics}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:subjectiveFrequentistBayes}{{2}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Chapter mission statement}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Chapter goals}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Bayes' rule - allowing us to go from the effect back to its cause}{38}}
\newlabel{sec:Intro_bayesCauseEffect}{{2.3}{38}}
\newlabel{eq:Intro_crookedCasinoBayes}{{2.1}{38}}
\citation{mcgrayne2011theory}
\newlabel{eq:Intro_BayesRuleCauseEffect}{{2.3}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}The purpose of statistical inference}{39}}
\newlabel{sec:Intro_purposeStatisticalInference}{{2.4}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}The world according to Frequentists}{40}}
\newlabel{sec:Intro_FrequentistsWorld}{{2.5}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Frequentist and Bayesian approaches to probability.\relax }}{41}}
\newlabel{fig:Intro_FrequentistBayesProbability}{{2.1}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}The world according to Bayesians}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Frequentist and Bayesian inference}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Frequentist and Bayesian inference.\relax }}{44}}
\newlabel{fig:Intro_BayesVsFrequentist}{{2.2}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}The Frequentist and Bayesian murder trials}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Radio control towers: example}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Bayesian inference via Bayes' rule}{46}}
\newlabel{eq:Intro_bayesianFormula}{{2.5}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Likelihoods}{47}}
\newlabel{sec:Intro_likelihoods}{{2.8.1}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.2}Priors}{48}}
\newlabel{sec:Intro_priors}{{2.8.2}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Left: All values of a the bias of a coin are equally likely. Right: It is believed that the coin is most likely fair.\relax }}{49}}
\newlabel{fig:Intro_priors}{{2.3}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.3}The denominator}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.4}Posteriors: the goal of Bayesian inference}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The posterior distribution for, $\theta $, the bias of a coin when flipped, assuming a flat uniform prior and Bernoulli likelihood. We assume that 7/10 times the coin came up 'heads'.\relax }}{50}}
\newlabel{fig:Intro_posterior}{{2.4}{50}}
\citation{ioannidis2005most}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Posterior distributions for, $\theta $, the bias of a coin when flipped, assuming a flat uniform prior and Bernoulli likelihood. The grey line assumes that 7/10 times the coin came up 'heads'. The blue line is for the case where 70/100 times the coin came up 'heads'.\relax }}{51}}
\newlabel{fig:Intro_posteriorPeaked}{{2.5}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Implicit vs Explicit subjectivity}{51}}
\newlabel{sec:Intro_implicitExplicitSubjectivity}{{2.9}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Chapter summary}{53}}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}Chapter outcomes}{53}}
\@writefile{toc}{\contentsline {section}{\numberline {2.12}Problem set}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.1}The deterministic nature of random coin throwing.}{53}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The results of a coin throw from a given angle and height above a table.\relax }}{54}}
\newlabel{tab:Intro_PS_coinThrowsDeterministic}{{2.1}{54}}
\@writefile{toc}{\contentsline {subsubsection}{Suppose that all combinations of angles and heights are equally likely to be chosen. What is the probability that the coin lands on heads?}{54}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The probability that a given person throws a coin at a particular angle, and at a certain height above a table.\relax }}{55}}
\newlabel{tab:Intro_PS_coinThrowsFrequency}{{2.2}{55}}
\@writefile{toc}{\contentsline {subsubsection}{What are the new probabilities that the coin lands heads-up?}{55}}
\@writefile{toc}{\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at an angle of 45 degrees. What is the probability that the coin lands heads-up?}{55}}
\@writefile{toc}{\contentsline {subsubsection}{Suppose we force the coin-thrower to throw the coin at a height of 0.2m. What is the probability that the coin lands heads-up?}{55}}
\@writefile{toc}{\contentsline {subsubsection}{If we constrained the angle and height to be fixed, what would happen in repetitions of the same experiment?}{55}}
\@writefile{toc}{\contentsline {subsubsection}{In light of the previous question, comment on the Frequentist assumption of \textit  {exact repetitions} of a given experiment.}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.2}Model choice}{55}}
\@writefile{toc}{\contentsline {subsubsection}{Fit a linear regression model using classical least squares. How reasonable is the fit?}{56}}
\@writefile{toc}{\contentsline {subsubsection}{Fit a quintic (powers up to the 5th) model to the data. How does its fit compare to that of the linear model?}{56}}
\@writefile{toc}{\contentsline {subsubsection}{Fit a linear regression to each of the data sets, and similarly for the quintic model. Which of these performs best?}{56}}
\@writefile{toc}{\contentsline {subsubsection}{Using the fits from the first part of this question, compare the performance of the linear regression model, with that of the quintic model.}{56}}
\@writefile{toc}{\contentsline {subsubsection}{Which of the two models do you prefer, and why?}{56}}
\@writefile{toc}{\contentsline {subsubsection}{If you then found out that the data were years of education (x), and salary in \$000s (y). Which model would you favour?}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {2.13}Appendix}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.13.1}The Frequentist and Bayesian murder trial}{56}}
\newlabel{sec:Intro_appendixMurder}{{2.13.1}{56}}
\newlabel{eq:Intro_murder}{{2.7}{57}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Probability - the nuts and bolts of Bayesian inference}{59}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Probability}{{3}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Chapter mission statement}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Chapter goals}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Probability distributions: helping us explicitly state our ignorance}{60}}
\newlabel{sec:Probability_probabilityDistributions}{{3.3}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}What makes a probability distribution \textit  {valid}?}{60}}
\newlabel{sec:Probability_validProbabilityDistribution}{{3.3.1}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Probability distributions representing \textbf  {left:} the chance of winning a lottery, and \textbf  {right:} the value of a second-hand car.\relax }}{61}}
\newlabel{fig:Probability_lotterySecondhandCarProbability}{{3.1}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Probabilities vs probability density : interpreting discrete and continuous probability distributions}{62}}
\newlabel{sec:Probability_densityVsMassFunctions}{{3.3.2}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The probability that a second-hand car's value lies between \$2,500 and \$3,000.\relax }}{64}}
\newlabel{fig:Probability_continuousLotteryInterval}{{3.2}{64}}
\newlabel{eq:Probability_continuousProbabilityIntervalExample}{{3.5}{64}}
\@writefile{toc}{\contentsline {subsubsection}{A (wet) river crossing}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Top: crossing on discrete stepping stones. Middle: crossing via a bridge whose height (B) is a probability density. Bottom: crossing on thin ice - the thickness of the ice (F) represents a probability density.\relax }}{67}}
\newlabel{fig:Probability_riverCrossing}{{3.3}{67}}
\@writefile{toc}{\contentsline {subsubsection}{Probability zero vs impossibility}{68}}
\@writefile{toc}{\contentsline {subsubsection}{The good news: Bayes' rule doesn't distinguish between probabilities and probability densities}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Mean and variance of distributions}{70}}
\newlabel{sec:Probability_meanVariance}{{3.3.3}{70}}
\newlabel{eq:Probability_meanDistributionDiscrete}{{3.9}{70}}
\newlabel{eq:Probability_meanDistributionContinuous}{{3.10}{70}}
\newlabel{eq:Probability_meanLottery}{{3.11}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Playing a computational lottery. We see the approach of the running mean of repeatedly playing the lottery to the long-run mean of $50\genfrac  {}{}{}1{1}{2}$, as the number of plays increases.\relax }}{71}}
\newlabel{fig:Probability_meanDiscreteLongRun}{{3.4}{71}}
\newlabel{eq:Probability_meanCoinContinuous}{{3.12}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Career second-hand car sales. We can see the approach of the sample mean towards the long-run mean of \$3,000.\relax }}{72}}
\newlabel{fig:Probability_meanContinuousLongRun}{{3.5}{72}}
\newlabel{eq:Probability_varianceDistributionExpectations}{{3.13}{72}}
\newlabel{eq:Probability_varianceDistributionDiscrete}{{3.15}{73}}
\newlabel{eq:Probability_varianceDistributionContinuous}{{3.16}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Generalising probability distributions to two dimensions}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Comparing the variance of two lotteries, left: a lottery where all values between 0 and 100 are equally likely. Middle: a lottery where only values between 40 and 60 have a positive probability. Right: comparing the variability of these distributions about their common mean.\relax }}{74}}
\newlabel{fig:Probability_varianceLottery}{{3.6}{74}}
\@writefile{toc}{\contentsline {subsubsection}{Horses for courses: a 2-dimensional discrete probability example}{74}}
\newlabel{sec:Probability_biasedCoinsTwoDimensionalDiscrete}{{3.3.4}{74}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces A probability distribution showing the historical performance of two horses, A and B, in two separate races. $\{0,1\}$ refers to each horse losing or winning in their respective races.\relax }}{75}}
\newlabel{tab:Probability_coinBiased}{{3.1}{75}}
\newlabel{eq:Probability_discreteTwoDimensionalCoinSum}{{3.18}{75}}
\newlabel{fig:Probability_footSizeIntelligenceTwoDimensionalExample}{{\caption@xref {fig:Probability_footSizeIntelligenceTwoDimensionalExample}{ on input line 789}}{76}}
\@writefile{toc}{\contentsline {subsubsection}{Foot length and literacy: a 2-dimensional continuous probability example}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Marginal distributions}{76}}
\newlabel{sec:Probability_marginal}{{3.3.5}{76}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces The marginal distribution of horses A and B, achieved by summing the values in each column or row respectively.\relax }}{77}}
\newlabel{tab:Probability_coinsMarginal}{{3.2}{77}}
\newlabel{eq:Probability_marginalCoinsExample}{{3.19}{77}}
\newlabel{eq:Probability_marginalDiscreteProbabilityTwoDimensions}{{3.20}{77}}
\newlabel{eq:Probability_marginalContinuousProbabilityTwoDimensions}{{3.22}{78}}
\newlabel{eq:Probability_marginalContinuousProbabilityTwoDimensionsFootExample}{{3.23}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Top-left: the joint density of foot size and intelligence. Right: the marginal density of literacy scores. Bottom: the marginal density of foot size. \textbf  {I want to add a line at a particular value of literacy scores, and at a particular value of FS, to illustrate the horizontal and vertical summing.}\relax }}{79}}
\newlabel{fig:Probability_footSizeIntelligenceMarginal}{{3.8}{79}}
\@writefile{toc}{\contentsline {subsubsection}{Venn diagrams}{80}}
\newlabel{eq:Probability_vennMarginals}{{3.24}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Conditional distributions}{80}}
\newlabel{sec:Probability_conditionalDistributionIntro}{{3.3.6}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces A Venn diagram showing one way of interpreting marginal and conditional distributions for the horse racing example.\relax }}{81}}
\newlabel{fig:Probability_Venn}{{3.9}{81}}
\newlabel{eq:Probability_conditionalProbability}{{3.25}{82}}
\newlabel{eq:Probability_conditionalDiscreteCoins}{{3.26}{82}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces The highlighted region indicates the new solution space, since we know that horse A has won.\relax }}{83}}
\newlabel{tab:Probability_coinsConditionalDiscrete}{{3.3}{83}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Higher dimensional probability densities: no harder than 2-D, just looks it!}{83}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The dashed blue lines indicate the new event space in each case. The height walked following these lines is related to the magnitude of the conditional distributions shown on the right.\relax }}{84}}
\newlabel{fig:Probability_footSizeIntelligenceConditional}{{3.10}{84}}
\newlabel{eq:Probability_3DDiscreteHorsesExample}{{3.27}{85}}
\newlabel{eq:Probability_higherDimensionalMarginal}{{3.29}{86}}
\newlabel{eq:Probability_higherDimensionsConditional}{{3.30}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Independence}{86}}
\newlabel{sec:Probability_independence}{{3.5}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Knowledge of the colour of a card provides information about the suit of the card. The colour and suit of a card are \textit  {dependent}.\relax }}{87}}
\newlabel{fig:Probability_IndependenceCards}{{3.11}{87}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Venn diagram depictions of left: disjoint, and right: independent, events $A$ and $B$.\relax }}{88}}
\newlabel{fig:Probability_VennIndependence}{{3.12}{88}}
\newlabel{eq:Probability_independentConditionalEqualMarginal}{{3.31}{88}}
\newlabel{eq:Probability_independentConditionalEqualMarginal1}{{3.32}{89}}
\newlabel{eq:Probability_independentMultiplicationForm}{{3.33}{89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Conditional independence}{89}}
\newlabel{sec:Probability_conditionalIndependence}{{3.5.1}{89}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces The probability distribution for horses C and D. The marginal distribution of horses C and D are achieved by summing the values in each column or row respectively.\relax }}{90}}
\newlabel{tab:Probability_horsesIndependent}{{3.4}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Central Limit Theorems}{90}}
\newlabel{sec:Probability_CLT}{{3.6}{90}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces A diagrammatic depiction of conditional independence.\relax }}{91}}
\newlabel{fig:Probability_conditionalIndependence}{{3.13}{91}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces The convergence to a normal distribution for the mean of a sum of uniform distributions for IQ. The pdf for the average is shown in blue, with a normal distribution of the same mean and variance indicated in grey.\relax }}{93}}
\newlabel{fig:Probability_CLTNormalSum}{{3.14}{93}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}The Bayesian formula}{93}}
\newlabel{sec:Probability_BayesianFormula}{{3.7}{93}}
\newlabel{eq:Probability_conditionalProbabilityAB}{{3.35}{93}}
\newlabel{eq:Probability_conditionalProbabilityBA}{{3.36}{93}}
\newlabel{eq:Probability_jointConditionalProbability}{{3.37}{94}}
\newlabel{eq:Probability_BayesianFormula}{{3.38}{94}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}The intuition behind the formula}{94}}
\newlabel{eq:Probability_BayesianIntuition}{{3.39}{94}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The two ways of arriving at the joint probability $p(A,B)$; providing some intuition behind Bayes' rule.\relax }}{95}}
\newlabel{fig:Probability_BayesianIntuition}{{3.15}{95}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Breast cancer screeing}{95}}
\newlabel{eq:Probability_bayesBreastCancer}{{3.40}{96}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}The Bayesian inference process from the Bayesian formula}{96}}
\newlabel{eq:Probability_BayesianFormula1}{{3.41}{96}}
\newlabel{eq:Probability_BayesianInferenceFormula}{{3.42}{97}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Chapter summary}{97}}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Chapter outcomes}{97}}
\@writefile{toc}{\contentsline {section}{\numberline {3.11}Problem set}{98}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.1}The expected returns of a derivative}{98}}
\@writefile{toc}{\contentsline {subsubsection}{What are the expected returns of the stock?}{98}}
\@writefile{toc}{\contentsline {subsubsection}{What are the variance in returns?}{98}}
\@writefile{toc}{\contentsline {subsubsection}{Would you expect the return to be equal to the mean of the original stock squared? If not, why not?}{98}}
\@writefile{toc}{\contentsline {subsubsection}{What would be a fair price to pay for this derivative?}{98}}
\@writefile{toc}{\contentsline {subsubsection}{What is a fair price to pay for this stock?}{99}}
\@writefile{toc}{\contentsline {subsubsection}{Which asset is more risky, $D_t$ or $R_t$?}{99}}
\@writefile{toc}{\contentsline {subsubsection}{Use \textit  {method of moments} to estimate $\mu $ and $\sigma $. Note: this requires use of a computer with mathematical software, as analytic solutions aren't possible.}{99}}
\@writefile{toc}{\contentsline {subsubsection}{Compare the estimated model with the data.}{99}}
\@writefile{toc}{\contentsline {subsubsection}{Is the model a reasonable approximation to the data generating process?}{99}}
\@writefile{toc}{\contentsline {subsubsection}{If not, suggest a better alternative.}{99}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.2}The boy or girl paradox\let \reserved@d =[\def \par }{99}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.3}The Bayesian game show}{99}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11.4}Blood doping}{100}}
\@writefile{toc}{\contentsline {subsubsection}{What is the probability that a professional cyclist wins a race?}{100}}
\@writefile{toc}{\contentsline {subsubsection}{What is the probability that a cyclist wins a race given that they have cheated?}{100}}
\@writefile{toc}{\contentsline {subsubsection}{What is the probability that a cyclist is cheating given that he wins?}{100}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces The historical probabilities of behaviour and outcome for professional cyclists.\relax }}{101}}
\newlabel{tab:Probability_PS_bloodDoping}{{3.5}{101}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Understanding the Bayesian formula}{103}}
\newlabel{part:bayesianFormula}{{II}{105}}
\@writefile{toc}{\contentsline {section}{\numberline {3.12}Part mission statement}{105}}
\@writefile{toc}{\contentsline {section}{\numberline {3.13}Part goals}{105}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}The posterior - the goal of Bayesian inference}{107}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:posterior}{{4}{107}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Chapter Mission statement}{107}}
\newlabel{eq:Posterior_BayesHighlighted}{{4.1}{107}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Chapter goals}{107}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Expressing uncertainty through the posterior probability distribution}{108}}
\newlabel{sec:Posterior_parameterUncertainty}{{4.3}{108}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A probability distribution representing uncertainty over the proportion of the electorate that will vote for the Democrats in an upcoming election.\relax }}{109}}
\newlabel{fig:Posterior_electionProportion}{{4.1}{109}}
\citation{angrist1990lifetime}
\newlabel{eq:Posterior_militaryParticipation}{{4.4}{110}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces An estimated posterior probability distribution for the parameter $\beta $ in (4.4\hbox {}). The shaded region represents the posterior probability that the parameter is positive.\relax }}{111}}
\newlabel{fig:Posterior_regressionMilitaryParticipation}{{4.2}{111}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Bayesian coastguard: introducing the prior and the posterior}{111}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Bayesian statistics: updating our pre-analysis uncertainty}{112}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Three plots. Left hand plot is a contour plot of probability radiated symmetrically in a semi-circle away from the radio control tower, with the density declining to zero at 25km. The middle plot shows a contour plot of probability, with a higher density towards the centre of the diagram (here the densities are still relatively smooth, indicating high uncertainty). The final plot shows a definite peak in intensity around a particular point about 10km away from the coast, just right of centre. The different contours will be increasing shades of a particular colour.\relax }}{113}}
\newlabel{fig:Posterior_bayesianLighthouse}{{4.3}{113}}
\citation{tegmark2014our}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Do parameters actually exist and have a point value?}{114}}
\newlabel{sec:Posterior_parametersExist}{{4.3.3}{114}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The Frequentist and Bayesian perspectives on parameters.\relax }}{115}}
\newlabel{fig:Posterior_manyWorldsDoParametersExist}{{4.4}{115}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Failings of the Frequentist confidence interval}{115}}
\newlabel{sec:Posterior_classicalConfidenceInterval}{{4.3.4}{115}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The classical confidence interval. In each sample, we can calculate a 95\% confidence interval. Across repeated samples from a given population distribution, the classical confidence interval will contain the true parameter value 95\% of time.\relax }}{116}}
\newlabel{fig:Posterior_classicalConfidenceInterval}{{4.5}{116}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Three examples of a 95\% credible interval for the regression parameter $\beta $ of the example described in section 4.3\hbox {}.\relax }}{117}}
\newlabel{fig:Posterior_infiniteCredibleIntervals}{{4.6}{117}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Credible intervals}{117}}
\@writefile{toc}{\contentsline {subsubsection}{Treasure hunting: The central posterior and highest density intervals}{118}}
\newlabel{sec:Posterior_CPI}{{4.3.5}{118}}
\newlabel{sec:Posterior_HDI}{{4.3.5}{118}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The posterior probability for treasure being found along the seashore (represented by a linear x-axis).\relax }}{119}}
\newlabel{fig:Posterior_CPIvsHDI}{{4.7}{119}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Reconciling the difference between confidence and credible intervals}{119}}
\@writefile{toc}{\contentsline {subsubsection}{The interval ENIGMA}{120}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Historical communication frequencies resulting in an attack on a given location.\relax }}{121}}
\newlabel{tab:Posterior_confidenceIntervalHistoric}{{4.1}{121}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Classical confidence intervals calculated from data shown in table 4.1\hbox {}. Confidence intervals greater than or equal to 75\% are indicated in red, surrounded by parentheses.\relax }}{121}}
\newlabel{tab:Posterior_confidenceIntervalClassical}{{4.2}{121}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Bayesian credible intervals calculated from data shown in table 4.1\hbox {}. Credible intervals greater than or equal to 75\% are indicated in red, surrounded by parentheses. Note: 'credibility' is calculated by dividing the sum of interval values in each row by the row's total (see section 4.11.1\hbox {} for a full explanation.)\relax }}{122}}
\newlabel{tab:Posterior_confidenceIntervalBayesian}{{4.3}{122}}
\citation{robert2007bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Point parameter estimates}{123}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Prediction using predictive distributions}{126}}
\newlabel{sec:Posterior_predictiveDistributions}{{4.5}{126}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The mean, median and mode for a skewed, multi-modal distribution.\relax }}{127}}
\newlabel{fig:Posterior_meanMedianMAP}{{4.8}{127}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Example: number of Republican voters within a sample}{127}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Top-left: the prior proportion of people voting for the Republican party in a sample of 100, resulting in the prior predictive distribution shown in the bottom-left. Top-right: the posterior proportion of people voting Republican, resulting the bottom-right posterior predictive distribution.\relax }}{129}}
\newlabel{fig:Posterior_priorPosteriorPredictiveVoting}{{4.9}{129}}
\newlabel{eq:Posterior_priorPredictiveVoting}{{4.7}{129}}
\newlabel{eq:Posterior_posteriorPredictiveVoting}{{4.8}{130}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Example: interest rate hedging}{131}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces The likelihood of different rates of return across different central bank interest rates.\relax }}{132}}
\newlabel{fig:Posterior_likelihoodInterestRate}{{4.10}{132}}
\newlabel{eq:Posterior_InterestpriorPredictiveDistribution}{{4.9}{132}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The prior and posterior probabilities of different central ban interest rates.\relax }}{133}}
\newlabel{fig:Posterior_priorPosteriorInterestRate}{{4.11}{133}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Building the prior predictive distribution, as a sum of weighted conditional probabilities. Left: the weighted conditional probabilities (compare with figure 4.10\hbox {}). Right: the cumulative weighted conditional probabilities, which converge on the prior predictive distribution.\relax }}{133}}
\newlabel{fig:Posterior_priorBuildInterestRate}{{4.12}{133}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Model comparison using the posterior}{134}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces The prior and posterior predictive distributions for investment returns.\relax }}{135}}
\newlabel{fig:Posterior_priorPosteriorPredictiveInterestRate}{{4.13}{135}}
\newlabel{eq:Posterior_modelGivenDataProbability}{{4.11}{135}}
\newlabel{eq:Posterior_modelComparisonFull}{{4.13}{136}}
\newlabel{eq:Posterior_bayesFactorDefinition}{{4.14}{136}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Example: epidemiologist comparison}{137}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Example: customer footfall}{138}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Calculating the probability of the data for the two epidemiologists' opinions on colds.\relax }}{139}}
\newlabel{fig:Posterior_bayesFactorFluEpidemiologist}{{4.14}{139}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Left: the footfall data. Right: the area under the curves represents the probability of the data from each of the two models.\relax }}{141}}
\newlabel{fig:Posterior_modelComparisonFootfall}{{4.15}{141}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Model comparison through posterior predictive checks}{142}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Example: stock returns}{142}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Left: the actual data. Middle: actual returns vs normal-simulated returns. Right: actual returns vs t-distribution-simulated returns.\relax }}{143}}
\newlabel{fig:Posterior_PPCstockReturns}{{4.16}{143}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Chapter summary}{143}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Chapter outcomes}{144}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Problem set}{145}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}The lesser evil}{145}}
\@writefile{toc}{\contentsline {subsubsection}{Which of the above loss functions do you think is most appropriate, and why?}{146}}
\@writefile{toc}{\contentsline {subsubsection}{Which loss function might you choose to be most robust to \textit  {any} situation?}{146}}
\@writefile{toc}{\contentsline {subsubsection}{Following from the previous point, which measure of posterior centrality might you choose?}{146}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.2}Google word search prediction}{146}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces The columns give the historic breakdown of the search traffic for three topics: Barack Obama, Baby clothes, and Bayes; by the first three letters of the user's search.\relax }}{146}}
\newlabel{tab:Posterior_PS_googleInterval}{{4.4}{146}}
\@writefile{toc}{\contentsline {subsubsection}{Find the minimum-coverage confidence intervals of topics that exceed 70\%.}{146}}
\@writefile{toc}{\contentsline {subsubsection}{Find most narrow credible intervals for topics that exceed 70\%.}{146}}
\@writefile{toc}{\contentsline {subsubsection}{Topic search volumes}{146}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces The historic search traffic broken down by topic.\relax }}{147}}
\newlabel{tab:Posterior_PS_googleIntervalHistoricTopic}{{4.5}{147}}
\@writefile{toc}{\contentsline {subsubsection}{Three-letter search volumes}{147}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces The historic search traffic broken down by keyword.\relax }}{147}}
\newlabel{tab:Posterior_PS_googleIntervalHistoricKeyword}{{4.6}{147}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.3}Prior and posterior predictive example (with PPCs maybe)}{147}}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}Appendix}{147}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.1}The interval ENIGMA - explained in full}{147}}
\newlabel{sec:Posterior_appendixConfidenceInterval}{{4.11.1}{147}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Likelihoods}{149}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Likelihoods}{{5}{149}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Chapter Mission statement}{149}}
\newlabel{eq:Likelihood_BayesHighlighted}{{5.1}{149}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Chapter goals}{149}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}What is a likelihood?}{150}}
\newlabel{eq:Likelihood_Bayes}{{5.3}{150}}
\newlabel{eq:Likelihood_simple}{{5.3}{150}}
\newlabel{eq:Likelihood_fairCoin}{{5.3}{151}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The probabilities of all possible numbers of heads for a fair coin.\relax }}{152}}
\newlabel{fig:Likelihood_fairCoin}{{5.1}{152}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Why use 'likelihood' rather than 'probability'?}{152}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces An example posterior distribution for the probability of obtaining a heads in a coin toss.\relax }}{153}}
\newlabel{fig:Likelihood_posteriorExample}{{5.2}{153}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The likelihood function for obtaining a single head from two throws. The area under the curve is $\frac  {1}{3}$.\relax }}{154}}
\newlabel{fig:Likelihood_coinLikelihood}{{5.3}{154}}
\newlabel{eq:Likelihood_notation}{{5.7}{154}}
\citation{epstein2008model}
\newlabel{eq:Likelihood_OneHead}{{5.8}{155}}
\newlabel{eq:Likelihood_TwoHead}{{5.10}{155}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}What are models and why do we need them?}{155}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces The values of likelihood for the case of tossing a coin twice, where the probability of heads is constrained to take on a discrete value: \{0.0,0.2,0.4,0.6,0.8,1.0\}.\relax }}{156}}
\newlabel{tab:Likeihood_BayesBox}{{5.1}{156}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}How to choose an appropriate likelihood?}{157}}
\newlabel{sec:chooseLikelihood}{{5.6}{157}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}A likelihood model for an individual's disease status}{158}}
\newlabel{sec:Likelihood_individualDisease}{{5.6.1}{158}}
\newlabel{eq:Likelihood_SimpleModel1}{{5.12}{158}}
\newlabel{eq:Likelihood_SimpleModel2}{{5.13}{158}}
\newlabel{eq:Likelihood_bernoulli}{{5.14}{159}}
\newlabel{eq:Likelihood_SimpleModel3}{{5.15}{159}}
\newlabel{eq:Likelihood_SimpleModel4}{{5.16}{159}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}A likelihood model for disease prevalence of a group}{159}}
\newlabel{sec:Likelihood_diseaseGroup}{{5.6.2}{159}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. The sum of likelihoods is found by the area under each line, whereas the sum of probabilities is a discrete sum.\relax }}{160}}
\newlabel{fig:Likelihood_bernoulli}{{5.4}{160}}
\newlabel{eq:Likelihood_bernoulli1}{{5.17}{161}}
\newlabel{eq:Likelihood_bernoulli2}{{5.18}{162}}
\newlabel{eq:Likelihood_bernoulli3}{{5.19}{162}}
\newlabel{eq:Likelihood_binomialTwo}{{5.20}{162}}
\newlabel{eq:Likelihood_binomialTwoProbs}{{5.21}{162}}
\newlabel{eq:Likelihood_binomialTwoProbsSimple}{{5.22}{163}}
\newlabel{eq:Likelihood_binomialNearly}{{5.23}{163}}
\newlabel{eq:Likelihood_quadratic}{{5.24}{163}}
\newlabel{eq:Likelihood_nCr}{{5.25}{163}}
\newlabel{eq:Likelihood_binomialTwoFull}{{5.26}{163}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The likelihood function as theta varies for a sample of 2 individuals.\relax }}{164}}
\newlabel{fig:Likelihood_binomial}{{5.5}{164}}
\newlabel{eq:Likelihood_binomialThreeProbsSimpler}{{5.27}{165}}
\newlabel{eq:Likelihood_binomialThreeFull}{{5.28}{165}}
\newlabel{eq:Likelihood_binomialNFull}{{5.29}{165}}
\newlabel{eq:Likelihood_binomialTest}{{5.6.2}{165}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}The intelligence of a group of people}{166}}
\newlabel{sec:Likelihood_normal}{{5.6.3}{166}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Left panel shows a normal with $\mu =70$ and $\sigma ^2 = 81$, with the area corresponding to a result as extreme as 90 indicated. This translates into a standard normal cdf shown in the right panel, which can be used to calculate this area from the first figure. This translation to the standard normal is done by taking away $\mu $, and dividing through by $\sigma $. This is done since usually only standard normal cdf tables are available.\relax }}{167}}
\newlabel{fig:Likelihood_normal}{{5.6}{167}}
\newlabel{eq:Likelihood_normal}{{5.6.3}{167}}
\newlabel{eq:Likelihood_normalSampleOne}{{5.32}{168}}
\newlabel{eq:Likelihood_normalN}{{5.6.3}{170}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Exchangeability vs random sampling}{170}}
\newlabel{sec:Likelihood_randomSampleExchangeable}{{5.7}{170}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}The subjectivity of model choice}{172}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Maximum likelihood - a short introduction}{173}}
\newlabel{sec:Likelihood_MLE}{{5.9}{173}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.1}Estimating disease prevalence}{173}}
\newlabel{sec:Likelihood_diseaseMLE}{{5.9.1}{173}}
\newlabel{eq:Likelihood_binomialNew}{{5.36}{173}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The monotonicity of log-likelihood (top-left), means that the peaks of likelihood and log-likelihood coincide (bottom-left). However, this is not the case for an arbitrary function (top-right and bottom-right).\textbf  {Add legends to the bottom two graphs.} \relax }}{174}}
\newlabel{fig:Likelihood_logMonotonicity}{{5.7}{174}}
\newlabel{eq:Likelihood_logLikelihoodBinomial}{{5.37}{174}}
\newlabel{eq:Likelihood_logRules}{{5.38}{174}}
\newlabel{eq:Likelihood_binomialderiv}{{5.39}{174}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Log-likelihood of disease prevalence from section 5.9.1\hbox {} as a function of the proportion of individuals which have the disease in a population, $\theta $. The dotted line shows the maximum likelihood estimate $\mathaccentV {hat}05E{\theta }=1/10$.\relax }}{175}}
\newlabel{fig:Likelihood_MLE}{{5.8}{175}}
\newlabel{eq:Likelihood_binomialestimator}{{5.40}{176}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.2}Estimating the mean and variance in intelligence scores}{176}}
\newlabel{eq:Likelihood_normalTwo}{{5.41}{176}}
\newlabel{eq:Likelihood_diseaseLogLikelihood}{{5.42}{176}}
\newlabel{eq:Likelihood_diseaseDerivativeOne}{{5.43}{176}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.3}Maximum likelihood in simple steps}{177}}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Frequentist inference in Maximum Likelihood}{178}}
\newlabel{sec:Likelihood_maxlikelihoodInference}{{5.10}{178}}
\@writefile{toc}{\contentsline {section}{\numberline {5.11}Chapter summary}{178}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Two likelihoods which result in the same maximum likelihood estimates of parameters, at 0.1. The gray likelihood is less strongly-peaked, meaning we can be less confident about the estimates.\relax }}{179}}
\newlabel{fig:Likelihood_likelihoodCurvature}{{5.9}{179}}
\@writefile{toc}{\contentsline {section}{\numberline {5.12}Chapter outcomes}{180}}
\@writefile{toc}{\contentsline {section}{\numberline {5.13}Problem set}{180}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.13.1}Blog blues.}{180}}
\@writefile{toc}{\contentsline {subsubsection}{What assumptions might you make about the first-time visits?}{181}}
\@writefile{toc}{\contentsline {subsubsection}{What model might be appropriate to model the time between visits?}{181}}
\@writefile{toc}{\contentsline {subsubsection}{Algebraically derive an estimate of the mean number of visits per hour}{181}}
\@writefile{toc}{\contentsline {subsubsection}{Data analysis:}{181}}
\@writefile{toc}{\contentsline {subsubsection}{Graph the log likelihood near your estimated value. What does this show? Why don't we plot the likelihood?}{181}}
\@writefile{toc}{\contentsline {subsubsection}{Estimate confidence intervals around your parameter.}{181}}
\@writefile{toc}{\contentsline {subsubsection}{What is the probability that you will wait:}{181}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluate your model.}{181}}
\@writefile{toc}{\contentsline {subsubsection}{What alternative models might be useful here?}{181}}
\@writefile{toc}{\contentsline {subsubsection}{What are the assumptions behind these models?}{181}}
\@writefile{toc}{\contentsline {subsubsection}{Estimate the parameters of your new model.}{181}}
\@writefile{toc}{\contentsline {subsubsection}{Use your new model to estimate the probability that you will wait:}{181}}
\@writefile{toc}{\contentsline {subsubsection}{Hints: the exponential is to the poisson model, what the ? is to the negative binomial.}{182}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.13.2}Violent crime counts in New York counties}{182}}
\@writefile{toc}{\contentsline {subsubsection}{Graph the violent crime account against population. What type of relationship does this suggest?}{182}}
\@writefile{toc}{\contentsline {subsubsection}{A simple model}{182}}
\@writefile{toc}{\contentsline {subsubsection}{What are the assumptions of this model?}{183}}
\@writefile{toc}{\contentsline {subsubsection}{Estimate the parameter $\theta $ from the data. What does this parameter represent?}{183}}
\@writefile{toc}{\contentsline {subsubsection}{Do these assumptions seem realistic?}{183}}
\@writefile{toc}{\contentsline {subsubsection}{Estimate a measure of uncertainty in your estimates.}{183}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluate the performance of your model.}{183}}
\@writefile{toc}{\contentsline {subsubsection}{A better model}{183}}
\@writefile{toc}{\contentsline {subsubsection}{Why is this model better?}{183}}
\@writefile{toc}{\contentsline {subsubsection}{What factors might affect $\theta _i$?}{183}}
\@writefile{toc}{\contentsline {subsubsection}{Write down a new model specification taking into account the previous point.}{183}}
\@writefile{toc}{\contentsline {subsubsection}{Estimate the parameters of this new specification.}{183}}
\@writefile{toc}{\contentsline {subsubsection}{How does this new model compare to the previous iteration?}{183}}
\@writefile{toc}{\contentsline {subsubsection}{What alternative specifications might be worth attempting?}{183}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.13.3}Monte Carlo evaluation of the performance of MLE in R}{183}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.13.4}The sample mean as MLE}{184}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Priors}{185}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Prior}{{6}{185}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Chapter Mission statement}{185}}
\newlabel{eq:Prior_BayesHighlighted}{{6.1}{185}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Chapter goals}{185}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}What are priors, and what do they represent?}{186}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Left - a prior for a doctor's pre-testing diagnostic probability of an individual having a disease. Right - a prior which represents pre-sample uncertainty in disease prevalence.\relax }}{187}}
\newlabel{fig:Prior_introduction}{{6.1}{187}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Why do we need priors at all?}{188}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Why don't we just normalise likelihood by choosing a unity prior?}{189}}
\newlabel{sec:Prior_unityPrior}{{6.5}{189}}
\newlabel{eq:Prior_BayesNormalisedLikelihood}{{6.5}{189}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces The top two boxes represent the different world views for left: the coin being fair, and right: the coin being biased. If we suppose that these two views have the same prior probability, illustrated here by them having the same area, then use of Bayes' rule means that we suppose the coin is likely bias.\relax }}{191}}
\newlabel{fig:Prior_priorJustificationCoin}{{6.2}{191}}
\citation{stewart2014teaching}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}The explicit subjectivity of priors}{192}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Combining a prior and likelihood to form a posterior}{192}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}The Goldfish game}{192}}
\newlabel{sec:Prior_urn}{{6.7.1}{192}}
\newlabel{eq:Prior_bernoulli}{{6.4}{193}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing fish from a bowl containing a 5 fish mixture of red and white fish, one of which has been drawn and shown to be red. Here we assume that pre-experiment all possible numbers of red fish are equally likely, by adopting a uniform prior.\relax }}{194}}
\newlabel{tab:Prior_bayesBoxDiscreteUrns}{{6.1}{194}}
\newlabel{tab:addlabel}{{6.1}{194}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The prior, likelihood and posterior for the fish example described in 6.7.1\hbox {}. The prior in the upper panel gives uniform weighting to all possible numbers of red fish. This is then multiplied by the likelihood (in the middle panel) at each number of fish, and normalised to make the posterior density shown in the bottom panel.\relax }}{194}}
\newlabel{fig:Prior_urnStacked}{{6.3}{194}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing fish from a bowl containing a 5 fish mixture of red and white fish, one of which has been drawn and shown to be red. Here we assume that pre-experiment all possible numbers of red fish are equally likely, by adopting a uniform prior.\relax }}{195}}
\newlabel{tab:Prior_bayesBoxUrnUpdated}{{6.2}{195}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Disease proportions revisited}{195}}
\newlabel{sec:Prior_diseaseProp}{{6.7.2}{195}}
\newlabel{eq:Prior_binomial}{{6.5}{195}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces The prior, likelihood and posterior for the goldfish game example described in 6.7.1\hbox {}. The prior in the upper panel gives more weighting to more equal numbers of red and white fish. This is then multiplied by the likelihood (in the middle panel) at each number of fish, and normalised to make the posterior density shown in the bottom panel.\relax }}{196}}
\newlabel{fig:Prior_bayesUrnUpdated}{{6.4}{196}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces A Bayes box for the discretised disease example of section 6.7.2\hbox {}.\relax }}{197}}
\newlabel{tab:Prior_BayesDiscretised}{{6.3}{197}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Constructing priors}{198}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.1}Vague priors}{198}}
\newlabel{sec:Prior_vague}{{6.8.1}{198}}
\newlabel{eq:Prior_BayesFlatPrior}{{6.8.1}{198}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The prior, likelihood and posterior for the \textbf  {left:} discretised, and \textbf  {right:} continuous disease proportion example described in section 6.7.2\hbox {}. Each point in $\theta $ along the continuous prior curve (top panel) is multiplied by the corresponding value of likelihood (middle panel), to form the numerator of Bayes' rule. The numerator is then normalised to make the posterior probability density shown in the bottom panel.\relax }}{199}}
\newlabel{fig:Prior_diseaseDiscretised}{{6.5}{199}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces The probability density for obtaining all diseased individuals within sample sizes of 1, 2 and 10 respectively. Starting out with a flat prior for the probability that one individual has a disease has resulted in non-flat priors for the other 2 probabilities.\relax }}{200}}
\newlabel{fig:Prior_jeffreysIntro}{{6.6}{200}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Two viable prior distributions for the average time taken before the onset of lung cancer after patients begin smoking.\relax }}{202}}
\newlabel{fig:Prior_lungcancerFlatandGammaPriors}{{6.7}{202}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.2}Informative priors}{202}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces The SAT scores for past students of a class. The mean and variance of this hypothetical sample are 1404, and 79,716 respectively, which are used to fit a normal distribution to the data, and is shown in red.\relax }}{203}}
\newlabel{fig:Prior_SATScoresHistogram}{{6.8}{203}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.3}The numerator of Bayes' rule determines the shape}{204}}
\newlabel{sec:Prior_numerator}{{6.8.3}{204}}
\newlabel{eq:Prior_BayesNumerator}{{6.8.3}{204}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.4}Eliciting priors}{204}}
\newlabel{eq:Prior_elicitingPriorNormal}{{6.8}{205}}
\newlabel{eq:Prior_elicitingPriorNormalRegression}{{6.9}{205}}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}A strong model is less sensitive to prior choice}{205}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Hypothetical data for the 25th and 75th percentiles of the estimated wage premium from 10 experts. In the left hand panel we regress these percentiles on the corresponding percentiles from a standard normal distribution, yielding estimates of the mean and variance of a normal prior, which is shown on the right.\relax }}{206}}
\newlabel{fig:Prior_elicitingRegression}{{6.9}{206}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces The effect of increasing sample size on the posterior density for the prevalence of a disease in a population. The leftmost column has N=10, the middle N=100, and the rightmost N=1,000. All three have the same proportion of disease cases in the sample.\relax }}{207}}
\newlabel{fig:Prior_weakPriorEffect}{{6.10}{207}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9.1}Caveat: overly-zealous priors \textit  {will} affect the posterior}{207}}
\@writefile{toc}{\contentsline {section}{\numberline {6.10}Chapter summary}{208}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces \textbf  {Left panels:} The effect of choosing a discontinuous prior (top-left) on the posterior. \textbf  {Right panels:} choosing a smoother prior that still puts most of its weight for higher values of literacy (top-right) results in a less-skewed mean (shown by orange line). In both cases the data are assumed to be the same, resulting in the same likelihood.\relax }}{209}}
\newlabel{fig:Prior_overZealous}{{6.11}{209}}
\@writefile{toc}{\contentsline {section}{\numberline {6.11}Chapter outcomes}{210}}
\@writefile{toc}{\contentsline {section}{\numberline {6.12}Problem set}{210}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.12.1}Counting sheep}{210}}
\@writefile{toc}{\contentsline {subsubsection}{What likelihood model might you use here?}{211}}
\@writefile{toc}{\contentsline {subsubsection}{What are the assumptions underpinning this model?}{211}}
\@writefile{toc}{\contentsline {subsubsection}{Introducing a prior}{211}}
\@writefile{toc}{\contentsline {subsubsection}{Which of the previous priors is most uninformative?}{211}}
\@writefile{toc}{\contentsline {subsubsection}{Suppose that you observe 10 sheep jumping over the fence, calculate the posterior distribution for each of the different priors using your chosen likelihood.}{211}}
\@writefile{toc}{\contentsline {subsubsection}{For what numbers of jumping sheep would one of your posteriors run into problems?}{211}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.12.2}Investigating priors through US elections}{211}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.12.3}Choosing prior distributions.}{212}}
\@writefile{toc}{\contentsline {subsubsection}{If we choose a prior distribution that sets $p(log(\sigma ))=1$, what does this imply about the distribution of $p(\sigma )$?}{212}}
\@writefile{toc}{\contentsline {subsubsection}{For a parameter $\theta $, which choice of prior will leave it invariant under transformations?}{212}}
\@writefile{toc}{\contentsline {subsubsection}{Pre-experimental data prior setting}{212}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.12.4}Expert data prior example}{212}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.12.5}Data analysis example showing the declining importance of prior as data set increases in size}{212}}
\@writefile{toc}{\contentsline {section}{\numberline {6.13}Appendix}{213}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.13.1}Bayes' rule for the urn}{213}}
\newlabel{app:Prior_bayesUrn}{{6.13.1}{213}}
\newlabel{eq:Prior_bayesDiscreteForm}{{6.10}{213}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.13.2}The probabilities of having a disease}{213}}
\newlabel{app:Prior_diseaseJeffreys}{{6.13.2}{213}}
\newlabel{eq:Prior_appChangeOfVariables}{{6.11}{213}}
\newlabel{eq:Prior_appChangeOfVariablesSolved}{{6.12}{213}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}The devil's in the denominator}{215}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:denominator}{{7}{215}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Chapter mission}{215}}
\newlabel{eq:Denominator_BayesHighlighted}{{7.1}{215}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Chapter goals}{215}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}An introduction to the denominator}{216}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}The denominator as a normalising factor}{216}}
\newlabel{eq:Denominator_discreteDenominator}{{7.2}{217}}
\newlabel{eq:Denominator_continuousDenominator}{{7.3}{217}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Example: disease}{217}}
\newlabel{sec:Denominator_discreteExample}{{7.3.2}{217}}
\newlabel{eq:Denominator_discreteLikelihood}{{7.5}{218}}
\newlabel{eq:Denominator_discreteExamplePosterior}{{7.7}{219}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Example: the proportion of people who vote the Conservatives}{219}}
\newlabel{sec:Denominator_continuousExample}{{7.3.3}{219}}
\newlabel{eq:Denominator_binomial}{{7.8}{219}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The prior is multiplied through by the likelihood, resulting in the numerator (the penultimate panel), which is then normalised by the sum over its values, to obtain the denominator.\relax }}{220}}
\newlabel{fig:Denominator_discreteExample}{{7.1}{220}}
\newlabel{eq:Denominator_continuousExampleApprox}{{7.9}{221}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}The denominator as a probability}{221}}
\newlabel{sec:Denominator_asAProbability}{{7.3.4}{221}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The prior, likelihood and posterior for the proportion of individuals voting for the conservative party in a general election, where we have found 40 people out of a sample of 100 voted conservative.\relax }}{222}}
\newlabel{fig:Denominator_continuousExample}{{7.2}{222}}
\newlabel{eq:Denominator_jointDensity}{{7.10}{223}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.5}Using the denominator to choose between competing models}{223}}
\newlabel{eq:Denominator_expectedLikelihood}{{7.12}{223}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Shows the derivation of the joint density for the disease example described in section 7.3.2\hbox {}. Each column of the likelihood - corresponding to a given disease status - is multiplied by the corresponding prior, resulting in the joint density. By summing the joint density across the different disease statuses of the patient, this results in $p(data)$. \textbf  {Add pluses and equals to the calculation of $p(data)$. Also add in the posterior calculation.} See figure 7.3\hbox {} for a graphical depiction of this joint density.\relax }}{224}}
\newlabel{tab:Denominator_discreteJoint}{{7.1}{224}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The joint density of the data and the parameter for the disease example described in section 7.3.2\hbox {}. When we uncover that the test result is positive, we are confined to look at the bars in dark grey; finding that the probability that an individual is diseased is significantly higher than the alternative (see the bottom panel of figure 7.1\hbox {}). \textbf  {Perhaps redo this figure with a contour plot opposed to a 3D graph, and show how the posterior is obtained in another panel. Or just get rid of it, the table does pretty much cover it.}\relax }}{225}}
\newlabel{fig:Denominator_discreteJointDensity}{{7.3}{225}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Top-left: a contour plot of the joint density of the voting example described in section 7.4\hbox {}. Top-right: the marginal density of $p(data)$ obtained by summing across all values of $\theta $. Bottom-left: the posterior obtained by summing the joint density across the line shown at 40. Note that in reality the data variable is discrete, but it is drawn here as continuous to make the plot simpler to interpret. \textbf  {The line at 40 may be dashed in the final version. The axes all need to be aligned.}\relax }}{226}}
\newlabel{fig:Denominator_continuousJointDensity}{{7.4}{226}}
\newlabel{eq:Denominator_modelProbability}{{7.13}{227}}
\newlabel{eq:Denominator_modelComparison}{{7.14}{227}}
\newlabel{eq:Denominator_bayesFactor}{{7.15}{227}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}The difficulty with the denominator}{228}}
\newlabel{sec:Denominator_difficulty}{{7.4}{228}}
\newlabel{eq:Denominator_doubleSum}{{7.16}{228}}
\newlabel{eq:Denominator_doubleIntegral}{{7.17}{228}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Multi-parameter discrete model example: the comorbidity between depression and anxiety}{228}}
\newlabel{sec:Denominator_comorbidityTwoParameterDiscrete}{{7.4.1}{228}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Calculation of posterior for the anxiety and depression example described.\relax }}{230}}
\newlabel{tab:Denominator_comorbidityTwoParameterDiscrete}{{7.2}{230}}
\newlabel{eq:Denominator_TwoParameterDiscreteBayesSimple}{{7.19}{231}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Continuous multi-parameter example: mean and variance of IQ}{231}}
\newlabel{sec:Denominator_continuousTwoParameterIQ}{{7.4.2}{231}}
\newlabel{eq:Denominator_continuousTwoParameterLikelihood}{{7.20}{231}}
\newlabel{sec:Denominator_continuousMultiparameterIndependence}{{7.21}{232}}
\newlabel{eq:Denominator_continuousTwoParameterPrior}{{7.22}{232}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces The prior, likelihood, and posterior distributions for the mean and variance of IQ example described in section 7.4.2\hbox {}.\relax }}{233}}
\newlabel{fig:Denominator_continuousTwoParameter3D}{{7.5}{233}}
\newlabel{eq:Denominator_continuousTwoParameterJointPosterior}{{7.23}{234}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}How to dispense with the difficulty: Bayesian computation}{235}}
\newlabel{sec:Denominator_dispensingWithNumerator}{{7.5}{235}}
\newlabel{eq:Denominator_proportional}{{7.28}{236}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Chapter summary}{237}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Chapter outcomes}{237}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Problem set}{238}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}New disease cases}{238}}
\@writefile{toc}{\contentsline {subsubsection}{What likelihood model might be appropriate here?}{238}}
\@writefile{toc}{\contentsline {subsubsection}{What are the assumptions of this model? Are they appropriate here?}{238}}
\@writefile{toc}{\contentsline {subsubsection}{A gamma prior}{238}}
\@writefile{toc}{\contentsline {subsubsection}{Data}{238}}
\@writefile{toc}{\contentsline {subsubsection}{Find the posterior distribution for the mean parameter $\lambda $}{239}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}The comorbidity between depression, anxiety and psychosis}{239}}
\@writefile{toc}{\contentsline {subsubsection}{Calculate the probability that a patient is depressed.}{239}}
\@writefile{toc}{\contentsline {subsubsection}{Calculate the probability that a patient is psychotic.}{239}}
\@writefile{toc}{\contentsline {subsubsection}{What is the probability that a patient is psychotic given that they are depressed, and anxious?}{239}}
\@writefile{toc}{\contentsline {subsubsection}{What is the probability that a patient is not psychotic if they are not depressed?}{239}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces The joint distribution representing historical patient diagnoses upon entering the clinic.\relax }}{239}}
\newlabel{tab:Denominator_PS_anxietyDepression}{{7.3}{239}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Finding mosquito larvae after rain}{239}}
\@writefile{toc}{\contentsline {subsubsection}{Find the mean of a $lognormal(d,1)$ distribution.}{240}}
\@writefile{toc}{\contentsline {subsubsection}{Finding the mode}{240}}
\@writefile{toc}{\contentsline {subsubsection}{Use a computer to graph the shape of the posterior distribution.}{240}}
\@writefile{toc}{\contentsline {subsubsection}{Calculate the posterior (difficult).}{240}}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}Appendix}{240}}
\newlabel{sec:Denominator_appendix}{{7.9}{240}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Analytic Bayesian methods}{241}}
\newlabel{part:analyticalBayes}{{III}{243}}
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Part mission statement}{243}}
\@writefile{toc}{\contentsline {section}{\numberline {7.11}Part goals}{243}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}An introduction to distributions for the mathematically-un-inclined}{245}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:distributions}{{8}{245}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Chapter mission statement}{245}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Chapter goals}{245}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}The interrelation among distributions}{246}}
\newlabel{sec:Distributions_relationships}{{8.3}{246}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces \textbf  {Some of the interrelations among common statistical distributions.} Discrete distributions are shown in rectangular boxes, and continuous ones in oval boxes. Black lines indicate that the resultant is a special case of the former, dashed lines indicate approximate/limiting cases, and red lines indicate that the latter can be made via a transformation of the former. Note that the relationships shown here are \textit  {not} exhaustive; there are many more relationships that are \textit  {not} shown. CAN WE MAKE THIS OVER A DOUBLE PAGE? WOULD ALSO BE GOOD TO HAVE THIS AS A POSTER INCLUDED IN THE BOOK.\relax }}{247}}
\newlabel{fig:Distributions_nexusOfRelations}{{8.1}{247}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Sampling distributions for likelihoods}{247}}
\newlabel{sec:Distributions_samplingDist}{{8.4}{247}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Bernoulli}{248}}
\newlabel{sec:Distributions_bernoulli}{{8.4.1}{248}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Bernoulli likelihoods for the event that the horse wins (red), and loses (blue). The maximum likelihood estimates are shown as dotted lines for each of the cases.\relax }}{250}}
\newlabel{fig:Distributions_bernoulliHorseRace}{{8.2}{250}}
\newlabel{eq:Distributions_binomialDefinition}{{8.2}{250}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}Binomial}{251}}
\newlabel{sec:Distributions_binomial}{{8.4.2}{251}}
\newlabel{eq:Distributions_binomialDef}{{8.6}{253}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces \textbf  {Left}: Binomial likelihoods for the event that 5 (blue), 2 (red), and 9 (grey) volunteers recovered in the week period. The maximum likelihood estimates are shown as dotted lines for each of the cases. \textbf  {Right:} the probability distribution of successful trials if $\theta =0.3$ (blue) and $\theta =0.7$ (orange).\relax }}{254}}
\newlabel{fig:Distributions_binomialClinicalTrial}{{8.3}{254}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}Poisson}{254}}
\newlabel{sec:Distributions_poisson}{{8.4.3}{254}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces \textbf  {Left}: The poisson likelihood for three different samples. \textbf  {Right}: the poisson distribution for different values of the mean rate. \textbf  {Add legends: left xbar=0.5,3,8. Right: lambda=0.5,3,8.}\relax }}{256}}
\newlabel{fig:Distributions_poissonLegionella}{{8.4}{256}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.4}Negative binomial}{257}}
\newlabel{sec:Distributions_negBin}{{8.4.4}{257}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces \textbf  {Left}: the overall distribution (blue, dashed) is essentially an average across those for each of the towns: Studentville (red), Commuterville (orange), and Academicville (green). \textbf  {Right}: a comparison of the overall distribution (blue, dashed), with a poisson distribution (orange) of the same mean. \relax }}{258}}
\newlabel{fig:Distributions_negativeBinomialDispersion}{{8.5}{258}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces \textbf  {Left}: The negative binomial distribution for low dispersion (blue), medium dispersion (orange) and high dispersion (green). All three cases have a mean of 8. \textbf  {Right}: a contour plot of the likelihood surface for the data sample: {0, 5, 0, 5, 8, 10, 15}. The parameter $\kappa $ here represents the dispersion, with smaller values indicating less dispersion. \textbf  {Add legends}\relax }}{261}}
\newlabel{fig:Distributions_negativeBinomial}{{8.6}{261}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.5}Beta-binomial}{262}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces \textbf  {Left:} a distribution that encompasses outcomes from both groups can be thought of as an average of the \textit  {mild} and \textit  {severe} groups. \textbf  {Right:} a comparison of the resultant distribution with a binomial distribution with the same mean. \textbf  {Add legends}\relax }}{264}}
\newlabel{fig:Distributions_betaBinomialDispersion}{{8.7}{264}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.6}Normal}{264}}
\newlabel{sec:Distributions_normal}{{8.4.6}{264}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces \textbf  {Left, uppermost panel:} the beta distribution for three different sets of parameter values. Blue (1,1), Orange (1,3), Green (10,10). \textbf  {Left, lowermost panel:} the beta-binomial distribution for the same parameter values, out of a sample size of 10. \textbf  {Right}: likelihood contour plot for a sample of size studies {1, 10, 8, 3, 8, 9}. Again the sample size is 10 in all cases. \textbf  {Add legends}\relax }}{265}}
\newlabel{fig:Distributions_betaBinomialDrug}{{8.8}{265}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces \textbf  {Left}: the normal probability density functions for three different sets of $(\mu ,\sigma )$: (36, 0.5), (36, 0.8), (37, 0.5). \textbf  {Right:} a contour plot of the normal likelihood for a sample of body temperatures: {36.4, 37.2, 35.8, 36.4, 37.1, 35.6, 36.4, 37.6, 37.5, 37.1}.\relax }}{267}}
\newlabel{fig:Distributions_normalBodyTemp}{{8.9}{267}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.7}Student t}{267}}
\newlabel{sec:Distributions_tDist}{{8.4.7}{267}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces \textbf  {Left}: the distribution covering test scores across both schools can be made from an average of the two individual distributions. \textbf  {Right:} a comparison of the resultant distribution with a normal distribution of the same mean and variance.\relax }}{269}}
\newlabel{fig:Distributions_tArithmeticDispersion}{{8.10}{269}}
\citation{taleb2010black}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces Student t distributions drawn for three parameter sets of $(\mu ,\sigma ,\nu )$: (100,10,1), (100,10,5), and (120,15,1).\relax }}{270}}
\newlabel{fig:Distributions_tArithmeticExposition}{{8.11}{270}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces Pairwise contour plots of the likelihood surface. In each of the plots, the remaining variable is set at its maximum likelihood estimated value.\relax }}{271}}
\newlabel{fig:Distributions_tArithmeticLikelihood}{{8.12}{271}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.8}Exponential}{272}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces \textbf  {Left:} the exponential probability distribution for three values of $\lambda $: 1 (blue), 2.5 (orange) and 5 (green). \textbf  {Right:} the likelihood surface for three different datasets with mean times between outbreaks given by 3.1 (blue), 0.7 (orange) and 0.4 (green) years. The dashed vertical lines indicate the maximum likelihood estimates of the parameters in each case.\textbf  {Add legends rather than state in text.}\relax }}{273}}
\newlabel{fig:Distributions_exponentialEbola}{{8.13}{273}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.9}Gamma}{274}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces \textbf  {Left:} the distribution of lottery claim times for 1 (blue) and 3 (orange) people, with $\lambda =1$. \textbf  {Middle:} the distribution of lottery claim times for 1 (blue) and 3 (orange) people, with $\lambda =\frac  {1}{2}$. \textbf  {Right:} the distribution of lottery claim times for 1 (blue) and 5 (orange) people, with $\lambda =1$. \textbf  {Add legends/titles.}\relax }}{275}}
\newlabel{fig:Distributions_gammaLottery}{{8.14}{275}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces A contour plot of the likelihood surface across the parameters of a $\Gamma (\alpha ,\beta )$ distribution modelling the time taken for 3 people to claim their prizes, with sample data: {5.1, 7.8, 3.4, 3.2, 4.3, 5.2, 8.1, 3.7, 2.3, 6.3}. \textbf  {Add legends/titles.}\relax }}{276}}
\newlabel{fig:Distributions_gammaLotteryLikelihood}{{8.15}{276}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.10}Multinomial}{277}}
\newlabel{sec:Distributions_multinomial}{{8.4.10}{277}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces The multinomial probability distribution for probabilities $(p_A=\frac  {1}{2},p_B=\frac  {1}{3},p_O = \frac  {1}{6})$.)\textbf  {Add legends/titles, and combine with other - although one in Matlab, the other in Mathematica.}\relax }}{280}}
\newlabel{fig:Distributions_multinomial}{{8.16}{280}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces A contour plot of the multinomial likelihood across $(p_A,p_B,p_O=1-p_A-p_B)$.\textbf  {Add legends/titles, and combine with other - although one in Matlab, the other in Mathematica.}\relax }}{280}}
\newlabel{fig:Distributions_multinomialLikelihoodl}{{8.17}{280}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.11}Multivariate normal and multivariate t}{281}}
\newlabel{sec:Distributions_multinormalSampling}{{8.4.11}{281}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces \textbf  {Left:} the actual data from all days of trading last year. \textbf  {Centre:} 1000 data points from a normal fit to the data. \textbf  {Right:} 1000 data points from a t distribution fit to the data.\relax }}{282}}
\newlabel{fig:Distributions_multivariateNormalTEstimates}{{8.18}{282}}
\citation{mandelbrot2008misbehaviour}
\citation{taleb2010black}
\@writefile{lof}{\contentsline {figure}{\numberline {8.19}{\ignorespaces \textbf  {Top:} contour plot of the 2D multivariate normal distribution function. \textbf  {Bottom:} contour plot of the 2D multivariate t distribution, with $\nu =3$ and the same covariance matrix as the normal.\textbf  {Add legends/titles.}\relax }}{284}}
\newlabel{fig:Distributions_multivariateNormalT}{{8.19}{284}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Prior distributions}{284}}
\newlabel{sec:Distributions_priorAll}{{8.5}{284}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Distributions for probabilities, proportions and percentages}{285}}
\@writefile{toc}{\contentsline {subsubsection}{Uniform}{285}}
\newlabel{sec:Distributions_uniform}{{8.5.1}{285}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.20}{\ignorespaces The continuous uniform distribution on $[0,1]$.\relax }}{286}}
\newlabel{fig:Distributions_uniform}{{8.20}{286}}
\@writefile{toc}{\contentsline {subsubsection}{Beta}{286}}
\newlabel{sec:Distributions_beta}{{8.5.1}{286}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {8.21}{\ignorespaces \textbf  {Left:} three symmetric Beta distributions about $\frac  {1}{2}$; $Beta(1,1)$ (blue), $Beta(2.5,2.5)$ (orange), and $Beta(10,10)$ (green). \textbf  {Right:} two non-symmetric Beta distributions; $Beta(10,2)$ (blue), and $Beta(2,10)$ (orange). \textbf  {Add legends.}\relax }}{287}}
\newlabel{fig:Distributions_betaDistribution}{{8.21}{287}}
\@writefile{toc}{\contentsline {subsubsection}{Logit-normal}{288}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.22}{\ignorespaces \textbf  {Left:} $logit-normal(0,\sigma )$ distributions for $\sigma =\frac  {1}{3}$ (blue),$\sigma =1$ (orange), and $\sigma =3$ (green). \textbf  {Right:} $logit-normal(1,\sigma )$ distributions for the same values of $\sigma $.\textbf  {Add legends.}\relax }}{289}}
\newlabel{fig:Distributions_logitNormal}{{8.22}{289}}
\@writefile{toc}{\contentsline {subsubsection}{Dirichlet}{290}}
\newlabel{sec:Distributions_dirichlet}{{8.5.1}{290}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.23}{\ignorespaces \textbf  {Left:} feasible region for probabilities for two categories. \textbf  {Right:} feasible region for probabilities for three categories.\textbf  {Add legends.}\relax }}{291}}
\newlabel{fig:Distributions_dirichlet}{{8.23}{291}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.24}{\ignorespaces The dirichlet distribution for $(\alpha _1,\alpha _2,\alpha _3)$ equal to (2,2,2), (5,5,5) and (4,2,2), for the left, middle and right plots respectively.\textbf  {Add legends, also change labelling to be pr, pd etc.}\relax }}{292}}
\newlabel{fig:Distributions_dirichletTriangle3D}{{8.24}{292}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}Distributions for means and regression coefficients}{293}}
\@writefile{toc}{\contentsline {subsubsection}{Normal}{294}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.25}{\ignorespaces \textbf  {Left:} the effect of a $\mu \sim N(17,1)$ prior (blue) for $\theta _i\sim N(\mu ,4)$ distribution (orange). \textbf  {Right:} the effect of a $\mu \sim N(25,3)$ prior (blue) for $\theta _i\sim N(\mu ,4)$ distribution (orange). The mean of each distribution is shown as a dotted line.\textbf  {Add legends.}\relax }}{295}}
\newlabel{fig:Distributions_normalPrior}{{8.25}{295}}
\@writefile{toc}{\contentsline {subsubsection}{Student t}{295}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.26}{\ignorespaces \textbf  {Left:} normal (blue) and t (orange) distributions with the same variance and mean. Here $\nu =4$ for the latter. \textbf  {Right:} $\nu =60$ for the t distribution.\textbf  {Add legends.}\relax }}{296}}
\newlabel{fig:Distributions_normalTPrior}{{8.26}{296}}
\@writefile{toc}{\contentsline {subsubsection}{Cauchy}{296}}
\newlabel{sec:Distributions_cauchy}{{8.5.2}{296}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.27}{\ignorespaces A normal (orange), a t (green) with 3 degrees of freedom, and a cauchy with scale parameter 0.3. The normal and t distributions drawn have variances of 0.64.\textbf  {Add legends.}\relax }}{298}}
\newlabel{fig:Distributions_cauchyPrior}{{8.27}{298}}
\@writefile{toc}{\contentsline {subsubsection}{Multivariate normal, and multivariate t}{298}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}Distributions for non-negative parameters}{300}}
\@writefile{toc}{\contentsline {subsubsection}{Gamma}{300}}
\newlabel{sec:Distributions_gamma}{{8.5.3}{300}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.28}{\ignorespaces \textbf  {Left:} three prior distributions for $\lambda \sim \Gamma (\alpha ,\beta )$ where $\frac  {\alpha }{\beta }=\mu $: $\Gamma (10,1)$ (blue), $\Gamma (4,2.5)$ (orange), and $\Gamma (1,10)$ (green). \textbf  {Right:} the resultant posteriors for the sample data \{17, 6, 11, 13, 16\} for each of the prior cases. \textbf  {Add legends}.\relax }}{301}}
\newlabel{fig:Distributions_gammaPrior}{{8.28}{301}}
\@writefile{toc}{\contentsline {subsubsection}{Half-Cauchy, inverse-gamma, inverse-$\chi ^2$ and uniform}{301}}
\newlabel{sec:Distributions_halfCauchyInvGamma}{{8.5.3}{301}}
\citation{gelman2006prior}
\@writefile{lof}{\contentsline {figure}{\numberline {8.29}{\ignorespaces \textbf  {Left:} the prior distributions for half-cauchy(0,5) (green), uniform(0,100) (orange), inverse-$\chi ^2$(0.1) (red-dashed) and an inverse-gamma(0.0001,0.0001) (red). \textbf  {Right:} estimated posteriors using the left priors for a data sample of size 3 from an MCMC sample of 100,000 observations using Stan (see web for program to exactly replicate samples).\textbf  {Add legends}.\relax }}{304}}
\newlabel{fig:Distributions_halfCauchyPrior}{{8.29}{304}}
\@writefile{toc}{\contentsline {subsubsection}{Log-normal}{304}}
\citation{lewandowski2009generating}
\@writefile{lof}{\contentsline {figure}{\numberline {8.30}{\ignorespaces \textbf  {Left:} the negative binomial sampling distribution for $\mu =5$ with $\kappa =0.8, 5, 40$ in blue, orange and green respectively. \textbf  {Right:} log-normal prior distributions for $\kappa $, with $(\mu ,\sigma )$ given by (2,1), (2,2) and (3,1) in blue, orange and green. Note that the $\mu $ used in the right-hand graph is different to that one in the left.\textbf  {Add legends}.\relax }}{306}}
\newlabel{fig:Distributions_lognormalPrior}{{8.30}{306}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Distributions for covariance and correlation matrices}{306}}
\@writefile{toc}{\contentsline {subsubsection}{LKJ}{307}}
\newlabel{sec:Distributions_LKJ}{{8.5.4}{307}}
\citation{lewandowski2009generating}
\citation{stan-manual:2015}
\citation{lewandowski2009generating}
\citation{tokuda2011visualizing}
\citation{stan-manual:2015}
\citation{lewandowski2009generating}
\citation{hanea2013asymptotic}
\@writefile{lof}{\contentsline {figure}{\numberline {8.31}{\ignorespaces \textbf  {Left:} the distribution of $\rho _{12}$ for allowable correlation matrices, for a dimension of 2 (blue), 3 (orange), 4 (green) and 20 (red). \textbf  {Right:} a contour plot of the joint distribution of $\rho _{12}$ and $\rho _{13}$ for a correlation matrix of dimension 4. \textbf  {Add legends}.\relax }}{311}}
\newlabel{fig:Distributions_LKJ_correlations}{{8.31}{311}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.32}{\ignorespaces The LKJ distributions over allowable correlation matrices for left: $d=2$, centre $d=3$, and right: $d=4$ when $\eta =1$, $\eta =10$ (orange), and $\eta =0.5$ (green). \textbf  {Add legends. Also need to draw last figure from 0.9-1, as it was too difficult to find pdf here.}.\relax }}{312}}
\newlabel{fig:Distributions_LKJ_determinants}{{8.32}{312}}
\@writefile{toc}{\contentsline {subsubsection}{Wishart and inverse-Wishart}{313}}
\citation{tokuda2011visualizing}
\citation{tokuda2011visualizing}
\citation{tokuda2011visualizing}
\citation{tokuda2011visualizing}
\newlabel{fig:Distributions_invWishart}{{\caption@xref {fig:Distributions_invWishart}{ on input line 4694}}{314}}
\newlabel{fig:Distributions_Wishart}{{\caption@xref {fig:Distributions_Wishart}{ on input line 4701}}{314}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Choosing a likelihood made easy}{316}}
\@writefile{toc}{\contentsline {section}{\numberline {8.7}Table of common likelihoods, their uses, and reasonable priors}{316}}
\newlabel{sec:Distributions_table}{{8.7}{316}}
\@writefile{toc}{\contentsline {section}{\numberline {8.8}Distributions of distributions, and mixtures - link to website, and relevance}{316}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.35}{\ignorespaces \textbf  {How to choose a likelihood.} Blue text represents an appropriate choice of distribution. \textbf  {Want to have circles or squares round distributions, to make them stand out more.}\relax }}{317}}
\newlabel{fig:Distributions_likelihoodTree}{{8.35}{317}}
\@writefile{toc}{\contentsline {section}{\numberline {8.9}Chapter summary}{318}}
\@writefile{toc}{\contentsline {section}{\numberline {8.10}Chapter outcomes}{318}}
\@writefile{toc}{\contentsline {section}{\numberline {8.11}Problem set}{319}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.11.1}Drug trials}{319}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.11.2}100m results across countries}{320}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.11.3}Triangular representation of simplexes}{320}}
\@writefile{toc}{\contentsline {subsubsection}{Show that the triangular representation of a 3 parameter Dirichlet is correct.}{320}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.11.4}Normal distribution with normal prior}{321}}
\@writefile{toc}{\contentsline {subsubsection}{Prove that the mean of the lower distribution is the same as that of the prior}{321}}
\@writefile{toc}{\contentsline {subsubsection}{Find the mean of the lower distribution}{321}}
\@writefile{toc}{\contentsline {subsubsection}{Prove that the distribution of the lower distribution is unconditionally normal}{321}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conjugate priors and their place in Bayesian analysis}{323}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conjugate}{{9}{323}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Chapter mission statement}{323}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Chapter goals}{323}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}What is a conjugate prior and why are they useful?}{324}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces The priors, likelihoods and posteriors for across left: samples with different numbers of people with allergies, and right: the same sample with different priors. \textbf  {Add legends. Left assumes a beta(1,1) prior, with samples of (3,5,10) (blue, orange, green) out of 30 with allergies. Right assumes a sample of 3/30 was obtained, with priors given by (1,1), (5,5), (10,1), (blue, orange, green). We should actually label the posterior curves with their posteriors, since this is the point of this diagram. Left: Beta(4,28), Beta(6,26), Beta(11,21). Right: Beta(4,28), Beta(8,32), Beta(13,28) with colours (blue, orange, green).}\relax }}{326}}
\newlabel{fig:Conjugate_binomial}{{9.1}{326}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Gamma-poisson example}{328}}
\newlabel{eq:Conjugate_gammaPrior}{{9.9}{328}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Normal example: extra}{329}}
\newlabel{sec:conjugate_normal}{{9.5}{329}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces The effect of left: different samples, and right: different priors, on the posterior. \textbf  {Add legends. LH prior has $(\alpha ,\beta )= (10,1)$ with samples of size 5, with means of (20.8,53) for (blue,orange). RH priors have (10,1) and (140,2) respectively for (blue,orange), with a sample of mean 20.8 and size 5. Add the labels to the posterior curves, importantly, with Gamma(114,1/6) and Gamma(275,1/6) for the left hand side (blue,orange), and Gamma(114,1/6) and Gamma(405,1/7), again for (blue,orange).}\relax }}{330}}
\newlabel{fig:Conjugate_poissonGamma}{{9.2}{330}}
\citation{gelman2013bayesian}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Table of conjugate priors}{332}}
\newlabel{sec:Conjugate_tableConjugatePriors}{{9.6}{332}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}The lessons and limits of a conjugate analysis}{332}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Fitting blood iron data (histogram), using left: a normal distribution, and right: a t distribution.\relax }}{333}}
\newlabel{fig:Conjugate_lessonsNormT}{{9.3}{333}}
\@writefile{toc}{\contentsline {section}{\numberline {9.8}Chapter summary}{334}}
\@writefile{toc}{\contentsline {section}{\numberline {9.9}Chapter outcomes}{334}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Evaluation of model fit and hypothesis testing}{335}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ModelFit}{{10}{335}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Chapter mission statement}{335}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Chapter goals}{335}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Posterior predictive checks}{336}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}Recap - posterior predictive distributions}{336}}
\newlabel{sec:Evaluation_posteriorPredDist}{{10.3.1}{336}}
\newlabel{eq:Evaluation_posteriorPredictiveDistribution}{{10.1}{337}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Prior, likelihood, posterior and posterior predictive distributions for the birth weight example. Blue corresponds to a lower sampling variability ($\sigma ^2$) than the orange case. Note the $W$ label for the horizontal axis for the posterior predictive distribution.\relax }}{338}}
\newlabel{fig:Evaluation_normalPosteriorPredictiveWeight}{{10.1}{338}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Graphical PPCs and Bayesian p values}{338}}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {subsubsection}{Amazon staff hiring}{339}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces The real weekly order volume for Amazon in the UK over the past year (blue), and 15 simulated yearly data sets from the posterior predictive distribution of our model. The blue dotted line in each case indicates the value of the highest weekly order volume from the real data. If the sample has at least a single data point more extreme than this value, then the graph is shown in green, and red otherwise.\relax }}{341}}
\newlabel{fig:Evaluation_amazonExample}{{10.2}{341}}
\@writefile{toc}{\contentsline {subsubsection}{Mosquito lifetime}{342}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Top row: The real recaptures of marked mosquitoes (red dots), vs simulations from the posterior predictive distribution, across both binomial, and beta-binomial models. Bottom row: Residual of simulated data vs actual for both model types, over time.\relax }}{343}}
\newlabel{fig:Evaluation_mosquitoPPC}{{10.3}{343}}
\@writefile{toc}{\contentsline {subsubsection}{Snow days}{343}}
\citation{mimno2011bayesian}
\citation{blei2003latent}
\citation{blei2003latent}
\citation{mimno2011bayesian}
\@writefile{toc}{\contentsline {subsubsection}{Word distributions within topics}{344}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces The real data set (blue), and 15 fake data samples from the Bernoulii model. In each case the average number of days in freezing spells, that occur in the data. If the fake data has an average freezing spell length that is greater than the real data, then we colour the graph green, and red otherwise.\relax }}{345}}
\newlabel{fig:Evaluation_snowDays}{{10.4}{345}}
\citation{mimno2011bayesian}
\citation{blei2003latent}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Frequencies of the word \textit  {kitchen} across 7 different documents, with simulations from the posterior predictive distributions for the LDA model shown as box and whisker charts.\relax }}{346}}
\newlabel{fig:Evaluation_wordPPC}{{10.5}{346}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces The real data set (blue), and 15 fake data samples from the Bernoulii model. If the number of infected samples in your fake data set exceeds the real, then we colour the graph red, and green otherwise.\relax }}{347}}
\newlabel{fig:Evaluation_bacteriaPPC}{{10.6}{347}}
\@writefile{toc}{\contentsline {subsubsection}{Bacterial infection}{347}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces The real data set (blue), and 15 fake data samples from the Bernoulii model. If the number of separate connected blocks of infection exceeds that of the real, then we colour the graph red, and green otherwise.\relax }}{349}}
\newlabel{fig:Evaluation_bateriaPPCArray}{{10.7}{349}}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Why do we call it a \textit  {p value}?}{350}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Statistics measuring predictive accuracy: AIC, Deviance, WAIC and LOO}{351}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}Independent evaluation of a model, and overfitting}{351}}
\citation{gelman2014understanding}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}How to measure a model's predictive capability?}{352}}
\citation{gelman2013bayesian}
\newlabel{eq:Evaluation_posteriorPredictiveDist}{{10.4}{353}}
\newlabel{eq:Evaluation_logPosteriorPred}{{10.5}{353}}
\newlabel{eq:Evaluation_logLikelihood}{{10.6}{353}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.3}The ideal measure of a model's predictive accuracy}{354}}
\newlabel{sec:Evaluation_idealAccuracy}{{10.5.3}{354}}
\newlabel{eq:Evaluation_elpd}{{10.7}{354}}
\newlabel{eq:Evaluation_KL}{{10.8}{354}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces The expected log predictive density (elpd) in top panel, as a function of the posterior predictive density (bottom). The true distribution - $f(y)$ in (10.7\hbox {}) - is shown in the middle in blue/green.\relax }}{355}}
\newlabel{fig:Evaluation_elpd}{{10.8}{355}}
\newlabel{eq:Evaluation_KLelpdDeriv}{{10.10}{356}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Top panels show a distribution $p(x)$ (blue) against two distributions: $q(x)$ and $q'(x)$ (both orange). The bottom panels show the products: $p(x)\frac  {p(x)}{q(x)}$ and $p(x)\frac  {p(x)}{q'(x)}$, which are then integrated (see (10.8\hbox {})) to yield the KL divergence from $p(x)$ to the other distributions, which are shown in each case.\relax }}{357}}
\newlabel{fig:Evaluation_KLDivergence}{{10.9}{357}}
\newlabel{eq:Evaluation_elppd}{{10.11}{357}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces Left: the fit of a linear model (green) and nonlinear model (orange) to data. Right: the performance of each model when applied to a new data set. The nonlinear model does not generalise as well as the linear one to new data.\relax }}{358}}
\newlabel{fig:Evaluation_overfit}{{10.10}{358}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.4}Estimating out-of-sample predictive accuracy from in-sample data}{358}}
\citation{gelman2013bayesian}
\citation{spiegelhalter2002bayesian}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.5}AIC}{359}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.6}DIC}{359}}
\citation{gelman2013bayesian}
\newlabel{eq:Evaluation_DIC}{{10.13}{360}}
\newlabel{eq:Evaluation_DIC1}{{10.14}{360}}
\newlabel{eq:Evaluation_DIC2}{{10.15}{360}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.7}WAIC}{360}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces Top: the posterior density for a normal model with a single observation. Middle: the actual expected log pointwise predictive density (elppd) in green vs the log predictive density at the maximum likelihood estimates (orange), the log predictive density at the posterior median parameter estimates (black), and the log pointwise predictive density (grey). Bottom: the middle panel after bias corrections showing the AIC (orange), DIC (black) and WAIC (grey) estimates of the expected log pointwise predictive density (green).\relax }}{361}}
\newlabel{fig:Evaluation_DIC}{{10.11}{361}}
\citation{gelman2013bayesian}
\citation{gelman2013bayesian}
\citation{gelman2013bayesian}
\newlabel{eq:Evaluation_WAIC}{{10.17}{362}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Bottom: the true posterior predictive density (blue solid line) vs posterior predictive densities after a single data point (dashed lines). Top: the resultant log posterior predictive density (grey) vs the expected log posterior predictive density (green) assuming a normal likelihood, and prior. In all cases, the lppd overestimates the elppd.\relax }}{363}}
\newlabel{fig:Evaluation_lppdOverconfidence}{{10.12}{363}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces A hypothetical posterior with only two potential parameter values (top), and the WAIC bias correction in both cases. The left two panels show this for high uncertainty, and the right for lower uncertainty.\textbf  {Add an arrow showing difference between the two dotted lines in each case in the bottom figures.}\relax }}{364}}
\newlabel{fig:Evaluation_WAICBiasCorrection}{{10.13}{364}}
\citation{gelman2013bayesian}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.8}LOO}{365}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.9}A practical and short summary of measures of predictive accuracy in simple terms}{366}}
\newlabel{sec:Evaluation_practicalSimplePredictiveAcc}{{10.5.9}{366}}
\citation{vehtari2015efficient}
\citation{gelman2013bayesian}
\citation{gelman2014understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces A representation of the different methods available for estimating the predictive accuracy of a model. The more blue the colour, the more Bayesian the method. The idea for this diagram came from a discussion on the Stan User Group forum, in particular due to suggestions from Michael Betancourt.\relax }}{367}}
\newlabel{fig:Evaluation_criteriaDiagram}{{10.14}{367}}
\@writefile{toc}{\contentsline {section}{\numberline {10.6}Choosing one model, or a number?}{368}}
\@writefile{toc}{\contentsline {section}{\numberline {10.7}Sensitivity analysis}{368}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.15}{\ignorespaces Left: the past 100 days of rainfall. Right: the maximum number of consecutive rainy days from simulations from the simple rainfall model (blue), and the more complex one (orange).\relax }}{370}}
\newlabel{fig:Evaluation_sensitivityRainfall}{{10.15}{370}}
\@writefile{toc}{\contentsline {section}{\numberline {10.8}Chapter summary}{370}}
\@writefile{toc}{\contentsline {section}{\numberline {10.9}Chapter outcomes}{371}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Making Bayesian analysis objective?}{373}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ObjectiveBayes}{{11}{373}}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Chapter mission statement}{373}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Chapter goals}{373}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}The illusion of the 'uninformative' uniform prior}{374}}
\newlabel{sec:Objective_illusionUniformUninform}{{11.3}{374}}
\newlabel{eq:Objective_jacobianTransform}{{11.1}{374}}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Jeffreys' priors}{375}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Another definition of 'uninformative'}{375}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Left: a uniform prior (blue) and Jeffrey's prior (orange) for a Bernoulli probability parameter, which represents the probability an individual is obese. Right: the inferred priors for the probability of an event that both people are obese in corresponding colours.\relax }}{376}}
\newlabel{fig:Objective_uniformPrior}{{11.1}{376}}
\newlabel{eq:Objective_jeffreysPrior}{{11.4}{376}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces Left: the $Beta\left (\frac  {1}{2},\frac  {1}{2}\right )$ distribution (grey) vs the Bernoulli standard deviation (black). Right: typical posteriors from a random sample of size 100, from $\theta =\frac  {1}{10}$ (blue), and $\theta =\frac  {1}{2}$ (orange), along with their respective 95\% central posterior intervals.\relax }}{379}}
\newlabel{fig:Objective_jeffreysBeta}{{11.2}{379}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}Being critical}{379}}
\citation{bernardo1979reference}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces The difference between using a uniform prior (blue) vs a Jeffrey's prior (orange) for different sample sizes, where in each case the proportion of obese cases is the same.\relax }}{380}}
\newlabel{fig:Objective_jeffreysPrior}{{11.3}{380}}
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Reference priors}{380}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Left: The reference prior for the exponential distribution, $p(\theta )=\frac  {1}{\theta }$, (blue), and a Gamma(4,2) prior (orange). The effects of the respective priors on posterior distributions with sample sizes middle: $n=10$, and right: $n=100$. The data samples were randomly drawn, $X\sim Exp(\frac  {1}{2})$. Note here that the reference prior distribution is improper.\relax }}{381}}
\newlabel{fig:Objective_referencePriorsExponential}{{11.4}{381}}
\citation{casella1985introduction}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}Empirical Bayes}{382}}
\citation{casella1985introduction}
\newlabel{eq:Objective_empiricalBayes}{{11.11}{383}}
\newlabel{eq:Objective_bayesEstimator}{{11.12}{383}}
\@writefile{toc}{\contentsline {section}{\numberline {11.7}A move towards weakly informative priors}{384}}
\@writefile{toc}{\contentsline {section}{\numberline {11.8}Chapter summary}{385}}
\@writefile{toc}{\contentsline {section}{\numberline {11.9}Chapter outcomes}{386}}
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}A practical guide to doing real life Bayesian analysis: Computational Bayes}{387}}
\newlabel{part:computationalBayes}{{IV}{389}}
\@writefile{toc}{\contentsline {section}{\numberline {11.10}Part mission statement}{389}}
\@writefile{toc}{\contentsline {section}{\numberline {11.11}Part goals}{389}}
\citation{stan-software:2014}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Leaving conjugates behind: Markov Chain Monte Carlo}{391}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:MCMC}{{12}{391}}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Chapter mission statement}{391}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Chapter goals}{391}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}The difficulty with real life Bayesian inference}{393}}
\newlabel{sec:MCMC_difficultyBayesPosterior}{{12.3}{393}}
\newlabel{eq:MCMC_denominatorBSE}{{12.3}{393}}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Discrete approximation to continuous posteriors}{394}}
\newlabel{sec:MCMC_discreteApproximation}{{12.4}{394}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Left: the actual prior, likelihood and posterior for the BSE problem with one datum equal to 7. Middle and Right: Discrete prior, likelihood and posterior(=likelihood x prior divided by their sum) for the same problem, at intervals of 2 units (middle), and 0.5 units (right).\textbf  {ADD LINES TO SHOW THAT POSTERIOR IS ESSENTIALLY JUST LIKELIHOOD TIMES PRIOR.}\relax }}{395}}
\newlabel{fig:MCMC_discreteApproxBSE}{{12.1}{395}}
\@writefile{lot}{\contentsline {table}{\numberline {12.1}{\ignorespaces A Bayes box illustrating the calculation of the posterior, and posterior mean for the BSE example, with a single data of 7.\relax }}{396}}
\newlabel{tab:MCMC_discreteApproxBSE}{{12.1}{396}}
\@writefile{toc}{\contentsline {section}{\numberline {12.5}The posterior through quadrature}{397}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces Left: the actual prior, likelihood and posterior for the politician expenditure example with data given by x=(105,100). Right: discrete prior, likelihood and posterior(=likelihood x prior divided by their sum) for the same problem.\textbf  {ADD LINES TO SHOW THAT POSTERIOR IS ESSENTIALLY JUST LIKELIHOOD TIMES PRIOR.}\relax }}{398}}
\newlabel{fig:MCMC_discreteApprox2DPolitician}{{12.2}{398}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces Rectangular quadrature in action: as the interval width decreases, there is a corresponding improvement in the error.\relax }}{399}}
\newlabel{fig:MCMC_quadratureBSE}{{12.3}{399}}
\@writefile{toc}{\contentsline {section}{\numberline {12.6}Integrating using independent samples: an introduction to Monte Carlo}{399}}
\newlabel{sec:MCMC_integratingUsingIndependentSampling}{{12.6}{399}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.1}BSE revisited}{402}}
\newlabel{eq:MCMC_denominatorBSE2}{{12.13}{402}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces Using independent sampling to estimate the denominator in the BSE example: as the number of independent samples increases, there is a corresponding improvement in the error. The orange dotted line shows the true value of the denominator, and the histogram show counts of values obtained at each sample size across 10,000 repetitions.\relax }}{403}}
\newlabel{fig:MCMC_BSEindependentSampling}{{12.4}{403}}
\newlabel{fig:MCMC_blackBox}{{\caption@xref {fig:MCMC_blackBox}{ on input line 5978}}{404}}
\@writefile{toc}{\contentsline {section}{\numberline {12.7}Why is independent sampling easier said than done?}{404}}
\newlabel{sec:MCMC_independentSamplingDifficult}{{12.7}{404}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces \textbf  {Using Rejection Sampling to generate samples from an exponential(1) distribution.} Left: independent $(x,y)\sim (U(0,8),U(0,1))$ samples and the exponential PDF (blue). The points are coloured blue if they are accepted, (if the PDF exceeds the $y$ value), and red otherwise. Right: shows a stacked histogram for accepted (blue), and rejected (red) points.\relax }}{405}}
\newlabel{fig:MCMC_rejectionSamplingExponential}{{12.6}{405}}
\@writefile{toc}{\contentsline {section}{\numberline {12.8}Ideal sampling from a posterior using only the unnormalised posterior}{406}}
\newlabel{sec:MCMC_idealSampling}{{12.8}{406}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.7}{\ignorespaces \textbf  {Using inverse transform sampling to sample from an exponential(1) distribution.} The top-left panel shows 1,000 samples generated from a uniform distribution, arranged in the order they were generated. The bottom-left shows the exponential samples that result from transforming the above using the exponential-inverse-CDF, with the red lines showing the amount they were moved by the transform. The histograms show the sampling distribution for both sets: original and transformed.\relax }}{407}}
\newlabel{fig:MCMC_inverseSamplingExponential}{{12.7}{407}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.8}{\ignorespaces A PDF (orange) and its histogram representation through sampling (blue).\relax }}{408}}
\newlabel{fig:MCMC_ideaHistogram}{{12.8}{408}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.8.1}The unnormalised posterior - a window onto the real deal}{409}}
\newlabel{sec:MCMC_unnormalisedPosteriorSampling}{{12.8.1}{409}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.9}{\ignorespaces A family of distributions, each with the similar shape, but with the upper surfaces stretched, according to different values of the denominator term.\relax }}{410}}
\newlabel{fig:MCMC_familityOfDistributionsShape}{{12.9}{410}}
\@writefile{toc}{\contentsline {section}{\numberline {12.9}Moving from independent to dependent sampling}{410}}
\newlabel{sec:MCMC_independentToDependent}{{12.9}{410}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.9.1}An analogy to MCMC: mapping mountains}{412}}
\@writefile{toc}{\contentsline {section}{\numberline {12.10}What's the catch with dependent samplers?}{413}}
\newlabel{sec:MCMC_correlationInDependentSamplers}{{12.10}{413}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.10}{\ignorespaces \textbf  {Visualising the \textit  {tilting} sampling methodology for a six-faced die, across three values of transition bias.} The edges represent possible transitions, with thicker edges representing more probable transitions.\relax }}{414}}
\newlabel{fig:MCMC_dieDependentMCMC}{{12.10}{414}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.11}{\ignorespaces Left and Middle: Average errors in estimating the mean of a fair die, with 6, or 8 faces respectively, across a range of sample sizes, and sampler dependencies (blue represents independent samples, and orange represents the case where only consecutive transitions are allowed). Right: dependent sample size vs effective sample size, for a 6-faced (black) or 8-faced (grey) fair die. A further black-dashed line is added to the right-hand plot to indicate the $y=x$ line for an independent sampler. The coloured dashed lines across all plots indicate the calculation of effective sample sizes for a six-faced (red) or eight-faced (green) die.\textbf  {Add legends.}\relax }}{416}}
\newlabel{fig:MCMC_dependentCorrelationDie}{{12.11}{416}}
\@writefile{toc}{\contentsline {section}{\numberline {12.11}Chapter summary}{417}}
\@writefile{toc}{\contentsline {section}{\numberline {12.12}Chapter outcomes}{418}}
\@writefile{toc}{\contentsline {section}{\numberline {12.13}Problem set}{418}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.13.1}Prove that the inverse-transform sampler works!}{418}}
\newlabel{prob:MCMC_inverseSamplerProof}{{12.13.1}{418}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.13.2}Computationally investigate the die example}{418}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.13.3}Evaluate the mean and variance of the posterior for the BSE example}{418}}
\newlabel{sec:MCMC_problemBSE}{{12.13.3}{418}}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}The Metropolis algorithm}{419}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:metropolisHastings}{{13}{419}}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Chapter mission statement}{419}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}Chapter goals}{419}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces Left: The connectivity of the four lakes for the Robinson fishing example. Note that Robinson cannot actually see the fish, only the size of the lakes. Right: the total number of fish in each pond.\relax }}{421}}
\newlabel{fig:metropolisHastings_fishInPonds}{{13.1}{421}}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Sustainable fishing}{421}}
\@writefile{toc}{\contentsline {section}{\numberline {13.4}Prospecting for gold}{422}}
\newlabel{sec:metropolisHastings_gold}{{13.4}{422}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces \textbf  {The Metropolis algorithm in action.} Left: The sequence of lakes in which Robinson fishes (A,B,C,D) over time. Right: the total number of fish he catches. Top: for 50 days, and bottom for 500 days.\relax }}{423}}
\newlabel{fig:metropolisHastings_MetropolisFish}{{13.2}{423}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.3}{\ignorespaces The desert above, with the gold deposits underneath. The red path shows a possible sampling path that could be taken by our intrepid worker. \textbf  {Add better palm trees, and illustrate that the worker has a device which allows them to survey underneath. REPLACE NORTH FOR WEST!}\relax }}{424}}
\newlabel{fig:metropolisHastings_goldMineSimple}{{13.3}{424}}
\@writefile{toc}{\contentsline {section}{\numberline {13.5}Defining the Metropolis algorithm}{425}}
\newlabel{sec:metropolisHastings_definingMetropolis}{{13.5}{425}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.4}{\ignorespaces \textbf  {Searching for gold, using the Metropolis algorithm.} Left panels represent actual gold deposits. All other panels represent MCMC sampling results using Metropolis algorithm. Top panel shows path taken through the desert (red). REPLACE NORTH FOR WEST!\relax }}{426}}
\newlabel{fig:metropolisHastings_miningGoldMetropolis}{{13.4}{426}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.5}{\ignorespaces \textbf  {Comparing step accept/reject mechanisms, with top corresponding to $p_{accept}=1$, (a drunkard's random walk), middle has $p_{accept}=min(1,\frac  {p(\theta _{new}|data)}{p(\theta _{current}|data)})$ corresponding to a Metropolis stepping algorithm, and bottom shows a Hillary stepper, which has $p_{accept}=1$ if and only if $p(\theta _{new}|data)>p(\theta _{current}|data)$}. Left hand plots blue lines are the path taken by each stepping algorithm over time, the histograms (left-middle) then show the binned samples, and the dashed-blue lines show the reconstructed posterior for each set of samples. The right-hand orange plot show the actual posterior, assumed to be the same in each case. \textbf  {Align axes, add labels and titles. REPLACE NORTH FOR WEST!}\relax }}{427}}
\newlabel{fig:metropolisHastings_definingMetropolis}{{13.5}{427}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.6}{\ignorespaces \textbf  {Exploring a landscape using a Metropolis algorithm.} Note the time here is computational runtime, and two of the plots are within the same computational time step ($t=2,t=4$). Blue arrows indicate the current proposed step; grey figures illustrate a proposed step which is probabilistic, black are deterministic.\relax }}{429}}
\newlabel{fig:metropolisHastings_walker}{{13.6}{429}}
\@writefile{toc}{\contentsline {section}{\numberline {13.6}When does Metropolis work?}{429}}
\newlabel{sec:metropolisHastings_conditionsForMetropolis}{{13.6}{429}}
\citation{chib1995understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {13.7}{\ignorespaces \textbf  {Convergence to a posterior in MCMC.} We start here with a uniform distribution over possible starting values of each Markov Chain (top-left), and as we repeatedly apply the transition operator, we eventually converge to a stationary distribution that is the posterior\let \reserved@d =[\def \par }}{430}}
\newlabel{fig:metropolisHastings_convergenceToPosterior}{{13.7}{430}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.1}Mathematical underpinnings of MCMC}{431}}
\newlabel{sec:metropolisHastings_mathematicalMCMC}{{13.6.1}{431}}
\citation{gelman2013bayesian}
\citation{gebali2008}
\citation{gebali2008}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.2}Desirable qualities of a Markov Chain}{432}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.3}Detailed balance}{432}}
\newlabel{sec:metropolisHastings_detailedBalance}{{13.6.3}{432}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.8}{\ignorespaces \textit  {Examples of reducible (left) and periodic chains (right).} Left: the fish once they enter the net cannot escape, and visit other parts of the ocean \cite  {gebali2008}. Right: a rollercoaster is constrained to (hopefully!) cycle forever more on its track.\relax }}{433}}
\newlabel{fig:metropolisHastings_reduciblePeriodic}{{13.8}{433}}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.4}The intuition behind the acceptance/rejection rule of Metropolis, and detailed balance}{435}}
\newlabel{sec:metropolisHastings_correctAcceptRejectRuleDetailedBalance}{{13.6.4}{435}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.9}{\ignorespaces \textbf  {How the choice of acceptance/rejection rule determines the distance of our sampler's distribution from the posterior.} Note this is a conceptual representation, and hence should not be interpreted quantitatively!\relax }}{436}}
\newlabel{fig:metropolisHastings_knifeEdge}{{13.9}{436}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.10}{\ignorespaces \textbf  {The principle of detailed balance explained through swimming.} Immediately after a chemical spill (left), the chemical particles are relatively localised in the non-swimming zone, and there is a net flux of particles from this area into the swimming area. After some time, the chemical particles reach equilibrium, with roughly similar numbers of them in each zone, and a \textit  {net} flux of zero from one zone into the other. Hint: we can think of the chemical particles as representing probability mass. \textbf  {Add detail to this cartoon, with people swimming, fish etc.}\relax }}{437}}
\newlabel{fig:metropolisHastings_swimmingDetailedBalance}{{13.10}{437}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.5}Proving things we know to be true: Mathematics revisited}{437}}
\citation{chib1995understanding}
\@writefile{toc}{\contentsline {section}{\numberline {13.7}Efficiency of convergence: the importance of choosing the right proposal scale}{438}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.11}{\ignorespaces The symmetry of a normal distribution as a proposal makes it an attractive choice.\relax }}{439}}
\newlabel{fig:metropolisHastings_normalProposal}{{13.11}{439}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.12}{\ignorespaces \textbf  {The effect of proposal step size on sampling, for a too-small step length (top), too large a step length (bottom), and just-right (middle).} The left plot illustrates the path of the sampler over time, the left-middle histogram plot shows the distribution of binned samples, with the right-middle plot showing the reconstructed posterior. The right plot (orange) shows the actual posterior. The samplers were all started at the same point, away from the bulk of probability mass.\relax }}{440}}
\newlabel{fig:metropolisHastings_rateOfConvergence}{{13.12}{440}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.13}{\ignorespaces \textbf  {Comparing the performance of different step sizes for the Metropolis sampler: too-small (red), just-right (green), and too-large (blue), against independent samples from the posterior (grey)}. Where available the theoretical optimal independent sampling strategy is shown as a dashed grey line.\relax }}{441}}
\newlabel{fig:metropolisHastings_rateOfConvergence1}{{13.13}{441}}
\citation{roberts1997weak}
\citation{rosenthal2011optimal}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.7.1}MCMC as finding and exploring the typical set\let \reserved@d =[\def \par }{442}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.7.2}Speeding up convergence: tuning the proposal distribution}{443}}
\newlabel{sec:metropolisHastings_tuneAcceptRejectRule}{{13.7.2}{443}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.14}{\ignorespaces \textbf  {Illustrating an adaptive Metropolis Markov Chain in action.} The step size is adjusted at regular intervals; subtracting a fixed amount off the proposal distribution (a normal distribution) $\sigma $, if the acceptance rate is below optimum at 0.44, conversely adding the amount if the acceptance rate is above this threshold. The left/right panels pertain to a sampler that begins with a too-small/too-large step size.\relax }}{444}}
\newlabel{fig:metropolisHastings_acceptanceRate}{{13.14}{444}}
\citation{saidi2006stationarity}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.7.3}Geometric convergence of Markov Chains}{445}}
\newlabel{sec:metropolisHastings_geometricConvergenceMarkovChain}{{13.7.3}{445}}
\citation{gelman1992inference}
\@writefile{toc}{\contentsline {section}{\numberline {13.8}Judging convergence}{446}}
\newlabel{sec:metropolisHastings_convergence}{{13.8}{446}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.1}Bob's bees in a house\let \reserved@d =[\def \par }{446}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.15}{\ignorespaces \textbf  {Using Bob's bees to produce blueprints of a house.} Top/bottom panels show the effect of using a single/multiple bees to randomly traverse the house. Right panels show blueprint that is reconstructed from the paths of the bees. \textbf  {If drawing image again, make sure that blue bee's paths are the same in top and bottom. There should also be more path mixing in the last case. Also add detail to house - stairs etc.}\relax }}{449}}
\newlabel{fig:metropolisHastings_bobBees}{{13.15}{449}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.2}Using multiple chains to monitor convergence}{449}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.16}{\ignorespaces \textbf  {Non-mixing (top) and better-mixing (bottom) of strings, before they are recoloured (left) and after (right).} In both cases the histogram corresponds to the count of the variable at that particular distance along the axis.\relax }}{451}}
\newlabel{fig:metropolisHastings_chainMixing}{{13.16}{451}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {13.17}{\ignorespaces \textbf  {The within variation for the blue string (W), vs the between-all-strings variation (B), for the case of non-mixing (left), and mixing strings (right).}\relax }}{452}}
\newlabel{fig:metropolisHastings_betweenWithinVarianceString}{{13.17}{452}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.3}Using within- and between-chain variation to estimate convergence}{452}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {13.18}{\ignorespaces \textbf  {Illustrating the calculation of within- and between-chain variance.}\relax }}{453}}
\newlabel{fig:metropolisHastings_withinBetweenExplicit}{{13.18}{453}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.19}{\ignorespaces \textbf  {Non-convergence due to left: no mixing between chains, and right: poor within-chain mixing.}\relax }}{454}}
\newlabel{fig:metropolisHastings_twoTypesNonConvergence}{{13.19}{454}}
\newlabel{eq:metropolisHastings_Rhat}{{13.16}{454}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.4}Two types of non-convergence}{454}}
\citation{stan-manual:2015}
\@writefile{lof}{\contentsline {figure}{\numberline {13.20}{\ignorespaces \textbf  {Diagnosing intra-chain non-convergence by chain splitting}.\relax }}{455}}
\newlabel{fig:metropolisHastings_splitChains}{{13.20}{455}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.5}Warm-up}{455}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.21}{\ignorespaces \textbf  {Measuring convergence as chains (left) run}. Middle panel shows the two components of the numerator of eqn. (13.16\hbox {}; black is due to within-chain variation, and grey is due to between. Right shows $\mathaccentV {hat}05E{R}$ computed as chain runs. In this case we would discard the first 400-500 samples, since from the left graph, we see that our chains have yet to converge. \relax }}{456}}
\newlabel{fig:metropolisHastings_warmUp}{{13.21}{456}}
\@writefile{toc}{\contentsline {section}{\numberline {13.9}Effective sample size revisited}{457}}
\newlabel{sec:MH_effectiveSampleSizeRevisited}{{13.9}{457}}
\citation{gelman2013bayesian}
\citation{stan-manual:2015}
\citation{bergland1969guided}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.9.1}Thinning samples to increase effective sample size}{459}}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {13.10}Chapter summary}{460}}
\citation{brooks2011handbook}
\@writefile{toc}{\contentsline {section}{\numberline {13.11}Chapter outcomes}{462}}
\@writefile{toc}{\contentsline {section}{\numberline {13.12}Problem set}{463}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.12.1}Metropolis-Hastings example}{463}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.12.2}Find the optimal rejection probability = Gelman says 0.44 for unimodal}{463}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.12.3}Adaptive MCMC}{463}}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Gibbs sampling}{465}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Gibbs}{{14}{465}}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Chapter mission statement}{465}}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Chapter goals}{465}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces \textbf  {A map of the underground gold deposits for described in section 14.3\hbox {}.} The blue lines show the directions that the satellite can scan along.\relax }}{467}}
\newlabel{fig:Gibbs_goldMiningAgain1}{{14.1}{467}}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Back to prospecting for gold}{467}}
\newlabel{sec:Gibbs_gold}{{14.3}{467}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces \textbf  {A contour map of the actual gold deposits(left), and a reconstructed map of deposits produced via the Metropolis sampler (middle), and Gibbs sampler (right).} The red lines show the paths through the desert for a person using either method.\relax }}{468}}
\newlabel{fig:Gibbs_goldMiningAgain2}{{14.2}{468}}
\citation{gelman2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}Defining the Gibbs algorithm}{469}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces \textbf  {Histograms of samples produced via the Metropolis (left), and Gibbs (right) samplers, across a range of iterations.}\relax }}{470}}
\newlabel{fig:Gibbs_goldMiningAgain3}{{14.3}{470}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.1}Crime and punishment/unemployment}{472}}
\newlabel{sec:Gibbs_crimeUnemployment}{{14.4.1}{472}}
\@writefile{toc}{\contentsline {section}{\numberline {14.5}Gibbs' earth: the intuition behind the Gibbs algorithm}{472}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.4}{\ignorespaces \textbf  {Actual posterior (left), with two distributions reconstructed from Gibbs' sampling, for a sample size of 100 (middle), and 1,000 (right).} In the left hand plot the lines illustrate the shape of the conditional densities here.\relax }}{473}}
\newlabel{fig:Gibbs_crimeMultivariateNormalSlice}{{14.4}{473}}
\@writefile{toc}{\contentsline {section}{\numberline {14.6}The benefits and problems with Gibbs and Random Walk Metropolis}{474}}
\newlabel{sec:Gibbs_benefitsProblemsGibbs}{{14.6}{474}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.5}{\ignorespaces \textbf  {Metropolis (middle) and Gibbs' (right) reconstructed densities for two different distributions (low correlation, top left, high correlation, bottom left panels).}\relax }}{475}}
\newlabel{fig:Gibbs_correlationProblemCrime}{{14.5}{475}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.6}{\ignorespaces \textbf  {Gibbs sampling on two different parameter spaces.} \textbf  {Add rotated axes to the left hand figure.}\relax }}{476}}
\newlabel{fig:Gibbs_reparametersCrime}{{14.6}{476}}
\@writefile{toc}{\contentsline {section}{\numberline {14.7}A change of parameters to speed up exploration}{476}}
\newlabel{sec:Gibbs_reparameters}{{14.7}{476}}
\citation{brooks2011handbook}
\@writefile{toc}{\contentsline {section}{\numberline {14.8}Chapter summary}{477}}
\@writefile{toc}{\contentsline {section}{\numberline {14.9}Chapter outcomes}{478}}
\@writefile{toc}{\contentsline {section}{\numberline {14.10}Problem set}{478}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.10.1}Prove that the Gibbs sampler can be viewed as a case of Metropolis-Hastings}{478}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.10.2}Prove that a Gibbs sampler on bivariate normal has an effective sample size of half its true sample size}{478}}
\@writefile{toc}{\contentsline {section}{\numberline {14.11}Appendix}{478}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.11.1}Derivation of conditional distributions for multivariate normal}{478}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.11.2}Derive uncorrelated normals from bivariate normal}{478}}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Hamiltonian Monte Carlo}{479}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:HMC}{{15}{479}}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}Chapter mission statement}{479}}
\@writefile{toc}{\contentsline {section}{\numberline {15.2}Chapter goals}{479}}
\citation{homan2014no}
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Hamiltonian Monte Carlo as a sledge}{480}}
\citation{brooks2011handbook}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.1}Rewriting our problem in the language of Statistical Mechanics\let \reserved@d =[\def \par }{482}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces \textbf  {(a) energy levels of discrete state system, and (b) how a canonical ensemble represents a summation over the states of a number of identical copies of our system.}\relax }}{484}}
\newlabel{fig:HMC_canonicalEnergyLevels}{{15.1}{484}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.2}{\ignorespaces \textbf  {How the distribution over energy states of our system changes with temperature.}\relax }}{484}}
\newlabel{fig:HMC_temperatureEnsemble}{{15.2}{484}}
\newlabel{eq:HMC_HMCCanonical}{{15.2}{485}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.2}Choosing the potential and kinetic energy terms}{485}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.3}{\ignorespaces \textbf  {The location (top) and momentum (arrows) of our sledge at a point in NLP space.} \textbf  {Replace point with sledge.}\relax }}{486}}
\newlabel{fig:HMC_idealised2DSystem}{{15.3}{486}}
\newlabel{eq:HMC_KE}{{15.5}{486}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.3}Simulating sledge movement in NLP space using the Leapfrog algorithm}{487}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.4}{\ignorespaces \textbf  {Approximating the true gradient (blue), by a difference method (shown in red).}\relax }}{488}}
\newlabel{fig:HMC_differential}{{15.4}{488}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.5}{\ignorespaces \textbf  {Using the Euler method to numerically solve for our path over time.} The arrows here indicate the overall momentum at each iteration.\relax }}{489}}
\newlabel{fig:HMC_pathOverTime}{{15.5}{489}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.4}Approximating our path in a way that conserves volume}{489}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.5}Revising our acceptance rule}{490}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.6}Putting it all together: the full HMC algorithm}{490}}
\newlabel{sec:HMC_fullHMCalgorithm}{{15.3.6}{490}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.7}Competing with Random Walk Metropolis and Gibbs}{490}}
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Avoiding manual labour: the No-U-turn sampler}{490}}
\@writefile{toc}{\contentsline {section}{\numberline {15.5}Riemannian MCMC}{490}}
\@writefile{toc}{\contentsline {section}{\numberline {15.6}Multimodality}{490}}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Stan and JAGS}{491}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:StanJAGS}{{16}{491}}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Motivation for Stan and JAGS}{491}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.1}Their similarities and differences}{491}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.2}The future}{491}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.3}The nuts and bolts of Stan and JAGS: access through R}{491}}
\@writefile{toc}{\contentsline {subsubsection}{How to get packages}{491}}
\@writefile{toc}{\contentsline {subsubsection}{How to package data}{491}}
\@writefile{toc}{\contentsline {section}{\numberline {16.2}Stan}{491}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.1}How to download, and install}{491}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.2}A first program in Stan}{491}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.3}The building blocks of a Stan program}{492}}
\@writefile{toc}{\contentsline {subsubsection}{Parameters}{492}}
\@writefile{toc}{\contentsline {subsubsection}{Model}{492}}
\@writefile{toc}{\contentsline {subsubsection}{Transformed parameters}{492}}
\@writefile{toc}{\contentsline {subsubsection}{Transformed data}{492}}
\@writefile{toc}{\contentsline {subsubsection}{Generated quantities}{492}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.4}A simple model with data}{492}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.5}Diagnostics}{492}}
\@writefile{toc}{\contentsline {subsubsection}{$\mathaccentV {hat}05E{R}$ and effective sample size}{492}}
\@writefile{toc}{\contentsline {subsubsection}{Divergent iterations}{492}}
\@writefile{toc}{\contentsline {subsubsection}{Tree depth exceeding maximum}{492}}
\@writefile{toc}{\contentsline {subsubsection}{Leapfrog steps}{492}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.6}More complex models with array indexing}{492}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.7}Essential Stan reading}{492}}
\@writefile{toc}{\contentsline {subsubsection}{The importance of log probability incrementing}{493}}
\@writefile{toc}{\contentsline {subsubsection}{Marginalising of discrete parameters}{493}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.8}Calculating LOO-CV and WAIC}{493}}
\@writefile{toc}{\contentsline {section}{\numberline {16.3}JAGS}{493}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.1}How to download, and install}{493}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.2}A first program in JAGS}{493}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.3}The literal meaning of a for loop}{494}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.4}A simple model}{494}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.5}Diagnosis}{494}}
\@writefile{toc}{\contentsline {subsubsection}{In-built using vanilla R}{494}}
\@writefile{toc}{\contentsline {subsubsection}{coda}{494}}
\@writefile{toc}{\contentsline {subsubsection}{More complex model}{494}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.6}Essential JAGS reading}{494}}
\@writefile{toc}{\contentsline {subsubsection}{Controlling the flow without if conditions}{494}}
\@writefile{toc}{\contentsline {subsubsection}{Storing values in matrices}{494}}
\@writefile{toc}{\contentsline {part}{V\hspace  {1em}Regression analysis and hierarchical models}{495}}
\newlabel{part:regressionHierarchical}{{V}{497}}
\@writefile{toc}{\contentsline {section}{\numberline {16.4}Part mission statement}{497}}
\@writefile{toc}{\contentsline {section}{\numberline {16.5}Part goals}{497}}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Hierarchical models}{499}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hierarchicalModels}{{17}{499}}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}The spectrum from pooled to heterogeneous}{499}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.1}The logic and benefits of partial pooling}{499}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.2}Shrinkage towards the mean}{499}}
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Meta analysis example: simple}{499}}
\@writefile{toc}{\contentsline {section}{\numberline {17.3}The importance of fake data simulation for complex models}{499}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.1}The importance of making 'good' fake data}{499}}
\@writefile{toc}{\contentsline {chapter}{\numberline {18}Linear regression models}{501}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {18.1}Choosing covariates: model averaging}{501}}
\newlabel{sec:Regression_modelAveraging}{{18.1}{501}}
\bibstyle{plain}
\bibdata{Bayes}
\@writefile{toc}{\contentsline {chapter}{\numberline {19}Generalised linear models}{503}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {19.1}Malarial example of complex meta-analysis}{503}}
\@writefile{toc}{\contentsline {section}{\numberline {19.2}Practical computational inference}{503}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.1}The importance of pre-simulation MLE}{503}}
\newlabel{sec:GLM_preSimMLE}{{19.2.1}{503}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.2}Fake data simulation}{503}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.3}Poor convergence}{503}}
\newlabel{sec:GLM_poorConvergence}{{19.2.3}{503}}
\@writefile{toc}{\contentsline {section}{\numberline {19.3}Posterior predictive checks}{503}}
\bibcite{stan-manual:2015}{{1}{}{{}}{{}}}
\bibcite{angrist1990lifetime}{{2}{}{{}}{{}}}
\bibcite{bergland1969guided}{{3}{}{{}}{{}}}
\bibcite{bernardo1979reference}{{4}{}{{}}{{}}}
\bibcite{blei2003latent}{{5}{}{{}}{{}}}
\bibcite{bolstad2007introduction}{{6}{}{{}}{{}}}
\bibcite{brooks2011handbook}{{7}{}{{}}{{}}}
\bibcite{casella1985introduction}{{8}{}{{}}{{}}}
\bibcite{chib1995understanding}{{9}{}{{}}{{}}}
\bibcite{epstein2008model}{{10}{}{{}}{{}}}
\bibcite{gebali2008}{{11}{}{{}}{{}}}
\bibcite{gelman2013bayesian}{{12}{}{{}}{{}}}
\bibcite{gelman2006prior}{{13}{}{{}}{{}}}
\bibcite{gelman2014understanding}{{14}{}{{}}{{}}}
\bibcite{gelman1992inference}{{15}{}{{}}{{}}}
\bibcite{gill2007bayesian}{{16}{}{{}}{{}}}
\bibcite{hanea2013asymptotic}{{17}{}{{}}{{}}}
\bibcite{ioannidis2005most}{{18}{}{{}}{{}}}
\bibcite{lewandowski2009generating}{{19}{}{{}}{{}}}
\bibcite{mandelbrot2008misbehaviour}{{20}{}{{}}{{}}}
\bibcite{mcgrayne2011theory}{{21}{}{{}}{{}}}
\bibcite{mimno2011bayesian}{{22}{}{{}}{{}}}
\bibcite{plummer2003jags}{{23}{}{{}}{{}}}
\bibcite{RLanguage}{{24}{}{{}}{{}}}
\bibcite{robert2007bayesian}{{25}{}{{}}{{}}}
\bibcite{roberts1997weak}{{26}{}{{}}{{}}}
\bibcite{rosenthal2011optimal}{{27}{}{{}}{{}}}
\bibcite{saidi2006stationarity}{{28}{}{{}}{{}}}
\bibcite{silver2012signal}{{29}{}{{}}{{}}}
\bibcite{spiegelhalter2002bayesian}{{30}{}{{}}{{}}}
\bibcite{stan-software:2014}{{31}{}{{}}{{}}}
\bibcite{stewart2014teaching}{{32}{}{{}}{{}}}
\bibcite{taleb2010black}{{33}{}{{}}{{}}}
\bibcite{tegmark2014our}{{34}{}{{}}{{}}}
\bibcite{tokuda2011visualizing}{{35}{}{{}}{{}}}
\bibcite{vehtari2015efficient}{{36}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
