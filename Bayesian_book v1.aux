\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}How to best use this book}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Why don't more people use Bayesian statistics?}{11}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Understanding the Bayesian formula}{13}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The subjective worlds of Frequentist and Bayesian statistics}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:subjectiveFrequentistBayes}{{2}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Chapter mission statement}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Chapter goals}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The purpose of statistical inference}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}The world according to Frequentists}{17}}
\newlabel{sec:Intro_FrequentistsWorld}{{2.4}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Frequentist and Bayesian approaches to probability.\relax }}{18}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Intro_FrequentistBayesProbability}{{2.1}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}The world according to Bayesians}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Frequentist and Bayesian inference}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Frequentist and Bayesian inference.\relax }}{20}}
\newlabel{fig:Intro_BayesVsFrequentist}{{2.2}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}The Frequentist and Bayesian murder trials}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Radio control towers: example}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Probability distributions: helping us explicitly state our ignorance}{22}}
\newlabel{sec:Intro_probabilityDistributions}{{2.7}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Probability distributions representing \textbf  {left:} the chance of winning a lottery, and \textbf  {right:} the value of a second-hand car.\relax }}{23}}
\newlabel{fig:Intro_lotterySecondhandCarProbability}{{2.3}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}What make a probability distribution \textit  {valid}?}{23}}
\newlabel{sec:Intro_validProbabilityDistribution}{{2.7.1}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Interpreting discrete and continuous probability distributions}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The probability that a second-hand car's value lies between \$2,500 and \$3,000.\relax }}{26}}
\newlabel{fig:Intro_continuousLotteryInterval}{{2.4}{26}}
\newlabel{eq:Intro_continuousProbabilityIntervalExample}{{2.6}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Mean and variance of distributions}{26}}
\newlabel{eq:Intro_meanDistributionDiscrete}{{2.7}{27}}
\newlabel{eq:Intro_meanDistributionContinuous}{{2.8}{27}}
\newlabel{eq:Intro_meanLottery}{{2.9}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Playing a computational lottery. We see the approach of the running mean of repeatedly playing the lottery to the long-run mean of $50\genfrac  {}{}{}1{1}{2}$, as the number of plays increases.\relax }}{28}}
\newlabel{fig:Intro_meanDiscreteLongRun}{{2.5}{28}}
\newlabel{eq:Intro_meanCoinContinuous}{{2.10}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Career second-hand car sales. We can see the approach of the sample mean towards the long-run mean of \$3,500.\relax }}{29}}
\newlabel{fig:Intro_meanContinuousLongRun}{{2.6}{29}}
\newlabel{eq:Intro_varianceDistributionExpectations}{{2.11}{29}}
\newlabel{eq:Intro_varianceDistributionDiscrete}{{2.13}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Comparing the variance of two lotteries, left: a lottery where all values between 0 and 100 are equally likely. Middle: a lottery where only values between 40 and 60 have a positive probability. Right: comparing the variability of these distributions about their common mean.\relax }}{30}}
\newlabel{fig:Intro_varianceLottery}{{2.7}{30}}
\newlabel{eq:Intro_varianceDistributionContinuous}{{2.14}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}Generalising probability distributions to two dimensions}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Horses for courses: a 2-dimensional discrete probability example}{31}}
\newlabel{sec:Intro_biasedCoinsTwoDimensionalDiscrete}{{2.7.4}{31}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces A probability distribution regarding the performance of two horses, A and B, in two separate races. $\{0,1\}$ refers to each horse losing or winning in their respective races.\relax }}{31}}
\newlabel{tab:Intro_coinBiased}{{2.1}{31}}
\newlabel{eq:Intro_discreteTwoDimensionalCoinSum}{{2.16}{32}}
\@writefile{toc}{\contentsline {subsubsection}{Foot length and intelligence: a 2-dimensional continuous probability example}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A probability distribution describing the foot size and IQ for an individual within our sample. Left) Represented as a 3-dimensional plot, and Right) Contour lines specify isolines of probability.\relax }}{33}}
\newlabel{fig:Intro_footSizeIntelligenceTwoDimensionalExample}{{2.8}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}Marginal distributions}{33}}
\newlabel{sec:Intro_marginal}{{2.7.5}{33}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The marginal distribution of horses A and B, achieved by summing the values in each column or row respectively.\relax }}{34}}
\newlabel{tab:Intro_coinsMarginal}{{2.2}{34}}
\newlabel{eq:Intro_marginalCoinsExample}{{2.17}{34}}
\newlabel{eq:Intro_marginalDiscreteProbabilityTwoDimensions}{{2.18}{34}}
\newlabel{eq:Intro_marginalContinuousProbabilityTwoDimensions}{{2.20}{35}}
\newlabel{eq:Intro_marginalContinuousProbabilityTwoDimensionsFootExample}{{2.21}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Top-left: the joint density of foot size and intelligence. Right: the marginal density of IQ. Bottom: the marginal density of foot size. \textbf  {I want to add a line at a particular value of IQ, and at a particular value of FS, to illustrate the horizontal and vertical summing.}\relax }}{36}}
\newlabel{fig:Intro_footSizeIntelligenceMarginal}{{2.9}{36}}
\newlabel{eq:Intro_vennMarginals}{{2.22}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.6}Conditional distributions}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces A Venn diagram showing one way of interpreting marginal and conditional distributions for the horse racing example.\relax }}{38}}
\newlabel{fig:Intro_Venn}{{2.10}{38}}
\newlabel{eq:Intro_conditionalProbability}{{2.23}{39}}
\newlabel{eq:Intro_conditionalDiscreteCoins}{{2.24}{39}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces The highlighted region indicates the new solution space, since we know that horse A has won.\relax }}{40}}
\newlabel{tab:Intro_coinsConditionalDiscrete}{{2.3}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces The dashed blue lines indicate the new event space in each case. The height walked following these lines is related to the magnitude of the conditional distributions shown on the right.\relax }}{40}}
\newlabel{fig:Intro_footSizeIntelligenceConditional}{{2.11}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Higher dimensional probability densities: no harder than 2-D, just looks it!}{41}}
\newlabel{eq:Intro_3DDiscreteHorsesExample}{{2.25}{41}}
\newlabel{eq:Intro_higherDimensionalMarginal}{{2.27}{42}}
\newlabel{eq:Intro_higherDimensionsConditional}{{2.28}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Knowledge of the colour of a card provides information about the suit of the card. The colour and suit of a card are \textit  {dependent}.\relax }}{43}}
\newlabel{fig:Intro_IndependenceCards}{{2.12}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Independence}{43}}
\newlabel{sec:Intro_independence}{{2.9}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Venn diagram depictions of left: disjoint, and right: independent, events $A$ and $B$.\relax }}{44}}
\newlabel{fig:Intro_VennIndependence}{{2.13}{44}}
\newlabel{eq:Intro_independentConditionalEqualMarginal}{{2.29}{44}}
\newlabel{eq:Intro_independentConditionalEqualMarginal1}{{2.30}{45}}
\newlabel{eq:Intro_independentMultiplicationForm}{{2.31}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Central Limit Theorems}{45}}
\newlabel{sec:Intro_CLT}{{2.10}{45}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces The probability distribution for horses C and D. The marginal distribution of horses C and D are achieved by summing the values in each column or row respectively.\relax }}{46}}
\newlabel{tab:Intro_horsesIndependent}{{2.4}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces The convergence to a normal distribution for the mean of a sum of uniform distributions for IQ. The pdf for the average is shown in blue, with a normal distribution of the same mean and variance indicated in grey.\relax }}{47}}
\newlabel{fig:Intro_CLTNormalSum}{{2.14}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}The Bayesian formula}{47}}
\newlabel{sec:Intro_BayesianFormula}{{2.11}{47}}
\newlabel{eq:Intro_conditionalProbabilityAB}{{2.33}{48}}
\newlabel{eq:Intro_conditionalProbabilityBA}{{2.34}{48}}
\newlabel{eq:Intro_jointConditionalProbability}{{2.35}{48}}
\newlabel{eq:Intro_BayesianFormula}{{2.36}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.1}The intuition behind the formula}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces The two ways of arriving at the joint probability $p(A,B)$; providing some intuition behind Bayes' rule.\relax }}{49}}
\newlabel{fig:Intro_BayesianIntuition}{{2.15}{49}}
\newlabel{eq:Intro_BayesianIntuition}{{2.37}{49}}
\newlabel{eq:Intro_bayesBreastCancer}{{2.38}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {2.12}The Bayesian inference process from the Bayesian formula}{50}}
\newlabel{eq:Intro_BayesianFormula1}{{2.39}{50}}
\newlabel{eq:Intro_BayesianInferenceFormula}{{2.40}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.1}Likelihoods}{51}}
\newlabel{sec:Intro_likelihoods}{{2.12.1}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Left: All values of a the bias of a coin are equally likely. Right: It is believed that the coin is most likely fair.\relax }}{52}}
\newlabel{fig:Intro_priors}{{2.16}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.2}Priors}{52}}
\newlabel{sec:Intro_priors}{{2.12.2}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.3}The denominator}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.4}Posteriors: the goal of Bayesian inference}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces The posterior distribution for, $\theta $, the bias of a coin when flipped, assuming a flat uniform prior and Bernoulli likelihood. We assume that 7/10 times the coin came up 'heads'.\relax }}{54}}
\newlabel{fig:Intro_posterior}{{2.17}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Posterior distributions for, $\theta $, the bias of a coin when flipped, assuming a flat uniform prior and Bernoulli likelihood. The grey line assumes that 7/10 times the coin came up 'heads'. The blue line is for the case where 70/100 times the coin came up 'heads'.\relax }}{54}}
\newlabel{fig:Intro_posteriorPeaked}{{2.18}{54}}
\citation{ioannidis2005most}
\@writefile{toc}{\contentsline {section}{\numberline {2.13}Implicit vs Explicit subjectivity}{55}}
\newlabel{sec:Intro_implicitExplicitSubjectivity}{{2.13}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {2.14}What are the tangible (non-academic) benefits of Bayesian statistics?}{56}}
\citation{silver2012signal}
\@writefile{toc}{\contentsline {section}{\numberline {2.15}Appendix}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.15.1}The Frequentist and Bayesian murder trial}{57}}
\newlabel{sec:Intro_appendixMurder}{{2.15.1}{57}}
\newlabel{eq:Intro_murder}{{2.42}{58}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}The posterior - the goal of Bayesian inference}{59}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:posterior}{{3}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Chapter Mission statement}{59}}
\newlabel{eq:Posterior_BayesHighlighted}{{3.1}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Chapter goals}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A probability distribution representing uncertainty over the proportion of the electorate that will vote for the Democrats in an upcoming election.\relax }}{60}}
\newlabel{fig:Posterior_electionProportion}{{3.1}{60}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Expressing uncertainty through the posterior probability distribution}{60}}
\newlabel{sec:Posterior_parameterUncertainty}{{3.3}{60}}
\citation{angrist1990lifetime}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An estimated posterior probability distribution for the parameter $\beta $ in (3.4\hbox {}). The shaded region represents the posterior probability that the parameter is positive.\relax }}{62}}
\newlabel{fig:Posterior_regressionMilitaryParticipation}{{3.2}{62}}
\newlabel{eq:Posterior_militaryParticipation}{{3.4}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Bayesian coastguard: introducing the prior and the posterior}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Bayesian statistics: updating our pre-analysis uncertainty}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Three plots. Left hand plot is a contour plot of probability radiated symmetrically in a semi-circle away from the lighthouse, with the density declining to zero at 25km. The middle plot shows a contour plot of probability, with a higher density towards the centre of the diagram (here the densities are still relatively smooth, indicating high uncertainty). The final plot shows a definite peak in intensity around a particular point about 10km away from the coast, just right of centre. The different contours will be increasing shades of a particular colour.\relax }}{64}}
\newlabel{fig:Posterior_bayesianLighthouse}{{3.3}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Do parameters actually exist and have a point value?}{65}}
\newlabel{sec:Posterior_parametersExist}{{3.3.3}{65}}
\citation{tegmark2014our}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The Frequentist and Bayesian perspectives on parameters.\relax }}{67}}
\newlabel{fig:Posterior_manyWorldsDoParametersExist}{{3.4}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Failings of the Frequentist confidence interval}{67}}
\newlabel{sec:Posterior_classicalConfidenceInterval}{{3.3.4}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The classical confidence interval. In each sample, we can calculate a 95\% confidence interval. Across repeated samples from a given population distribution, the classical confidence interval will contain the true parameter value 95\% of time.\relax }}{68}}
\newlabel{fig:Posterior_classicalConfidenceInterval}{{3.5}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Three examples of a 95\% credible interval for the regression parameter $\beta $ of the example described in section 3.3\hbox {}.\relax }}{69}}
\newlabel{fig:Posterior_infiniteCredibleIntervals}{{3.6}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Credible intervals}{69}}
\@writefile{toc}{\contentsline {subsubsection}{Treasure hunting: The central posterior and highest density intervals}{70}}
\newlabel{sec:Posterior_CPI}{{3.3.5}{70}}
\newlabel{sec:Posterior_HDI}{{3.3.5}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The posterior probability for treasure being found along the seashore (represented by a linear x-axis).\relax }}{71}}
\newlabel{fig:Posterior_CPIvsHDI}{{3.7}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Reconciling the difference between confidence and credible intervals}{71}}
\@writefile{toc}{\contentsline {subsubsection}{The interval ENIGMA}{72}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Historical communication frequencies resulting in an attack on a given location.\relax }}{73}}
\newlabel{tab:Posterior_confidenceIntervalHistoric}{{3.1}{73}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Classical confidence intervals calculated from data shown in table 3.1\hbox {}. Confidence intervals greater than or equal to 75\% are indicated in red, surrounded by parentheses.\relax }}{73}}
\newlabel{tab:Posterior_confidenceIntervalClassical}{{3.2}{73}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Bayesian credible intervals calculated from data shown in table 3.1\hbox {}. Credible intervals greater than or equal to 75\% are indicated in red, surrounded by parentheses. Note: 'credibility' is calculated by dividing the sum of interval values in each row by the row's total (see section 3.8.1\hbox {} for a full explanation.)\relax }}{74}}
\newlabel{tab:Posterior_confidenceIntervalBayesian}{{3.3}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Prediction using predictive distributions}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Example: number of Republican voters within a sample}{75}}
\newlabel{eq:Posterior_priorPredictiveVoting}{{3.6}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Top-left: the prior proportion of people voting for the Republican party in a sample of 100, resulting in the prior predictive distribution shown in the bottom-left. Top-right: the posterior proportion of people voting Republican, resulting the bottom-right posterior predictive distribution.\relax }}{77}}
\newlabel{fig:Posterior_priorPosteriorPredictiveVoting}{{3.8}{77}}
\newlabel{eq:Posterior_posteriorPredictiveVoting}{{3.7}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Example: interest rate hedging}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The likelihood of different rates of return across different central bank interest rates.\relax }}{79}}
\newlabel{fig:Posterior_likelihoodInterestRate}{{3.9}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The prior and posterior probabilities of different central ban interest rates.\relax }}{80}}
\newlabel{fig:Posterior_priorPosteriorInterestRate}{{3.10}{80}}
\newlabel{eq:Posterior_InterestpriorPredictiveDistribution}{{3.8}{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Building the prior predictive distribution, as a sum of weighted conditional probabilities. Left: the weighted conditional probabilities (compare with figure 3.9\hbox {}). Right: the cumulative weighted conditional probabilities, which converge on the prior predictive distribution.\relax }}{81}}
\newlabel{fig:Posterior_priorBuildInterestRate}{{3.11}{81}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Model comparison using the posterior}{82}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces The prior and posterior predictive distributions for investment returns.\relax }}{83}}
\newlabel{fig:Posterior_priorPosteriorPredictiveInterestRate}{{3.12}{83}}
\newlabel{eq:Posterior_modelGivenDataProbability}{{3.10}{83}}
\newlabel{eq:Posterior_modelComparisonFull}{{3.12}{84}}
\newlabel{eq:Posterior_bayesFactorDefinition}{{3.13}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Example: epidemiologist comparison}{85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Example: customer footfall}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Calculating the probability of the data for the two epidemiologists' opinions on colds.\relax }}{87}}
\newlabel{fig:Posterior_bayesFactorFluEpidemiologist}{{3.13}{87}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Left: the footfall data. Right: the area under the curves represents the probability of the data from each of the two models.\relax }}{88}}
\newlabel{fig:Posterior_modelComparisonFootfall}{{3.14}{88}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Model comparison through posterior predictive checks}{88}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Example: stock returns}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Left: the actual data. Middle: actual returns vs normal-simulated returns. Right: actual returns vs t-distribution-simulated returns.\relax }}{90}}
\newlabel{fig:Posterior_PPCstockReturns}{{3.15}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Chapter summary}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Appendix}{91}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}The interval ENIGMA - explained in full}{91}}
\newlabel{sec:Posterior_appendixConfidenceInterval}{{3.8.1}{91}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Likelihoods}{93}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Likelihoods}{{4}{93}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Chapter Mission statement}{93}}
\newlabel{eq:Likelihood_BayesHighlighted}{{4.1}{93}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Chapter goals}{93}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}What is a likelihood?}{94}}
\newlabel{eq:Likelihood_Bayes}{{4.3}{94}}
\newlabel{eq:Likelihood_simple}{{4.3}{94}}
\newlabel{eq:Likelihood_fairCoin}{{4.3}{95}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The probabilities of all possible numbers of heads for a fair coin.\relax }}{96}}
\newlabel{fig:Likelihood_fairCoin}{{4.1}{96}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Why use 'likelihood' rather than 'probability'?}{96}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces An example posterior distribution for the probability of obtaining a heads in a coin toss.\relax }}{97}}
\newlabel{fig:Likelihood_posteriorExample}{{4.2}{97}}
\newlabel{eq:Likelihood_notation}{{4.7}{97}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The likelihood function for obtaining a single head from two throws. The area under the curve is $\frac  {1}{3}$.\relax }}{98}}
\newlabel{fig:Likelihood_coinLikelihood}{{4.3}{98}}
\newlabel{eq:Likelihood_OneHead}{{4.8}{98}}
\newlabel{eq:Likelihood_TwoHead}{{4.10}{98}}
\citation{epstein2008model}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The values of likelihood for the case of tossing a coin twice, where the probability of heads is constrained to take on a discrete value: \{0.0,0.2,0.4,0.6,0.8,1.0\}.\relax }}{99}}
\newlabel{tab:Likeihood_BayesBox}{{4.1}{99}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}What are models and why do we need them?}{100}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}How to choose an appropriate likelihood?}{101}}
\newlabel{sec:chooseLikelihood}{{4.6}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}A likelihood model for an individual's disease status}{102}}
\newlabel{sec:Likelihood_individualDisease}{{4.6.1}{102}}
\newlabel{eq:Likelihood_SimpleModel1}{{4.12}{102}}
\newlabel{eq:Likelihood_SimpleModel2}{{4.13}{102}}
\newlabel{eq:Likelihood_bernoulli}{{4.14}{102}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. The sum of likelihoods is found by the area under each line, whereas the sum of probabilities is a discrete sum.\relax }}{103}}
\newlabel{fig:Likelihood_bernoulli}{{4.4}{103}}
\newlabel{eq:Likelihood_SimpleModel3}{{4.15}{103}}
\newlabel{eq:Likelihood_SimpleModel4}{{4.16}{103}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}A likelihood model for disease prevalence of a group}{104}}
\newlabel{sec:Likelihood_diseaseGroup}{{4.6.2}{104}}
\newlabel{eq:Likelihood_bernoulli1}{{4.17}{104}}
\newlabel{eq:Likelihood_bernoulli2}{{4.18}{105}}
\newlabel{eq:Likelihood_bernoulli3}{{4.19}{105}}
\newlabel{eq:Likelihood_binomialTwo}{{4.20}{105}}
\newlabel{eq:Likelihood_binomialTwoProbs}{{4.21}{105}}
\newlabel{eq:Likelihood_binomialTwoProbsSimple}{{4.22}{106}}
\newlabel{eq:Likelihood_binomialNearly}{{4.23}{106}}
\newlabel{eq:Likelihood_quadratic}{{4.24}{106}}
\newlabel{eq:Likelihood_nCr}{{4.25}{106}}
\newlabel{eq:Likelihood_binomialTwoFull}{{4.26}{106}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The likelihood function as theta varies for a sample of 2 individuals.\relax }}{107}}
\newlabel{fig:Likelihood_binomial}{{4.5}{107}}
\newlabel{eq:Likelihood_binomialThreeProbsSimpler}{{4.27}{108}}
\newlabel{eq:Likelihood_binomialThreeFull}{{4.28}{108}}
\newlabel{eq:Likelihood_binomialNFull}{{4.29}{108}}
\newlabel{eq:Likelihood_binomialTest}{{4.6.2}{108}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}The intelligence of a group of people}{109}}
\newlabel{sec:Likelihood_normal}{{4.6.3}{109}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Left panel shows a normal with $\mu =70$ and $\sigma ^2 = 81$, with the area corresponding to a result as extreme as 90 indicated. This translates into a standard normal cdf shown in the right panel, which can be used to calculate this area from the first figure. This translation to the standard normal is done by taking away $\mu $, and dividing through by $\sigma $. This is done since usually only standard normal cdf tables are available.\relax }}{110}}
\newlabel{fig:Likelihood_normal}{{4.6}{110}}
\newlabel{eq:Likelihood_normal}{{4.6.3}{110}}
\newlabel{eq:Likelihood_normalSampleOne}{{4.32}{111}}
\newlabel{eq:Likelihood_normalN}{{4.6.3}{111}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Exchangeability vs random sampling}{111}}
\newlabel{sec:Likelihood_randomSampleExchangeable}{{4.7}{111}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}The subjectivity of model choice}{113}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Maximum likelihood - a short introduction}{114}}
\newlabel{sec:Likelihood_MLE}{{4.9}{114}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}Estimating disease prevalence}{114}}
\newlabel{sec:Likelihood_diseaseMLE}{{4.9.1}{114}}
\newlabel{eq:Likelihood_binomialNew}{{4.36}{114}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The monotonicity of log-likelihood (top-left), means that the peaks of likelihood and log-likelihood coincide (bottom-left). However, this is not the case for an arbitrary function (top-right and bottom-right).\textbf  {Add legends to the bottom two graphs.}\relax }}{115}}
\newlabel{fig:Likelihood_logMonotonicity}{{4.9.1}{115}}
\newlabel{eq:Likelihood_logLikelihoodBinomial}{{4.37}{115}}
\newlabel{eq:Likelihood_logRules}{{4.38}{115}}
\newlabel{eq:Likelihood_binomialderiv}{{4.39}{115}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Log-likelihood of disease prevalence from section 4.9.1\hbox {} as a function of the proportion of individuals which have the disease in a population, $\theta $. The dotted line shows the maximum likelihood estimate $\mathaccentV {hat}05E{\theta }=1/10$.\relax }}{116}}
\newlabel{fig:Likelihood_MLE}{{4.8}{116}}
\newlabel{eq:Likelihood_binomialestimator}{{4.40}{117}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}Estimating the mean and variance in intelligence scores}{117}}
\newlabel{eq:Likelihood_normalTwo}{{4.41}{117}}
\newlabel{eq:Likelihood_diseaseLogLikelihood}{{4.42}{117}}
\newlabel{eq:Likelihood_diseaseDerivativeOne}{{4.43}{117}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Frequentist inference in Maximum Likelihood}{118}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Two likelihoods which result in the same maximum likelihood estimates of parameters, at 0.1. The gray likelihood is less strongly-peaked, meaning we can be less confident about the estimates.\relax }}{119}}
\newlabel{fig:Likelihood_likelihoodCurvature}{{4.9}{119}}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}Chapter summary}{120}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Priors}{121}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Prior}{{5}{121}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Chapter Mission statement}{121}}
\newlabel{eq:Prior_BayesHighlighted}{{5.1}{121}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Chapter goals}{121}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}What are priors, and what do they represent?}{122}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Left - a prior for a doctor's pre-testing diagnostic probability of an individual having a disease. Right - a prior which represents pre-sample uncertainty in disease prevalence.\relax }}{123}}
\newlabel{fig:Prior_introduction}{{5.1}{123}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Why do we need priors at all?}{124}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Why don't we just normalise likelihood by choosing a unity prior?}{125}}
\newlabel{sec:Prior_unityPrior}{{5.5}{125}}
\newlabel{eq:Prior_BayesNormalisedLikelihood}{{5.5}{125}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Ignoring common sense, by setting a uniform prior between a coin being fair and biased results in unrealistic posteriors.\relax }}{126}}
\newlabel{fig:Prior_priorJustificationCoin}{{5.2}{126}}
\citation{stewart2014teaching}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}The explicit subjectivity of priors}{127}}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Interpreting priors through prior predictive distributions}{128}}
\newlabel{sec:Priors_priorPredictiveDistribution}{{5.7}{128}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Combining a prior and likelihood to form a posterior}{128}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.1}An urn of balls\let \reserved@d =[\def \par }{128}}
\newlabel{sec:Prior_urn}{{5.8.1}{128}}
\newlabel{eq:Prior_bernoulli}{{5.4}{128}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here we assume that pre-experiment all possible numbers of red balls are equally likely, by adopting a uniform prior.\relax }}{129}}
\newlabel{tab:Prior_bayesBoxDiscreteUrns}{{5.1}{129}}
\newlabel{tab:addlabel}{{5.1}{129}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 5.8.1\hbox {}. The prior in the upper panel gives uniform weighting to all possible numbers of red balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{130}}
\newlabel{fig:Prior_urnStacked}{{5.3}{130}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here a higher weighting is given to more equal numbers of red and white balls in the prior.\relax }}{131}}
\newlabel{tab:Prior_bayesBoxUrnUpdated}{{5.2}{131}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.2}Disease proportions revisited}{131}}
\newlabel{sec:Prior_diseaseProp}{{5.8.2}{131}}
\newlabel{eq:Prior_binomial}{{5.5}{131}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 5.8.1\hbox {}. The prior in the upper panel gives more weighting to more equal numbers of red and white balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{132}}
\newlabel{fig:Prior_bayesUrnUpdated}{{5.4}{132}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The prior, likelihood and posterior for the disease proportion example described in section 5.8.2\hbox {}. Each point in $\theta $ along the continuous prior curve (top panel) is multiplied by the corresponding value of likelihood (middle panel), to form the numerator of Bayes' rule. The numerator is then normalised to make the posterior probability density shown in the bottom panel.\relax }}{133}}
\newlabel{fig:Prior_disease}{{5.5}{133}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Constructing priors}{134}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.1}Vague priors}{134}}
\newlabel{sec:Prior_vague}{{5.9.1}{134}}
\newlabel{eq:Prior_BayesFlatPrior}{{5.9.1}{134}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The probability density for obtaining all diseased individuals within sample sizes of 1, 2 and 10 respectively. Starting out with a flat prior for the probability that one individual has a disease has resulted in non-flat priors for the other 2 probabilities.\relax }}{135}}
\newlabel{fig:Prior_jeffreysIntro}{{5.6}{135}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.2}Informative priors}{136}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Two viable prior distributions for the average time taken before the onset of lung cancer after patients begin smoking.\relax }}{137}}
\newlabel{fig:Prior_lungcancerFlatandGammaPriors}{{5.7}{137}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The SAT scores for past students of a class. The mean and variance of this hypothetical sample are 1404, and 79,716 respectively, which are used to fit a normal distribution to the data, and is shown in red.\relax }}{138}}
\newlabel{fig:Prior_SATScoresHistogram}{{5.8}{138}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.3}The numerator of Bayes' rule determines the shape}{139}}
\newlabel{sec:Prior_numerator}{{5.9.3}{139}}
\newlabel{eq:Prior_BayesNumerator}{{5.9.3}{139}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.4}Eliciting priors}{139}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Hypothetical data for the 25th and 75th percentiles of the estimated wage premium from 10 experts. In the left hand panel we regress these percentiles on the corresponding percentiles from a standard normal distribution, yielding estimates of the mean and variance of a normal prior, which is shown on the right.\relax }}{140}}
\newlabel{fig:Prior_elicitingRegression}{{5.9}{140}}
\newlabel{eq:Prior_elicitingPriorNormal}{{5.8}{140}}
\newlabel{eq:Prior_elicitingPriorNormalRegression}{{5.9}{140}}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}A strong model is not heavily influenced by priors}{141}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The effect of increasing sample size on the posterior density for the prevalence of a disease in a population. The leftmost column has N=10, the middle N=100, and the rightmost N=1,000. All three have the same proportion of disease cases in the sample.\relax }}{142}}
\newlabel{fig:Prior_weakPriorEffect}{{5.10}{142}}
\@writefile{toc}{\contentsline {section}{\numberline {5.11}Chapter summary}{143}}
\@writefile{toc}{\contentsline {section}{\numberline {5.12}Appendix}{143}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.12.1}Bayes' rule for the urn}{143}}
\newlabel{app:Prior_bayesUrn}{{5.12.1}{143}}
\newlabel{eq:Prior_bayesDiscreteForm}{{5.10}{143}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.12.2}The probabilities of having a disease}{144}}
\newlabel{app:Prior_diseaseJeffreys}{{5.12.2}{144}}
\newlabel{eq:Prior_appChangeOfVariables}{{5.11}{144}}
\newlabel{eq:Prior_appChangeOfVariablesSolved}{{5.12}{144}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The devil's in the denominator}{145}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:denominator}{{6}{145}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Chapter mission}{145}}
\newlabel{eq:Denominator_BayesHighlighted}{{6.1}{145}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Chapter goals}{145}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}An introduction to the denominator}{146}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}The denominator as a normalising factor}{146}}
\newlabel{eq:Denominator_discreteDenominator}{{6.2}{146}}
\newlabel{eq:Denominator_continuousDenominator}{{6.3}{147}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Example: disease}{147}}
\newlabel{sec:Denominator_discreteExample}{{6.3.2}{147}}
\newlabel{eq:Denominator_discreteLikelihood}{{6.5}{147}}
\newlabel{eq:Denominator_discreteExamplePosterior}{{6.7}{148}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The prior is multiplied through by the likelihood, resulting in the numerator (the penultimate panel), which is then normalised by the sum over its values, to obtain the denominator.\relax }}{149}}
\newlabel{fig:Denominator_discreteExample}{{6.1}{149}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Example: the proportion of people who vote for conservatively}{150}}
\newlabel{sec:Denominator_continuousExample}{{6.3.3}{150}}
\newlabel{eq:Denominator_binomial}{{6.8}{150}}
\newlabel{eq:Denominator_continuousExampleApprox}{{6.9}{150}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces The prior, likelihood and posterior for the proportion of individuals voting for the conservative party in a general election, where we have found 40 people out of a sample of 100 voted conservative.\relax }}{151}}
\newlabel{fig:Denominator_continuousExample}{{6.2}{151}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}The denominator as a probability}{152}}
\newlabel{sec:Denominator_asAProbability}{{6.3.4}{152}}
\newlabel{eq:Denominator_jointDensity}{{6.10}{152}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Shows the derivation of the joint density for the disease example described in section 6.3.2\hbox {}. Each column of the likelihood - corresponding to a given disease status - is multiplied by the corresponding prior, resulting in the joint density. By summing the joint density across the different disease statuses of the patient, this results in $p(data)$. \textbf  {Add pluses and equals to the calculation of $p(data)$. Also add in the posterior calculation.} See figure 6.3\hbox {} for a graphical depiction of this joint density.\relax }}{153}}
\newlabel{tab:Denominator_discreteJoint}{{6.1}{153}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Using the denominator to choose between competing models}{153}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The joint density of the data and the parameter for the disease example described in section 6.3.2\hbox {}. When we uncover that the test result is positive, we are confined to look at the bars in dark grey; finding that the probability that an individual is diseased is significantly higher than the alternative (see the bottom panel of figure 6.1\hbox {}). \textbf  {Perhaps redo this figure with a contour plot opposed to a 3D graph, and show how the posterior is obtained in another panel. Or just get rid of it, the table does pretty much cover it.}\relax }}{154}}
\newlabel{fig:Denominator_discreteJointDensity}{{6.3}{154}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Top-left: a contour plot of the joint density of the voting example described in section 6.4\hbox {}. Top-right: the marginal density of $p(data)$ obtained by summing across all values of $\theta $. Bottom-left: the posterior obtained by summing the joint density across the line shown at 40. Note that in reality the data variable is discrete, but I have drawn it here as continuous to make the plot simpler to interpret. \textbf  {The line at 40 may be dashed in the final version. The axes all need to be aligned.}\relax }}{155}}
\newlabel{fig:Denominator_continuousJointDensity}{{6.4}{155}}
\newlabel{eq:Denominator_expectedLikelihood}{{6.12}{156}}
\newlabel{eq:Denominator_modelProbability}{{6.13}{156}}
\newlabel{eq:Denominator_modelComparison}{{6.14}{156}}
\newlabel{eq:Denominator_bayesFactor}{{6.15}{156}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.6}The denominator for improper priors}{157}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}The difficulty with the denominator}{157}}
\newlabel{sec:Denominator_difficulty}{{6.4}{157}}
\newlabel{eq:Denominator_doubleSum}{{6.16}{157}}
\newlabel{eq:Denominator_doubleIntegral}{{6.17}{157}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Multi-parameter discrete model example: the comorbidity between depression and anxiety}{158}}
\newlabel{sec:Denominator_comorbidityTwoParameterDiscrete}{{6.4.1}{158}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces \relax }}{159}}
\newlabel{tab:Denominator_comorbidityTwoParameterDiscrete}{{6.2}{159}}
\newlabel{eq:Denominator_TwoParameterDiscreteBayesSimple}{{6.19}{159}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Continuous multi-parameter example: mean and variance of IQ}{160}}
\newlabel{sec:Denominator_continuousTwoParameterIQ}{{6.4.2}{160}}
\newlabel{eq:Denominator_continuousTwoParameterLikelihood}{{6.20}{160}}
\newlabel{sec:Denominator_continuousMultiparameterIndependence}{{6.21}{160}}
\newlabel{eq:Denominator_continuousTwoParameterPrior}{{6.22}{161}}
\newlabel{eq:Denominator_continuousTwoParameterJointPosterior}{{6.23}{161}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The prior, likelihood, and posterior distributions for the mean and variance of IQ example described in section 6.4.2\hbox {}.\relax }}{162}}
\newlabel{fig:Denominator_continuousTwoParameter3D}{{6.5}{162}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}How to dispense with the difficulty: Bayesian computation}{163}}
\newlabel{sec:Denominator_dispensingWithNumerator}{{6.5}{163}}
\newlabel{eq:Denominator_proportional}{{6.27}{164}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Chapter summary}{165}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Appendix}{165}}
\newlabel{sec:Denominator_appendix}{{6.7}{165}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Analytic Bayesian methods}{167}}
\newlabel{part:analyticalBayes}{{II}{169}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}An introduction to distributions for the mathematically-un-inclined}{169}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:distributions}{{7}{169}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Chapter mission statement}{169}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Chapter goals}{169}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Sampling distributions for likelihoods}{169}}
\@writefile{toc}{\contentsline {subsubsection}{Bernoulli}{169}}
\newlabel{sec:Distributions_bernoulli}{{7.3}{169}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Bernoulli likelihoods for the event that the horse wins (red), and loses (blue). The maximum likelihood estimates are shown as dotted lines for each of the cases.\relax }}{171}}
\newlabel{fig:Distributions_bernoulliHorseRace}{{7.1}{171}}
\newlabel{eq:Distributions_binomialDefinition}{{7.2}{171}}
\@writefile{toc}{\contentsline {subsubsection}{Binomial}{172}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces \textbf  {Left}: Binomial likelihoods for the event that 5 (blue), 2 (red), and 9 (grey) volunteers recovered in the week period. The maximum likelihood estimates are shown as dotted lines for each of the cases. \textbf  {Right:} the probability distribution of successful trials if $\theta =0.3$ (blue) and $\theta =0.7$ (orange).\relax }}{174}}
\newlabel{fig:Distributions_binomialClinicalTrial}{{7.2}{174}}
\newlabel{eq:Distributions_binomialDef}{{7.5}{174}}
\@writefile{toc}{\contentsline {subsubsection}{Normal}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Poisson}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Negative binomial}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Logistic}{175}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Prior distributions}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Distributions for probabilities, proportions and percentages}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Uniform}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Beta}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Dirichlet}{175}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Table of distributions, their uses, and reasonable priors}{175}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Distributions for means and medians}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Normal}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Student t}{175}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Distributions for variances, and shape parameters}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Gamma}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Half-Cauchy}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Inverse Gamma}{175}}
\@writefile{toc}{\contentsline {subsubsection}{Inverse chi}{175}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Multinomial - or other regression}{175}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}LBG prior - see Michael Betancourt video and Stan doc}{175}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.5}Wishart}{176}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.6}Distributions for categories}{176}}
\@writefile{toc}{\contentsline {subsubsection}{Categorical}{176}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Chapter summary}{176}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conjugate priors and their place in Bayesian analysis}{177}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conjugate}{{8}{177}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Objective Bayesian analysis}{179}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ObjectiveBayes}{{9}{179}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}A practical guide to doing real life Bayesian analysis: Computational Bayes}{181}}
\newlabel{part:computationalBayes}{{III}{183}}
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Regression analysis and hierarchical models}{183}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Hierarchical models}{185}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hierarchicalModels}{{10}{185}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Hypothesis testing I: Classical Frequentist vs Bayesian approaches}{187}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{Bayes}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Evaluation of model fit}{189}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ModelFit}{{12}{189}}
