\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}How to best use this book}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Why don't more people use Bayesian statistics?}{9}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Understanding the Bayesian formula}{11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The subjective worlds of frequentist and Bayesian statistics}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:subjectiveFrequentistBayes}{{2}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Chapter mission statement}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Chapter goals}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The purpose of statistical inference}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}The world according to frequentists}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Frequentist and Bayesian approaches to probability.\relax }}{16}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Intro_FrequentistBayesProbability}{{2.1}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}The world according to Bayesians}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Frequentist and Bayesian inference}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Frequentist and Bayesian inference.\relax }}{18}}
\newlabel{fig:Intro_BayesVsFrequentist}{{2.2}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}The frequentist and Bayesian murder trials}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Locating a submarine: example}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Probability distributions: helping us explicitly state our ignorance}{19}}
\newlabel{sec:Intro_probabilityDistributions}{{2.7}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Probability distributions representing \textbf  {left:} the chance of winning a lottery, and \textbf  {right:} the value of a second-hand car.\relax }}{20}}
\newlabel{fig:Intro_lotterySecondhandCarProbability}{{2.3}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}What make a probability distribution \textit  {valid}?}{20}}
\newlabel{sec:Intro_validProbabilityDistribution}{{2.7.1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Interpreting discrete and continuous probability distributions}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The probability that a second-hand car's value lies between \$2,500 and \$3,000.\relax }}{23}}
\newlabel{fig:Intro_continuousLotteryInterval}{{2.4}{23}}
\newlabel{eq:Intro_continuousProbabilityIntervalExample}{{2.6}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Mean and variance of distributions}{23}}
\newlabel{eq:Intro_meanDistributionDiscrete}{{2.7}{24}}
\newlabel{eq:Intro_meanDistributionContinuous}{{2.8}{24}}
\newlabel{eq:Intro_meanLottery}{{2.9}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Playing a computational lottery. We see the approach of the running mean of repeatedly playing the lottery to the long-run mean of $50\genfrac  {}{}{}1{1}{2}$, as the number of plays increases.\relax }}{25}}
\newlabel{fig:Intro_meanDiscreteLongRun}{{2.5}{25}}
\newlabel{eq:Intro_meanCoinContinuous}{{2.10}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Career second-hand car sales. We can see the approach of the sample mean towards the long-run mean of \$3,500.\relax }}{26}}
\newlabel{fig:Intro_meanContinuousLongRun}{{2.6}{26}}
\newlabel{eq:Intro_varianceDistributionExpectations}{{2.11}{26}}
\newlabel{eq:Intro_varianceDistributionDiscrete}{{2.13}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Comparing the variance of two lotteries, left: a lottery where all values between 0 and 100 are equally likely. Middle: a lottery where only values between 40 and 60 have a positive probability. Right: comparing the variability of these distributions about their common mean.\relax }}{27}}
\newlabel{fig:Intro_varianceLottery}{{2.7}{27}}
\newlabel{eq:Intro_varianceDistributionContinuous}{{2.14}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}Generalising probability distributions to two dimensions}{28}}
\@writefile{toc}{\contentsline {subsubsection}{Horses for courses: a 2-dimensional discrete probability example}{28}}
\newlabel{sec:Intro_biasedCoinsTwoDimensionalDiscrete}{{2.7.4}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces A probability distribution regarding the performance of two horses, A and B, in two separate races. $\{0,1\}$ refers to each horse losing or winning in their respective races.\relax }}{28}}
\newlabel{tab:Intro_coinBiased}{{2.1}{28}}
\newlabel{eq:Intro_discreteTwoDimensionalCoinSum}{{2.16}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Foot length and intelligence: a 2-dimensional continuous probability example}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A probability distribution describing the foot size and IQ for an individual within our sample. Left) Represented as a 3-dimensional plot, and Right) Contour lines specify isolines of probability.\relax }}{30}}
\newlabel{fig:Intro_footSizeIntelligenceTwoDimensionalExample}{{2.8}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}Marginal distributions}{30}}
\newlabel{sec:Intro_marginal}{{2.7.5}{30}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The marginal distribution of horses A and B, achieved by summing the values in each column or row respectively.\relax }}{31}}
\newlabel{tab:Intro_coinsMarginal}{{2.2}{31}}
\newlabel{eq:Intro_marginalCoinsExample}{{2.17}{31}}
\newlabel{eq:Intro_marginalDiscreteProbabilityTwoDimensions}{{2.18}{31}}
\newlabel{eq:Intro_marginalContinuousProbabilityTwoDimensions}{{2.20}{32}}
\newlabel{eq:Intro_marginalContinuousProbabilityTwoDimensionsFootExample}{{2.21}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Top-left: the joint density of foot size and intelligence. Right: the marginal density of IQ. Bottom: the marginal density of foot size. \textbf  {I want to add a line at a particular value of IQ, and at a particular value of FS, to illustrate the horizontal and vertical summing.}\relax }}{33}}
\newlabel{fig:Intro_footSizeIntelligenceMarginal}{{2.9}{33}}
\newlabel{eq:Intro_vennMarginals}{{2.22}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.6}Conditional distributions}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces A Venn diagram showing one way of interpreting marginal and conditional distributions for the horse racing example.\relax }}{35}}
\newlabel{fig:Intro_Venn}{{2.10}{35}}
\newlabel{eq:Intro_conditionalProbability}{{2.23}{36}}
\newlabel{eq:Intro_conditionalDiscreteCoins}{{2.24}{36}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces The highlighted region indicates the new solution space, since we know that horse A has won.\relax }}{37}}
\newlabel{tab:Intro_coinsConditionalDiscrete}{{2.3}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces The dashed blue lines indicate the new event space in each case. The height walked following these lines is related to the magnitude of the conditional distributions shown on the right.\relax }}{37}}
\newlabel{fig:Intro_footSizeIntelligenceConditional}{{2.11}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Higher dimensional probability densities: no harder than 2-D, just looks it!}{38}}
\newlabel{eq:Intro_3DDiscreteHorsesExample}{{2.25}{38}}
\newlabel{eq:Intro_higherDimensionalMarginal}{{2.27}{39}}
\newlabel{eq:Intro_higherDimensionsConditional}{{2.28}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Knowledge of the colour of a card provides information about the suit of the card. The colour and suit of a card are \textit  {dependent}.\relax }}{40}}
\newlabel{fig:Intro_IndependenceCards}{{2.12}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Independence}{40}}
\newlabel{sec:Intro_independence}{{2.9}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Venn diagram depictions of left: disjoint, and right: independent, events $A$ and $B$.\relax }}{41}}
\newlabel{fig:Intro_VennIndependence}{{2.13}{41}}
\newlabel{eq:Intro_independentConditionalEqualMarginal}{{2.29}{41}}
\newlabel{eq:Intro_independentConditionalEqualMarginal1}{{2.30}{42}}
\newlabel{eq:Intro_independentMultiplicationForm}{{2.31}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Central Limit Theorems}{42}}
\newlabel{sec:Intro_CLT}{{2.10}{42}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces The probability distribution for horses C and D. The marginal distribution of horses C and D are achieved by summing the values in each column or row respectively.\relax }}{43}}
\newlabel{tab:Intro_horsesIndependent}{{2.4}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces The convergence to a normal distribution for the mean of a sum of uniform distributions for IQ. The pdf for the average is shown in blue, with a normal distribution of the same mean and variance indicated in grey.\relax }}{44}}
\newlabel{fig:Intro_CLTNormalSum}{{2.14}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}The Bayesian formula}{44}}
\newlabel{sec:Intro_BayesianFormula}{{2.11}{44}}
\newlabel{eq:Intro_conditionalProbabilityAB}{{2.33}{45}}
\newlabel{eq:Intro_conditionalProbabilityBA}{{2.34}{45}}
\newlabel{eq:Intro_jointConditionalProbability}{{2.35}{45}}
\newlabel{eq:Intro_BayesianFormula}{{2.36}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.1}The intuition behind the formula}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces The two ways of arriving at the joint probability $p(A,B)$; providing some intuition behind Bayes' rule.\relax }}{46}}
\newlabel{fig:Intro_BayesianIntuition}{{2.15}{46}}
\newlabel{eq:Intro_BayesianIntuition}{{2.37}{46}}
\newlabel{eq:Intro_bayesBreastCancer}{{2.38}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {2.12}The Bayesian inference process from the Bayesian formula}{47}}
\newlabel{eq:Intro_BayesianFormula1}{{2.39}{47}}
\newlabel{eq:Intro_BayesianInferenceFormula}{{2.40}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.1}Likelihoods}{48}}
\newlabel{sec:Intro_likelihoods}{{2.12.1}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Left: All values of a the bias of a coin are equally likely. Right: It is believed that the coin is most likely fair.\relax }}{49}}
\newlabel{fig:Intro_priors}{{2.16}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.2}Priors}{49}}
\newlabel{sec:Intro_priors}{{2.12.2}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.3}The denominator}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.4}Posteriors: the goal of Bayesian inference}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces The posterior distribution for, $\theta $, the bias of a coin when flipped, assuming a flat uniform prior and Bernoulli likelihood. We assume that 7/10 times the coin came up 'heads'.\relax }}{51}}
\newlabel{fig:Intro_posterior}{{2.17}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Posterior distributions for, $\theta $, the bias of a coin when flipped, assuming a flat uniform prior and Bernoulli likelihood. The grey line assumes that 7/10 times the coin came up 'heads'. The blue line is for the case where 70/100 times the coin came up 'heads'.\relax }}{51}}
\newlabel{fig:Intro_posteriorPeaked}{{2.18}{51}}
\citation{ioannidis2005most}
\@writefile{toc}{\contentsline {section}{\numberline {2.13}Implicit vs Explicit subjectivity}{52}}
\newlabel{sec:Intro_implicitExplicitSubjectivity}{{2.13}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {2.14}What are the tangible (non-academic) benefits of Bayesian statistics?}{53}}
\citation{silver2012signal}
\@writefile{toc}{\contentsline {section}{\numberline {2.15}Appendix}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.15.1}The frequentist and Bayesian murder trial}{54}}
\newlabel{sec:Intro_appendixMurder}{{2.15.1}{54}}
\newlabel{eq:Intro_murder}{{2.42}{55}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}The posterior - the goal of Bayesian inference}{57}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:posterior}{{3}{57}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Chapter Mission statement}{57}}
\newlabel{eq:Posterior_BayesHighlighted}{{3.1}{57}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Chapter goals}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A probability distribution representing uncertainty over the proportion of the electorate that will vote for the Democrats in an upcoming election.\relax }}{58}}
\newlabel{fig:Posterior_electionProportion}{{3.1}{58}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Expressing uncertainty in a parameter through the posterior probability distribution}{58}}
\newlabel{sec:Posterior_parameterUncertainty}{{3.3}{58}}
\citation{angrist1990lifetime}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An estimated posterior probability distribution for the parameter $\beta $ in (3.4\hbox {}). The shaded region represents the posterior probability that the parameter lies is positive.\relax }}{60}}
\newlabel{fig:Posterior_regressionMilitaryParticipation}{{3.2}{60}}
\newlabel{eq:Posterior_militaryParticipation}{{3.4}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Bayesian statistics: updating our pre-analysis uncertainty}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Do parameters actually exist and have a point value? Bayesian vs classical standpoints}{61}}
\newlabel{sec:Posterior_parametersExist}{{3.3.2}{61}}
\citation{tegmark2014our}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Failings of the Frequentist confidence interval}{63}}
\newlabel{sec:Posterior_classicalConfidenceInterval}{{3.3.3}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Credible intervals}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The classical confidence interval. In each sample, we can calculate a 95\% confidence interval. Across repeated samples from a given population distribution, the classical confidence interval will contain the true parameter value 95\% of time.\relax }}{64}}
\newlabel{fig:Posterior_classicalConfidenceInterval}{{3.3}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Three examples of a 95\% credible interval for the regression parameter $\beta $ of the example described in section 3.3\hbox {}.\relax }}{65}}
\newlabel{fig:Posterior_infiniteCredibleIntervals}{{3.4}{65}}
\@writefile{toc}{\contentsline {subsubsection}{Treasure hunting: The central posterior and highest density intervals}{66}}
\newlabel{sec:Posterior_CPI}{{3.3.4}{66}}
\newlabel{sec:Posterior_HDI}{{3.3.4}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The posterior probability for treasure being found along the seashore (represented by a linear x-axis).\relax }}{67}}
\newlabel{fig:Posterior_CPIvsHDI}{{3.5}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Reconciling the difference between confidence and credible intervals}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Prediction using a posterior distribution}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Before experiment, using prior}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}After experiment, using posterior}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Model comparison using the posterior}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Model testing through the posterior}{68}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Likelihoods}{69}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Likelihoods}{{4}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Chapter Mission statement}{69}}
\newlabel{eq:Likelihood_BayesHighlighted}{{4.1}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Chapter goals}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}What is a likelihood?}{70}}
\newlabel{eq:Likelihood_Bayes}{{4.3}{70}}
\newlabel{eq:Likelihood_simple}{{4.3}{70}}
\newlabel{eq:Likelihood_fairCoin}{{4.3}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The probabilities of all possible numbers of heads for a fair coin.\relax }}{72}}
\newlabel{fig:Likelihood_fairCoin}{{4.1}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Why use 'likelihood' rather than 'probability'?}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces An example posterior distribution for the probability of obtaining a heads in a coin toss.\relax }}{73}}
\newlabel{fig:Likelihood_posteriorExample}{{4.2}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The likelihood function for obtaining a single head from two throws. The area under the curve is $\frac  {1}{3}$.\relax }}{73}}
\newlabel{fig:Likelihood_coinLikelihood}{{4.3}{73}}
\newlabel{eq:Likelihood_notation}{{4.5}{74}}
\newlabel{eq:Likelihood_OneHead}{{4.6}{74}}
\newlabel{eq:Likelihood_TwoHead}{{4.8}{74}}
\citation{epstein2008model}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The values of likelihood for the case of tossing a coin twice, where the probability of heads is constrained to take on a discrete value: \{0.0,0.2,0.4,0.6,0.8,1.0\}.\relax }}{75}}
\newlabel{tab:Likeihood_BayesBox}{{4.1}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}What are models and why do we need them?}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}How to choose an appropriate likelihood?}{76}}
\newlabel{sec:chooseLikelihood}{{4.6}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}A likelihood model for an individual's disease status}{77}}
\newlabel{sec:Likelihood_individualDisease}{{4.6.1}{77}}
\newlabel{eq:Likelihood_SimpleModel1}{{4.10}{78}}
\newlabel{eq:Likelihood_SimpleModel2}{{4.11}{78}}
\newlabel{eq:Likelihood_bernoulli}{{4.12}{78}}
\newlabel{eq:Likelihood_SimpleModel3}{{4.13}{78}}
\newlabel{eq:Likelihood_SimpleModel4}{{4.14}{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}A likelihood model for disease prevalence of a group}{78}}
\newlabel{sec:Likelihood_diseaseGroup}{{4.6.2}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. The sum of likelihoods is found by the area under each line, whereas the sum of probabilities is a discrete sum.\relax }}{79}}
\newlabel{fig:Likelihood_bernoulli}{{4.4}{79}}
\newlabel{eq:Likelihood_bernoulli1}{{4.15}{80}}
\newlabel{eq:Likelihood_bernoulli2}{{4.16}{81}}
\newlabel{eq:Likelihood_bernoulli3}{{4.17}{81}}
\newlabel{eq:Likelihood_binomialTwo}{{4.18}{81}}
\newlabel{eq:Likelihood_binomialTwoProbs}{{4.19}{81}}
\newlabel{eq:Likelihood_binomialTwoProbsSimple}{{4.20}{81}}
\newlabel{eq:Likelihood_binomialNearly}{{4.21}{82}}
\newlabel{eq:Likelihood_quadratic}{{4.22}{82}}
\newlabel{eq:Likelihood_nCr}{{4.23}{82}}
\newlabel{eq:Likelihood_binomialTwoFull}{{4.24}{82}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The likelihood function as theta varies for a sample of 2 individuals.\relax }}{83}}
\newlabel{fig:Likelihood_binomial}{{4.5}{83}}
\newlabel{eq:Likelihood_binomialThreeProbsSimpler}{{4.25}{84}}
\newlabel{eq:Likelihood_binomialThreeFull}{{4.26}{84}}
\newlabel{eq:Likelihood_binomialNFull}{{4.27}{84}}
\newlabel{eq:Likelihood_binomialTest}{{4.6.2}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}The intelligence of a group of people}{85}}
\newlabel{sec:Likelihood_normal}{{4.6.3}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Left panel shows a normal with $\mu =70$ and $\sigma ^2 = 81$, with the area corresponding to a result as extreme as 90 indicated. This translates into a standard normal cdf shown in the right panel, which can be used to calculate this area from the first figure. This translation to the standard normal is done by taking away $\mu $, and dividing through by $\sigma $. This is done since usually only standard normal cdf tables are available.\relax }}{86}}
\newlabel{fig:Likelihood_normal}{{4.6}{86}}
\newlabel{eq:Likelihood_normal}{{4.6.3}{86}}
\newlabel{eq:Likelihood_normalSampleOne}{{4.30}{86}}
\newlabel{eq:Likelihood_normalN}{{4.6.3}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}The subjectivity of model choice}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Maximum likelihood - a short introduction}{88}}
\newlabel{sec:Likelihood_MLE}{{4.8}{88}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Estimating disease prevalence}{88}}
\newlabel{sec:Likelihood_diseaseMLE}{{4.8.1}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A figure with four panes. The top-left is log-likelihood as a function of likelihood. The bottom-left is likelihood as a function of theta, and log-likelihood plotted on the same axis. Top-right is a weird function of likelihood as a function of likelihood. Below it a graph of likelihood as a function of theta, with a different maximum reached for the weird function.\relax }}{89}}
\newlabel{fig:Likelihood_logMonotonicity}{{4.7}{89}}
\newlabel{eq:Likelihood_binomialNew}{{4.32}{89}}
\newlabel{eq:Likelihood_logLikelihoodBinomial}{{4.33}{89}}
\newlabel{eq:Likelihood_logRules}{{4.34}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Log-likelihood of disease prevalence from section 4.8.1\hbox {} as a function of theta, maximised at 1/10.\relax }}{90}}
\newlabel{fig:Likelihood_MLE}{{4.8}{90}}
\newlabel{eq:Likelihood_binomialderiv}{{4.35}{90}}
\newlabel{eq:Likelihood_binomialestimator}{{4.36}{91}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Estimating the mean and variance in intelligence scores}{91}}
\newlabel{eq:Likelihood_normalTwo}{{4.37}{91}}
\newlabel{eq:Likelihood_diseaseLogLikelihood}{{4.38}{91}}
\newlabel{eq:Likelihood_diseaseDerivativeOne}{{4.39}{91}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Frequentist inference in Maximum Likelihood}{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Two likelihoods which result in the same maximum likelihood estimates of parameters, at 0.1. The gray likelihood is less strongly-peaked, meaning we can be less confident about the estimates.\relax }}{93}}
\newlabel{fig:Likelihood_likelihoodCurvature}{{4.9}{93}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Chapter summary}{94}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Priors}{95}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Prior}{{5}{95}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Chapter Mission statement}{95}}
\newlabel{eq:Prior_BayesHighlighted}{{5.1}{95}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Chapter goals}{95}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}What are priors, and what do they represent?}{96}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Left - a prior for a doctor's pre-testing diagnostic probability of an individual having a disease. Right - a prior which represents pre-sample uncertainty in disease prevalence.\relax }}{97}}
\newlabel{fig:Prior_introduction}{{5.1}{97}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Why don't we just normalise likelihood by choosing a unity prior?}{98}}
\newlabel{sec:Prior_unityPrior}{{5.4}{98}}
\newlabel{eq:Prior_BayesNormalisedLikelihood}{{5.4}{98}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces A figure showing in the left hand panel the Venn diagrams when we assume that a) the coin is fair with a quarter of the area shaded, and the other half not. b) Biased, and the majority of the figure is shaded - corresponding to a high overlap between data and parameter. The right hand panel then shows the joint probability of the data and the parameters $\theta $, and we see that even though the ratio of the area is better for b), it is much less likely \textit  {a priori} that the coin is biased. The bottom panel shows the ML implied posterior distribution, with the bar for unfairness much higher than for fairness. The right shows a much more logical conclusion which takes into account their prior probabilities.\relax }}{99}}
\newlabel{fig:Prior_justification}{{5.2}{99}}
\citation{stewart2014teaching}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}The explicit subjectivity of priors}{100}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Combining a prior and likelihood to form a posterior}{100}}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}An urn of balls\let \reserved@d =[\def \par }{101}}
\newlabel{sec:Prior_urn}{{5.6.1}{101}}
\newlabel{eq:Prior_bernoulli}{{5.3}{101}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here we assume that pre-experiment all possible numbers of red balls are equally likely, by adopting a uniform prior.\relax }}{102}}
\newlabel{tab:Prior_bayesBoxDiscreteUrns}{{5.1}{102}}
\newlabel{tab:addlabel}{{5.1}{102}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Disease proportions revisited}{102}}
\newlabel{sec:Prior_diseaseProp}{{5.6.2}{102}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 5.6.1\hbox {}. The prior in the upper panel gives uniform weighting to all possible numbers of red balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{103}}
\newlabel{fig:Prior_urnStacked}{{5.3}{103}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here a higher weighting is given to more equal numbers of red and white balls in the prior.\relax }}{103}}
\newlabel{tab:Prior_bayesBoxUrnUpdated}{{5.2}{103}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 5.6.1\hbox {}. The prior in the upper panel gives more weighting to more equal numbers of red and white balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{104}}
\newlabel{fig:Prior_bayesUrnUpdated}{{5.4}{104}}
\newlabel{eq:Prior_binomial}{{5.4}{105}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Constructing priors}{105}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The prior, likelihood and posterior for the disease proportion example described in section 5.6.2\hbox {}. Each point in $\theta $ along the continuous prior curve (top panel) is multiplied by the corresponding value of likelihood (middle panel), to form the numerator of Bayes' rule. The numerator is then normalised to make the posterior probability density shown in the bottom panel.\relax }}{106}}
\newlabel{fig:Prior_disease}{{5.5}{106}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Vague priors}{107}}
\newlabel{sec:Prior_vague}{{5.7.1}{107}}
\newlabel{eq:Prior_BayesFlatPrior}{{5.7.1}{107}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The probability density for obtaining all diseased individuals within sample sizes of 1, 2 and 10 respectively. Starting out with a flat prior for the probability that one individual has a disease has resulted in non-flat priors for the other 2 probabilities.\relax }}{108}}
\newlabel{fig:Prior_jeffreysIntro}{{5.6}{108}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Informative priors}{109}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Two viable prior distributions for the average time taken before the onset of lung cancer after patients begin smoking.\relax }}{110}}
\newlabel{fig:Prior_lungcancerFlatandGammaPriors}{{5.7}{110}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The SAT scores for past students of a class. The mean and variance of this hypothetical sample are 1404, and 79,716 respectively, which are used to fit a normal distribution to the data, and is shown in red.\relax }}{111}}
\newlabel{fig:Prior_SATScoresHistogram}{{5.8}{111}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}The numerator of Bayes' rule determines the shape}{112}}
\newlabel{sec:Prior_numerator}{{5.7.3}{112}}
\newlabel{eq:Prior_BayesNumerator}{{5.7.3}{112}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.4}Eliciting priors}{112}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Hypothetical data for the 25th and 75th percentiles of the estimated wage premium from 10 experts. In the left hand panel we regress these percentiles on the corresponding percentiles from a standard normal distribution, yielding estimates of the mean and variance of a normal prior, which is shown on the right.\relax }}{113}}
\newlabel{fig:Prior_elicitingRegression}{{5.9}{113}}
\newlabel{eq:Prior_elicitingPriorNormal}{{5.7}{113}}
\newlabel{eq:Prior_elicitingPriorNormalRegression}{{5.8}{113}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}A strong model is not heavily influenced by priors}{114}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The effect of increasing sample size on the posterior density for the prevalence of a disease in a population. The leftmost column has N=10, the middle N=100, and the rightmost N=1,000. All three have the same proportion of disease cases in the sample.\relax }}{115}}
\newlabel{fig:Prior_weakPriorEffect}{{5.10}{115}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Chapter summary}{116}}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Appendix}{116}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10.1}Bayes' rule for the urn}{116}}
\newlabel{app:Prior_bayesUrn}{{5.10.1}{116}}
\newlabel{eq:Prior_bayesDiscreteForm}{{5.9}{116}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10.2}The probabilities of having a disease}{117}}
\newlabel{app:Prior_diseaseJeffreys}{{5.10.2}{117}}
\newlabel{eq:Prior_appChangeOfVariables}{{5.10}{117}}
\newlabel{eq:Prior_appChangeOfVariablesSolved}{{5.11}{117}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The devil's in the denominator}{119}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:denominator}{{6}{119}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Chapter mission}{119}}
\newlabel{eq:Denominator_BayesHighlighted}{{6.1}{119}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Chapter goals}{119}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}An introduction to the denominator}{120}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}The denominator as a normalising factor}{120}}
\newlabel{eq:Denominator_discreteDenominator}{{6.2}{120}}
\newlabel{eq:Denominator_continuousDenominator}{{6.3}{121}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Example: disease}{121}}
\newlabel{sec:Denominator_discreteExample}{{6.3.2}{121}}
\newlabel{eq:Denominator_discreteLikelihood}{{6.5}{121}}
\newlabel{eq:Denominator_discreteExamplePosterior}{{6.7}{122}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The prior is multiplied through by the likelihood, resulting in the numerator (the penultimate panel), which is then normalised by the sum over its values, to obtain the denominator.\relax }}{123}}
\newlabel{fig:Denominator_discreteExample}{{6.1}{123}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Example: the proportion of people who vote for conservatively}{124}}
\newlabel{sec:Denominator_continuousExample}{{6.3.3}{124}}
\newlabel{eq:Denominator_binomial}{{6.8}{124}}
\newlabel{eq:Denominator_continuousExampleApprox}{{6.9}{124}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces The prior, likelihood and posterior for the proportion of individuals voting for the conservative party in a general election, where we have found 40 people out of a sample of 100 voted conservative.\relax }}{125}}
\newlabel{fig:Denominator_continuousExample}{{6.2}{125}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}The denominator as a probability}{126}}
\newlabel{sec:Denominator_asAProbability}{{6.3.4}{126}}
\newlabel{eq:Denominator_jointDensity}{{6.10}{126}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Shows the derivation of the joint density for the disease example described in section 6.3.2\hbox {}. Each column of the likelihood - corresponding to a given disease status - is multiplied by the corresponding prior, resulting in the joint density. By summing the joint density across the different disease statuses of the patient, this results in $p(data)$. \textbf  {Add pluses and equals to the calculation of $p(data)$. Also add in the posterior calculation.} See figure 6.3\hbox {} for a graphical depiction of this joint density.\relax }}{127}}
\newlabel{tab:Denominator_discreteJoint}{{6.1}{127}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Using the denominator to choose between competing models}{127}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The joint density of the data and the parameter for the disease example described in section 6.3.2\hbox {}. When we uncover that the test result is positive, we are confined to look at the bars in dark grey; finding that the probability that an individual is diseased is significantly higher than the alternative (see the bottom panel of figure 6.1\hbox {}). \textbf  {Perhaps redo this figure with a contour plot opposed to a 3D graph, and show how the posterior is obtained in another panel. Or just get rid of it, the table does pretty much cover it.}\relax }}{128}}
\newlabel{fig:Denominator_discreteJointDensity}{{6.3}{128}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Top-left: a contour plot of the joint density of the voting example described in section 6.4\hbox {}. Top-right: the marginal density of $p(data)$ obtained by summing across all values of $\theta $. Bottom-left: the posterior obtained by summing the joint density across the line shown at 40. Note that in reality the data variable is discrete, but I have drawn it here as continuous to make the plot simpler to interpret. \textbf  {The line at 40 may be dashed in the final version. The axes all need to be aligned.}\relax }}{129}}
\newlabel{fig:Denominator_continuousJointDensity}{{6.4}{129}}
\newlabel{eq:Denominator_expectedLikelihood}{{6.12}{130}}
\newlabel{eq:Denominator_modelProbability}{{6.13}{130}}
\newlabel{eq:Denominator_modelComparison}{{6.14}{130}}
\newlabel{eq:Denominator_bayesFactor}{{6.15}{130}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.6}The denominator for improper priors}{131}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}The difficulty with the denominator}{131}}
\newlabel{sec:Denominator_difficulty}{{6.4}{131}}
\newlabel{eq:Denominator_doubleSum}{{6.16}{131}}
\newlabel{eq:Denominator_doubleIntegral}{{6.17}{131}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Multi-parameter discrete model example: the comorbidity between depression and anxiety}{132}}
\newlabel{sec:Denominator_comorbidityTwoParameterDiscrete}{{6.4.1}{132}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces \relax }}{133}}
\newlabel{tab:Denominator_comorbidityTwoParameterDiscrete}{{6.2}{133}}
\newlabel{eq:Denominator_TwoParameterDiscreteBayesSimple}{{6.19}{133}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Continuous multi-parameter example: mean and variance of IQ}{134}}
\newlabel{sec:Denominator_continuousTwoParameterIQ}{{6.4.2}{134}}
\newlabel{eq:Denominator_continuousTwoParameterLikelihood}{{6.20}{134}}
\newlabel{sec:Denominator_continuousMultiparameterIndependence}{{6.21}{134}}
\newlabel{eq:Denominator_continuousTwoParameterPrior}{{6.22}{135}}
\newlabel{eq:Denominator_continuousTwoParameterJointPosterior}{{6.23}{135}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The prior, likelihood, and posterior distributions for the mean and variance of IQ example described in section 6.4.2\hbox {}.\relax }}{136}}
\newlabel{fig:Denominator_continuousTwoParameter3D}{{6.4.2}{136}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}How to dispense with the difficulty: Bayesian computation}{137}}
\newlabel{sec:Denominator_dispensingWithNumerator}{{6.5}{137}}
\newlabel{eq:Denominator_proportional}{{6.27}{138}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Chapter summary}{139}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Appendix}{139}}
\newlabel{sec:Denominator_appendix}{{6.7}{139}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Analytic Bayesian methods}{141}}
\newlabel{part:analyticalBayes}{{II}{143}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}An introduction to distributions for the mathematically-un-inclined}{143}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:distributions}{{7}{143}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conjugate priors and their place in Bayesian analysis}{145}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conjugate}{{8}{145}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Objective Bayesian analysis}{147}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ObjectiveBayes}{{9}{147}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}A practical guide to doing real life Bayesian analysis: Computational Bayes}{149}}
\newlabel{part:computationalBayes}{{III}{151}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Hierarchical models}{151}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hierarchicalModels}{{10}{151}}
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Regression analysis and hierarchical models}{153}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Hypothesis testing I: Classical frequentist vs Bayesian approaches}{155}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{Bayes}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Evaluation of model fit}{157}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ModelFit}{{12}{157}}
\bibcite{angrist1990lifetime}{{1}{}{{}}{{}}}
\bibcite{bolstad2007introduction}{{2}{}{{}}{{}}}
\bibcite{epstein2008model}{{3}{}{{}}{{}}}
\bibcite{gelman2013bayesian}{{4}{}{{}}{{}}}
\bibcite{gill2007bayesian}{{5}{}{{}}{{}}}
\bibcite{ioannidis2005most}{{6}{}{{}}{{}}}
\bibcite{silver2012signal}{{7}{}{{}}{{}}}
\bibcite{stewart2014teaching}{{8}{}{{}}{{}}}
\bibcite{tegmark2014our}{{9}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
