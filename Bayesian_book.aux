\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}How to best use this book}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Understanding the Bayesian formula}{9}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The subjective worlds of frequentist and Bayesian statistics}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}The posterior - the goal of Bayesian inference}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Likelihoods}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Likelihoods}{{4}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Chapter Mission statement}{15}}
\newlabel{eq:Likelihood_BayesHighlighted}{{4.1}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Chapter goals}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}What is a likelihood?}{16}}
\newlabel{eq:Likelihood_Bayes}{{4.3}{16}}
\newlabel{eq:Likelihood_simple}{{4.3}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Insert bar chart here of the number of heads along the x axis - 0,1,2 - and the associated probability of each of these outcomes as being the bar height - (1/4,1/2,1/4).\relax }}{17}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Likelihood_fairCoin}{{4.1}{17}}
\newlabel{eq:Likelihood_fairCoin}{{4.3}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces An example posterior distribution for the probability of a heads.\relax }}{18}}
\newlabel{fig:Likelihood_posteriorExample}{{4.2}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The x-axis here is theta, ranging between 0 and 1, assuming that one head is obtained this graphs the likelihood, which does not sum to 1.\relax }}{18}}
\newlabel{fig:Likelihood_coinLikelihood}{{4.3}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Why is a likelihood not a probability for Bayesians?}{18}}
\newlabel{eq:Likelihood_notation}{{4.4}{19}}
\newlabel{eq:Likelihood_OneHead}{{4.6}{19}}
\newlabel{eq:Likelihood_TwoHead}{{4.8}{19}}
\citation{epstein2008model}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The values of likelihood for the case of tossing a coin twice, where the probability of heads is constrained to take on a discrete value: \{0.0,0.2,0.4,0.6,0.8,1.0\}. In each of the rows, the value of $theta$ is held constant, meaning that $P(data|\theta )$ is a proper probability distribution and thus the probabilities sum to 1. However, in the columns, the data - the number of heads thrown - is held constant, and thus the probabilities do not sum to 1, and we thus we are better off viewing these data as likelihoods, since they do not satisfy the properties of a proper probability distribution.\relax }}{20}}
\newlabel{tab:Likeihood_BayesBox}{{4.1}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}What are models, and why do we need them?}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}How to choose an appropriate model for likelihood?}{22}}
\newlabel{sec:chooseLikelihood}{{4.6}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}A likelihood model for an individual's disease status}{22}}
\newlabel{sec:Likelihood_individualDisease}{{4.6.1}{22}}
\newlabel{eq:Likelihood_SimpleModel1}{{4.10}{23}}
\newlabel{eq:Likelihood_SimpleModel2}{{4.11}{23}}
\newlabel{eq:Likelihood_bernoulli}{{4.12}{23}}
\newlabel{eq:Likelihood_SimpleModel3}{{4.13}{23}}
\newlabel{eq:Likelihood_SimpleModel4}{{4.14}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. For a fixed value of $\theta $ we find that the sum across the values of the probability is always 1. This is because when viewed in this way, the likelihood is really a discrete probability density across the two values which x can take on. However, when we hold the data fixed (choose either the red or blue line) and sum the likelihood horizontally across the values of $theta$ we do not find that the sum is generally equal to 1. \relax }}{24}}
\newlabel{fig:Likelihood_bernoulli}{{4.4}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}A likelihood model for disease prevalence of a group}{24}}
\newlabel{sec:Likelihood_diseaseGroup}{{4.6.2}{24}}
\newlabel{eq:Likelihood_bernoulli1}{{4.15}{25}}
\newlabel{eq:Likelihood_bernoulli2}{{4.16}{26}}
\newlabel{eq:Likelihood_bernoulli3}{{4.17}{26}}
\newlabel{eq:Likelihood_binomialTwo}{{4.18}{26}}
\newlabel{eq:Likelihood_binomialTwoProbs}{{4.19}{26}}
\newlabel{eq:Likelihood_binomialTwoProbsSimple}{{4.20}{27}}
\newlabel{eq:Likelihood_binomialNearly}{{4.21}{27}}
\newlabel{eq:Likelihood_quadratic}{{4.22}{27}}
\newlabel{eq:Likelihood_nCr}{{4.23}{27}}
\newlabel{eq:Likelihood_binomialTwoFull}{{4.24}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. For a fixed value of $\theta $ we find that the sum across the values of the probability is always 1. This is because when viewed in this way, the likelihood is really a discrete probability density across the two values which x can take on. However, when we hold the data fixed (choose either the red or blue line) and sum area under the likelihood curve across the values of $theta$ we do not find that the sum is generally equal to 1. \relax }}{28}}
\newlabel{fig:Likelihood_binomial}{{4.5}{28}}
\newlabel{eq:Likelihood_binomialThreeProbsSimple}{{4.25}{29}}
\newlabel{eq:Likelihood_binomialThreeProbsSimpler}{{4.26}{29}}
\newlabel{eq:Likelihood_binomialThreeFull}{{4.27}{29}}
\newlabel{eq:Likelihood_binomialNFull}{{4.28}{29}}
\newlabel{eq:Likelihood_binomialTest}{{4.6.2}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}The intelligence of a group of people}{30}}
\newlabel{sec:Likelihood_normal}{{4.6.3}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces A figure with LHS showing a normal distribution with the area to the right of 140 indicated by shading. The RHS shows a normal CDF with the value of 140 indicated.\relax }}{31}}
\newlabel{fig:Likelihood_normal}{{4.6}{31}}
\newlabel{eq:Likelihood_normal}{{4.6.3}{31}}
\newlabel{eq:Likelihood_normalSampleOne}{{4.31}{32}}
\newlabel{eq:Likelihood_normalN}{{4.6.3}{32}}
\newlabel{eq:Likelihood_normalSampleN}{{4.6.3}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}The subjectivity of model choice}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Maximum likelihood - a short introduction}{34}}
\newlabel{sec:Likelihood_MLE}{{4.8}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Estimating disease prevalence}{34}}
\newlabel{sec:Likelihood_diseaseMLE}{{4.8.1}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A figure with four panes. The top-left is log-likelihood as a function of likelihood. The bottom-left is likelihood as a function of theta, and log-likelihood plotted on the same axis. Top-right is a weird function of likelihood as a function of likelihood. Below it a graph of likelihood as a function of theta, with a different maximum reached for the weird function.\relax }}{35}}
\newlabel{fig:Likelihood_logMonotonicity}{{4.7}{35}}
\newlabel{eq:Likelihood_binomialNew}{{4.34}{35}}
\newlabel{eq:Likelihood_logLikelihoodBinomial}{{4.35}{35}}
\newlabel{eq:Likelihood_logRules}{{4.36}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces A figure displaying log-likelihood as a function of theta, maximised at 1/10.\relax }}{36}}
\newlabel{fig:Likelihood_MLE}{{4.8}{36}}
\newlabel{eq:Likelihood_binomialderiv}{{4.37}{36}}
\newlabel{eq:Likelihood_binomialestimator}{{4.38}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Estimating the mean and variance in intelligence scores}{36}}
\newlabel{eq:Likelihood_normalTwo}{{4.39}{36}}
\newlabel{eq:Likelihood_diseaseLogLikelihood}{{4.40}{37}}
\newlabel{eq:Likelihood_diseaseDerivativeOne}{{4.41}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Frequentist inference in Maximum Likelihood}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Chapter summary}{38}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Priors}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Chapter Mission statement}{39}}
\newlabel{eq:Prior_BayesHighlighted}{{5.1}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Chapter goals}{39}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}What are priors, and what do they represent?}{40}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Left - a prior for a doctor's pre-testing diagnostic probability of an individual having a disease. Right - a prior which represents pre-sample uncertainty in disease prevalence.\relax }}{41}}
\newlabel{fig:Prior_introduction}{{5.1}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Why don't we just normalise likelihood by choosing a unity prior?}{42}}
\newlabel{sec:Prior_unityPrior}{{5.4}{42}}
\newlabel{eq:Prior_BayesNormalisedLikelihood}{{5.4}{42}}
\citation{stewart2014teaching}
\citation{bolstad2007introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces A figure showing in the left hand panel the Venn diagrams when we assume that a) the coin is fair with a quarter of the area shaded, and the other half not. b) Biased, and the majority of the figure is shaded - corresponding to a high overlap between data and parameter. The right hand panel then shows the joint probability of the data and the parameters $\theta $, and we see that even though the ratio of the area is better for b), it is much less likely \textit  {a priori} that the coin is biased. The bottom panel shows the ML implied posterior distribution, with the bar for unfairness much higher than for fairness. The right shows a much more logical conclusion which takes into account their prior probabilities.\relax }}{44}}
\newlabel{fig:Prior_justification}{{5.2}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}The explicit subjectivity of priors}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Combining a prior and likelihood to form a posterior}{44}}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}An urn of balls\let \reserved@d =[\def \par }{45}}
\newlabel{sec:Prior_urn}{{5.6.1}{45}}
\newlabel{eq:Prior_bernoulli}{{5.3}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 5.6.1\hbox {}. The prior in the upper panel gives uniform weighting to all possible numbers of red balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{46}}
\newlabel{fig:Prior_urnStacked}{{5.3}{46}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here we assume that pre-experiment all possible numbers of red balls are equally likely, by adopting a uniform prior.\relax }}{47}}
\newlabel{tab:Prior_bayesBoxDiscreteUrns}{{5.1}{47}}
\newlabel{tab:addlabel}{{5.1}{47}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here a higher weighting is given to more equal numbers of red and white balls in the prior.\relax }}{47}}
\newlabel{tab:Prior_bayesBoxUrnUpdated}{{5.2}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 5.6.1\hbox {}. The prior in the upper panel gives more weighting to more equal numbers of red and white balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{48}}
\newlabel{fig:Prior_bayesUrnUpdated}{{5.4}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Disease proportions revisited}{49}}
\newlabel{sec:Prior_diseaseProp}{{5.6.2}{49}}
\newlabel{eq:Prior_binomial}{{5.4}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}The numerator of Bayes' rule determines the shape}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The prior, likelihood and posterior for the disease proportion example described in section 5.6.2\hbox {}. Each point in $\theta $ along the continuous prior curve (top panel) is multiplied by the corresponding value of likelihood (middle panel), to form the numerator of Bayes' rule. The numerator is then normalised to make the posterior probability density shown in the bottom panel.\relax }}{50}}
\newlabel{fig:Prior_disease}{{5.5}{50}}
\newlabel{eq:Prior_BayesNumerator}{{5.6.3}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Constructing priors}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Vague priors}{51}}
\newlabel{sec:Prior_vague}{{5.7.1}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The probability density for obtaining all diseased individuals within sample sizes of 1, 2 and 10 respectively. Starting out with a flat prior for the probability that one individual has a disease has resulted in non-flat priors for the other 2 probabilities.\relax }}{52}}
\newlabel{fig:Prior_jeffreysIntro}{{5.6}{52}}
\newlabel{eq:Prior_BayesFlatPrior}{{5.7.1}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Two viable prior distributions for the average time taken before the onset of lung cancer after patients begin smoking.\relax }}{54}}
\newlabel{fig:Prior_lungcancerFlatandGammaPriors}{{5.7}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Informative priors}{54}}
\citation{gill2007bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The SAT scores for past students of a class. The mean and variance of this hypothetical sample are 1404, and 79,716 respectively, which are used to fit a normal distribution to the data, and is shown in red.\relax }}{55}}
\newlabel{fig:Prior_SATScoresHistogram}{{5.8}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Eliciting priors}{56}}
\newlabel{eq:Prior_elicitingPriorNormal}{{5.7}{56}}
\newlabel{eq:Prior_elicitingPriorNormalRegression}{{5.8}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Hypothetical data for the 25th and 75th percentiles of the estimated wage premium from 10 experts. In the left hand panel we regress these percentiles on the corresponding percentiles from a standard normal distribution, yielding estimates of the mean and variance of a normal prior, which is shown on the right.\relax }}{57}}
\newlabel{fig:Prior_elicitingRegression}{{5.9}{57}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}A strong model is not heavily influenced by priors}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The effect of increasing sample size on the posterior density for the prevalence of a disease in a population. The leftmost column has N=10, the middle N=100, and the rightmost N=1,000. All three have the same proportion of disease cases in the sample.\relax }}{58}}
\newlabel{fig:Prior_weakPriorEffect}{{5.10}{58}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Chapter summary}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Appendix}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10.1}Bayes' rule for the urn}{59}}
\newlabel{app:Prior_bayesUrn}{{5.10.1}{59}}
\newlabel{eq:Prior_bayesDiscreteForm}{{5.9}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10.2}The probabilities of having a disease}{60}}
\newlabel{app:Prior_diseaseJeffreys}{{5.10.2}{60}}
\newlabel{eq:Prior_appChangeOfVariables}{{5.10}{60}}
\newlabel{eq:Prior_appChangeOfVariablesSolved}{{5.11}{60}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The difficulty is in the denominator}{61}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:denominator}{{6}{61}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}An introduction to distributions for the mathematically-un-inclined}{63}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:distributions}{{7}{63}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conjugate priors and their place in Bayesian analysis}{65}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Objective Bayesian analysis}{67}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ObjectiveBayes}{{9}{67}}
\bibstyle{plain}
\bibdata{Bayes}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Hierarchical models}{69}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hierarchicalModels}{{10}{69}}
\bibcite{bolstad2007introduction}{{1}{}{{}}{{}}}
\bibcite{epstein2008model}{{2}{}{{}}{{}}}
\bibcite{gelman2013bayesian}{{3}{}{{}}{{}}}
\bibcite{gill2007bayesian}{{4}{}{{}}{{}}}
\bibcite{stewart2014teaching}{{5}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
