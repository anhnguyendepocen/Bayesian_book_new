\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}How to best use this book}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Understanding the Bayesian formula}{7}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The subjective worlds of frequentist and Bayesian statistics}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Likelihoods}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Likelihoods}{{3}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Chapter Mission statement}{11}}
\newlabel{eq:Likelihood_BayesHighlighted}{{3.1}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Chapter goals}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}What is a likelihood?}{12}}
\newlabel{eq:Likelihood_Bayes}{{3.3}{12}}
\newlabel{eq:Likelihood_simple}{{3.3}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Insert bar chart here of the number of heads along the x axis - 0,1,2 - and the associated probability of each of these outcomes as being the bar height - (1/4,1/2,1/4).\relax }}{13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Likelihood_fairCoin}{{3.1}{13}}
\newlabel{eq:Likelihood_fairCoin}{{3.3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An example posterior distribution for the probability of a heads.\relax }}{14}}
\newlabel{fig:Likelihood_posteriorExample}{{3.2}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The x-axis here is theta, ranging between 0 and 1, assuming that one head is obtained this graphs the likelihood, which does not sum to 1.\relax }}{14}}
\newlabel{fig:Likelihood_coinLikelihood}{{3.3}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Why is a likelihood not a probability for Bayesians?}{14}}
\newlabel{eq:Likelihood_notation}{{3.4}{15}}
\newlabel{eq:Likelihood_OneHead}{{3.6}{15}}
\newlabel{eq:Likelihood_TwoHead}{{3.8}{15}}
\citation{epstein2008model}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The values of likelihood for the case of tossing a coin twice, where the probability of heads is constrained to take on a discrete value: \{0.0,0.2,0.4,0.6,0.8,1.0\}. In each of the rows, the value of $theta$ is held constant, meaning that $P(data|\theta )$ is a proper probability distribution and thus the probabilities sum to 1. However, in the columns, the data - the number of heads thrown - is held constant, and thus the probabilities do not sum to 1, and we thus we are better off viewing these data as likelihoods, since they do not satisfy the properties of a proper probability distribution.\relax }}{16}}
\newlabel{tab:Likeihood_BayesBox}{{3.1}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}What are models, and why do we need them?}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}How to choose an appropriate model for likelihood?}{18}}
\newlabel{sec:chooseLikelihood}{{3.6}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}A likelihood model for an individual's disease status}{18}}
\newlabel{sec:Likelihood_individualDisease}{{3.6.1}{18}}
\newlabel{eq:Likelihood_SimpleModel1}{{3.10}{19}}
\newlabel{eq:Likelihood_SimpleModel2}{{3.11}{19}}
\newlabel{eq:Likelihood_bernoulli}{{3.12}{19}}
\newlabel{eq:Likelihood_SimpleModel3}{{3.13}{19}}
\newlabel{eq:Likelihood_SimpleModel4}{{3.14}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. For a fixed value of $\theta $ we find that the sum across the values of the probability is always 1. This is because when viewed in this way, the likelihood is really a discrete probability density across the two values which x can take on. However, when we hold the data fixed (choose either the red or blue line) and sum the likelihood horizontally across the values of $theta$ we do not find that the sum is generally equal to 1. \relax }}{20}}
\newlabel{fig:Likelihood_bernoulli}{{3.4}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}A likelihood model for disease prevalence of a group}{20}}
\newlabel{sec:Likelihood_diseaseGroup}{{3.6.2}{20}}
\newlabel{eq:Likelihood_bernoulli1}{{3.15}{21}}
\newlabel{eq:Likelihood_bernoulli2}{{3.16}{22}}
\newlabel{eq:Likelihood_bernoulli3}{{3.17}{22}}
\newlabel{eq:Likelihood_binomialTwo}{{3.18}{22}}
\newlabel{eq:Likelihood_binomialTwoProbs}{{3.19}{22}}
\newlabel{eq:Likelihood_binomialTwoProbsSimple}{{3.20}{23}}
\newlabel{eq:Likelihood_binomialNearly}{{3.21}{23}}
\newlabel{eq:Likelihood_quadratic}{{3.22}{23}}
\newlabel{eq:Likelihood_nCr}{{3.23}{23}}
\newlabel{eq:Likelihood_binomialTwoFull}{{3.24}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. For a fixed value of $\theta $ we find that the sum across the values of the probability is always 1. This is because when viewed in this way, the likelihood is really a discrete probability density across the two values which x can take on. However, when we hold the data fixed (choose either the red or blue line) and sum area under the likelihood curve across the values of $theta$ we do not find that the sum is generally equal to 1. \relax }}{24}}
\newlabel{fig:Likelihood_binomial}{{3.5}{24}}
\newlabel{eq:Likelihood_binomialThreeProbsSimple}{{3.25}{25}}
\newlabel{eq:Likelihood_binomialThreeProbsSimpler}{{3.26}{25}}
\newlabel{eq:Likelihood_binomialThreeFull}{{3.27}{25}}
\newlabel{eq:Likelihood_binomialNFull}{{3.28}{25}}
\newlabel{eq:Likelihood_binomialTest}{{3.6.2}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}The intelligence of a group of people}{26}}
\newlabel{sec:Likelihood_normal}{{3.6.3}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A figure with LHS showing a normal distribution with the area to the right of 140 indicated by shading. The RHS shows a normal CDF with the value of 140 indicated.\relax }}{27}}
\newlabel{fig:Likelihood_normal}{{3.6}{27}}
\newlabel{eq:Likelihood_normal}{{3.6.3}{27}}
\newlabel{eq:Likelihood_normalSampleOne}{{3.31}{28}}
\newlabel{eq:Likelihood_normalN}{{3.6.3}{28}}
\newlabel{eq:Likelihood_normalSampleN}{{3.6.3}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}The subjectivity of model choice}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Maximum likelihood - a short introduction}{30}}
\newlabel{sec:Likelihood_MLE}{{3.8}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Estimating disease prevalence}{30}}
\newlabel{sec:Likelihood_diseaseMLE}{{3.8.1}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A figure with four panes. The top-left is log-likelihood as a function of likelihood. The bottom-left is likelihood as a function of theta, and log-likelihood plotted on the same axis. Top-right is a weird function of likelihood as a function of likelihood. Below it a graph of likelihood as a function of theta, with a different maximum reached for the weird function.\relax }}{31}}
\newlabel{fig:Likelihood_logMonotonicity}{{3.7}{31}}
\newlabel{eq:Likelihood_binomialNew}{{3.34}{31}}
\newlabel{eq:Likelihood_logLikelihoodBinomial}{{3.35}{31}}
\newlabel{eq:Likelihood_logRules}{{3.36}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces A figure displaying log-likelihood as a function of theta, maximised at 1/10.\relax }}{32}}
\newlabel{fig:Likelihood_MLE}{{3.8}{32}}
\newlabel{eq:Likelihood_binomialderiv}{{3.37}{32}}
\newlabel{eq:Likelihood_binomialestimator}{{3.38}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Estimating the mean and variance in intelligence scores}{32}}
\newlabel{eq:Likelihood_normalTwo}{{3.39}{32}}
\newlabel{eq:Likelihood_diseaseLogLikelihood}{{3.40}{33}}
\newlabel{eq:Likelihood_diseaseDerivativeOne}{{3.41}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Frequentist inference in Maximum Likelihood}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Chapter summary}{34}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Priors}{35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Chapter Mission statement}{35}}
\newlabel{eq:Prior_BayesHighlighted}{{4.1}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Chapter goals}{35}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}What are priors, and what do they represent?}{36}}
\bibstyle{plain}
\bibdata{Bayes}
\newlabel{eq:Prior_BayesNormalisedLikelihood}{{4.3}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}The explicit subjectivity of priors}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}An urn of balls}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Disease proportions revisited}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Constructing priors}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Vague priors}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Informative priors}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Conjugate priors}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Eliciting priors}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}A strong model is independent of priors}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Chapter summary}{38}}
\bibcite{epstein2008model}{{1}{}{{}}{{}}}
\bibcite{gill2007bayesian}{{2}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
