\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}How to best use this book}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Understanding the Bayesian formula}{9}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The subjective worlds of frequentist and Bayesian statistics}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Likelihoods}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Likelihoods}{{3}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Chapter Mission statement}{13}}
\newlabel{eq:Likelihood_BayesHighlighted}{{3.1}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Chapter goals}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}What is a likelihood?}{14}}
\newlabel{eq:Likelihood_Bayes}{{3.3}{14}}
\newlabel{eq:Likelihood_simple}{{3.3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Insert bar chart here of the number of heads along the x axis - 0,1,2 - and the associated probability of each of these outcomes as being the bar height - (1/4,1/2,1/4).\relax }}{15}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Likelihood_fairCoin}{{3.1}{15}}
\newlabel{eq:Likelihood_fairCoin}{{3.3}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An example posterior distribution for the probability of a heads.\relax }}{16}}
\newlabel{fig:Likelihood_posteriorExample}{{3.2}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The x-axis here is theta, ranging between 0 and 1, assuming that one head is obtained this graphs the likelihood, which does not sum to 1.\relax }}{16}}
\newlabel{fig:Likelihood_coinLikelihood}{{3.3}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Why is a likelihood not a probability for Bayesians?}{16}}
\newlabel{eq:Likelihood_notation}{{3.4}{17}}
\newlabel{eq:Likelihood_OneHead}{{3.6}{17}}
\newlabel{eq:Likelihood_TwoHead}{{3.8}{17}}
\citation{epstein2008model}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The values of likelihood for the case of tossing a coin twice, where the probability of heads is constrained to take on a discrete value: \{0.0,0.2,0.4,0.6,0.8,1.0\}. In each of the rows, the value of $theta$ is held constant, meaning that $P(data|\theta )$ is a proper probability distribution and thus the probabilities sum to 1. However, in the columns, the data - the number of heads thrown - is held constant, and thus the probabilities do not sum to 1, and we thus we are better off viewing these data as likelihoods, since they do not satisfy the properties of a proper probability distribution.\relax }}{18}}
\newlabel{tab:Likeihood_BayesBox}{{3.1}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}What are models, and why do we need them?}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}How to choose an appropriate model for likelihood?}{20}}
\newlabel{sec:chooseLikelihood}{{3.6}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}A likelihood model for an individual's disease status}{20}}
\newlabel{sec:Likelihood_individualDisease}{{3.6.1}{20}}
\newlabel{eq:Likelihood_SimpleModel1}{{3.10}{21}}
\newlabel{eq:Likelihood_SimpleModel2}{{3.11}{21}}
\newlabel{eq:Likelihood_bernoulli}{{3.12}{21}}
\newlabel{eq:Likelihood_SimpleModel3}{{3.13}{21}}
\newlabel{eq:Likelihood_SimpleModel4}{{3.14}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. For a fixed value of $\theta $ we find that the sum across the values of the probability is always 1. This is because when viewed in this way, the likelihood is really a discrete probability density across the two values which x can take on. However, when we hold the data fixed (choose either the red or blue line) and sum the likelihood horizontally across the values of $theta$ we do not find that the sum is generally equal to 1. \relax }}{22}}
\newlabel{fig:Likelihood_bernoulli}{{3.4}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}A likelihood model for disease prevalence of a group}{22}}
\newlabel{sec:Likelihood_diseaseGroup}{{3.6.2}{22}}
\newlabel{eq:Likelihood_bernoulli1}{{3.15}{23}}
\newlabel{eq:Likelihood_bernoulli2}{{3.16}{24}}
\newlabel{eq:Likelihood_bernoulli3}{{3.17}{24}}
\newlabel{eq:Likelihood_binomialTwo}{{3.18}{24}}
\newlabel{eq:Likelihood_binomialTwoProbs}{{3.19}{24}}
\newlabel{eq:Likelihood_binomialTwoProbsSimple}{{3.20}{25}}
\newlabel{eq:Likelihood_binomialNearly}{{3.21}{25}}
\newlabel{eq:Likelihood_quadratic}{{3.22}{25}}
\newlabel{eq:Likelihood_nCr}{{3.23}{25}}
\newlabel{eq:Likelihood_binomialTwoFull}{{3.24}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. For a fixed value of $\theta $ we find that the sum across the values of the probability is always 1. This is because when viewed in this way, the likelihood is really a discrete probability density across the two values which x can take on. However, when we hold the data fixed (choose either the red or blue line) and sum area under the likelihood curve across the values of $theta$ we do not find that the sum is generally equal to 1. \relax }}{26}}
\newlabel{fig:Likelihood_binomial}{{3.5}{26}}
\newlabel{eq:Likelihood_binomialThreeProbsSimple}{{3.25}{27}}
\newlabel{eq:Likelihood_binomialThreeProbsSimpler}{{3.26}{27}}
\newlabel{eq:Likelihood_binomialThreeFull}{{3.27}{27}}
\newlabel{eq:Likelihood_binomialNFull}{{3.28}{27}}
\newlabel{eq:Likelihood_binomialTest}{{3.6.2}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}The intelligence of a group of people}{28}}
\newlabel{sec:Likelihood_normal}{{3.6.3}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A figure with LHS showing a normal distribution with the area to the right of 140 indicated by shading. The RHS shows a normal CDF with the value of 140 indicated.\relax }}{29}}
\newlabel{fig:Likelihood_normal}{{3.6}{29}}
\newlabel{eq:Likelihood_normal}{{3.6.3}{29}}
\newlabel{eq:Likelihood_normalSampleOne}{{3.31}{30}}
\newlabel{eq:Likelihood_normalN}{{3.6.3}{30}}
\newlabel{eq:Likelihood_normalSampleN}{{3.6.3}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}The subjectivity of model choice}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Maximum likelihood - a short introduction}{32}}
\newlabel{sec:Likelihood_MLE}{{3.8}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Estimating disease prevalence}{32}}
\newlabel{sec:Likelihood_diseaseMLE}{{3.8.1}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A figure with four panes. The top-left is log-likelihood as a function of likelihood. The bottom-left is likelihood as a function of theta, and log-likelihood plotted on the same axis. Top-right is a weird function of likelihood as a function of likelihood. Below it a graph of likelihood as a function of theta, with a different maximum reached for the weird function.\relax }}{33}}
\newlabel{fig:Likelihood_logMonotonicity}{{3.7}{33}}
\newlabel{eq:Likelihood_binomialNew}{{3.34}{33}}
\newlabel{eq:Likelihood_logLikelihoodBinomial}{{3.35}{33}}
\newlabel{eq:Likelihood_logRules}{{3.36}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces A figure displaying log-likelihood as a function of theta, maximised at 1/10.\relax }}{34}}
\newlabel{fig:Likelihood_MLE}{{3.8}{34}}
\newlabel{eq:Likelihood_binomialderiv}{{3.37}{34}}
\newlabel{eq:Likelihood_binomialestimator}{{3.38}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Estimating the mean and variance in intelligence scores}{34}}
\newlabel{eq:Likelihood_normalTwo}{{3.39}{34}}
\newlabel{eq:Likelihood_diseaseLogLikelihood}{{3.40}{35}}
\newlabel{eq:Likelihood_diseaseDerivativeOne}{{3.41}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Frequentist inference in Maximum Likelihood}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Chapter summary}{36}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Priors}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Chapter Mission statement}{37}}
\newlabel{eq:Prior_BayesHighlighted}{{4.1}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Chapter goals}{37}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}What are priors, and what do they represent?}{38}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Left hand panel shows a bar chart with 75\% assigned to the patient having the disease. Right hand panel is a continuous normal plot centred around 5\%.\relax }}{39}}
\newlabel{fig:Prior_introduction}{{4.1}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Left hand panel shows a bar chart with 75\% assigned to the patient having the disease. Right hand panel is a continuous normal plot centred around 5\%.\relax }}{40}}
\newlabel{fig:Prior_coinHistogram}{{4.2}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Why don't we just normalise likelihood by choosing a unity prior?}{40}}
\newlabel{sec:Prior_unityPrior}{{4.4}{40}}
\newlabel{eq:Prior_BayesNormalisedLikelihood}{{4.4}{41}}
\citation{stewart2014teaching}
\citation{bolstad2007introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A figure showing in the left hand panel the Venn diagrams when we assume that a) the coin is fair with a quarter of the area shaded, and the other half not. b) Biased, and the majority of the figure is shaded - corresponding to a high overlap between data and parameter. The right hand panel then shows the joint probability of the data and the parameters $\theta $, and we see that even though the ratio of the area is better for b), it is much less likely \textit  {a priori} that the coin is biased. The bottom panel shows the ML implied posterior distribution, with the bar for unfairness much higher than for fairness. The right shows a much more logical conclusion which takes into account their prior probabilities.\relax }}{42}}
\newlabel{fig:Prior_justification}{{4.3}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}The explicit subjectivity of priors}{42}}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Combining a prior and likelihood to form a posterior}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}An urn of balls\let \reserved@d =[\def \par }{43}}
\newlabel{sec:Prior_urn}{{4.6.1}{43}}
\newlabel{eq:Prior_bernoulli}{{4.3}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here we assume that pre-experiment all possible numbers of red balls are equally likely, by adopting a uniform prior.\relax }}{44}}
\newlabel{tab:Prior_bayesBoxDiscreteUrns}{{4.1}{44}}
\newlabel{tab:addlabel}{{4.1}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 4.6.1\hbox {}. The prior in the upper panel gives uniform weighting to all possible numbers of red balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{45}}
\newlabel{fig:Prior_urnStacked}{{4.4}{45}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here a higher weighting is given to more equal numbers of red and white balls in the prior.\relax }}{46}}
\newlabel{tab:Prior_bayesBoxUrnUpdated}{{4.2}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Disease proportions revisited}{46}}
\newlabel{sec:Prior_diseaseProp}{{4.6.2}{46}}
\newlabel{eq:Prior_binomial}{{4.4}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 4.6.1\hbox {}. The prior in the upper panel gives more weighting to more equal numbers of red and white balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{47}}
\newlabel{fig:Prior_bayesUrnUpdated}{{4.5}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The prior, likelihood and posterior for the disease proportion example described in section 4.6.2\hbox {}. Each point in $\theta $ along the continuous prior curve (top panel) is multiplied by the corresponding value of likelihood (middle panel), to form the numerator of Bayes' rule. The numerator is then normalised to make the posterior probability density shown in the bottom panel.\relax }}{48}}
\newlabel{fig:Prior_disease}{{4.6}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}The numerator of Bayes' rule determines the shape}{49}}
\newlabel{eq:Prior_BayesNumerator}{{4.6.3}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Constructing priors}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Vague priors}{49}}
\newlabel{sec:Prior_vague}{{4.7.1}{49}}
\newlabel{eq:Prior_BayesFlatPrior}{{4.7.1}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The probability density for obtaining all diseased individuals within sample sizes of 1, 2 and 10 respectively. Starting out with a flat prior for the probability that one individual has a disease has resulted in non-flat priors for the other 2 probabilities.\relax }}{51}}
\newlabel{fig:Prior_jeffreysIntro}{{4.7}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Informative priors}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Two viable prior distributions for the average time taken before the onset of lung cancer after patients begin smoking.\relax }}{53}}
\newlabel{fig:Prior_lungcancerFlatandGammaPriors}{{4.8}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces The SAT scores for past students of a class. The mean and variance of this hypothetical sample are 1404, and 79,716 respectively, which are used to fit a normal distribution to the data, and is shown in red.\relax }}{54}}
\newlabel{fig:Prior_SATScoresHistogram}{{4.9}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}A strong model is independent of priors}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Chapter summary}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Appendix}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}Bayes' rule for the urn}{55}}
\newlabel{app:Prior_bayesUrn}{{4.10.1}{55}}
\newlabel{eq:Prior_bayesDiscreteForm}{{4.7}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.2}The probabilities of having a disease}{55}}
\newlabel{app:Prior_diseaseJeffreys}{{4.10.2}{55}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}The difficulty is in the denominator}{57}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:denominator}{{5}{57}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Inference using the posterior}{59}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}An introduction to distributions for the mathematically-un-inclined}{61}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:distributions}{{7}{61}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conjugate priors and their place in Bayesian analysis}{63}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Objective Bayesian analysis}{65}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ObjectiveBayes}{{9}{65}}
\bibstyle{plain}
\bibdata{Bayes}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Hierarchical models}{67}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hierarchicalModels}{{10}{67}}
\bibcite{bolstad2007introduction}{{1}{}{{}}{{}}}
\bibcite{epstein2008model}{{2}{}{{}}{{}}}
\bibcite{gelman2013bayesian}{{3}{}{{}}{{}}}
\bibcite{gill2007bayesian}{{4}{}{{}}{{}}}
\bibcite{stewart2014teaching}{{5}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
