\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}How to best use this book}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Understanding the Bayesian formula}{9}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The subjective worlds of frequentist and Bayesian statistics}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}The posterior - the goal of Bayesian inference}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Likelihoods}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Likelihoods}{{4}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Chapter Mission statement}{15}}
\newlabel{eq:Likelihood_BayesHighlighted}{{4.1}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Chapter goals}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}What is a likelihood?}{16}}
\newlabel{eq:Likelihood_Bayes}{{4.3}{16}}
\newlabel{eq:Likelihood_simple}{{4.3}{16}}
\newlabel{eq:Likelihood_fairCoin}{{4.3}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Why use 'likelihood' rather than 'probability'?}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The probabilities of all possible numbers of heads for a fair coin.\relax }}{18}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Likelihood_fairCoin}{{4.1}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces An example posterior distribution for the probability of obtaining a heads in a coin toss.\relax }}{19}}
\newlabel{fig:Likelihood_posteriorExample}{{4.2}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The likelihood function for obtaining a single head from two throws. The area under the curve is $\frac  {1}{3}$.\relax }}{19}}
\newlabel{fig:Likelihood_coinLikelihood}{{4.3}{19}}
\newlabel{eq:Likelihood_notation}{{4.5}{20}}
\newlabel{eq:Likelihood_OneHead}{{4.6}{20}}
\newlabel{eq:Likelihood_TwoHead}{{4.8}{20}}
\citation{epstein2008model}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The values of likelihood for the case of tossing a coin twice, where the probability of heads is constrained to take on a discrete value: \{0.0,0.2,0.4,0.6,0.8,1.0\}.\relax }}{21}}
\newlabel{tab:Likeihood_BayesBox}{{4.1}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}What are models and why do we need them?}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}How to choose an appropriate likelihood?}{22}}
\newlabel{sec:chooseLikelihood}{{4.6}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}A likelihood model for an individual's disease status}{23}}
\newlabel{sec:Likelihood_individualDisease}{{4.6.1}{23}}
\newlabel{eq:Likelihood_SimpleModel1}{{4.10}{24}}
\newlabel{eq:Likelihood_SimpleModel2}{{4.11}{24}}
\newlabel{eq:Likelihood_bernoulli}{{4.12}{24}}
\newlabel{eq:Likelihood_SimpleModel3}{{4.13}{24}}
\newlabel{eq:Likelihood_SimpleModel4}{{4.14}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}A likelihood model for disease prevalence of a group}{24}}
\newlabel{sec:Likelihood_diseaseGroup}{{4.6.2}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The likelihood function as theta varies for the case of the two possible data. The sum of likelihoods is found by the area under each line, whereas the sum of probabilities is a discrete sum.\relax }}{25}}
\newlabel{fig:Likelihood_bernoulli}{{4.4}{25}}
\newlabel{eq:Likelihood_bernoulli1}{{4.15}{26}}
\newlabel{eq:Likelihood_bernoulli2}{{4.16}{26}}
\newlabel{eq:Likelihood_bernoulli3}{{4.17}{27}}
\newlabel{eq:Likelihood_binomialTwo}{{4.18}{27}}
\newlabel{eq:Likelihood_binomialTwoProbs}{{4.19}{27}}
\newlabel{eq:Likelihood_binomialTwoProbsSimple}{{4.20}{27}}
\newlabel{eq:Likelihood_binomialNearly}{{4.21}{28}}
\newlabel{eq:Likelihood_quadratic}{{4.22}{28}}
\newlabel{eq:Likelihood_nCr}{{4.23}{28}}
\newlabel{eq:Likelihood_binomialTwoFull}{{4.24}{28}}
\newlabel{eq:Likelihood_binomialThreeProbsSimpler}{{4.25}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The likelihood function as theta varies for a sample of 2 individuals.\relax }}{29}}
\newlabel{fig:Likelihood_binomial}{{4.5}{29}}
\newlabel{eq:Likelihood_binomialThreeFull}{{4.26}{30}}
\newlabel{eq:Likelihood_binomialNFull}{{4.27}{30}}
\newlabel{eq:Likelihood_binomialTest}{{4.6.2}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}The intelligence of a group of people}{31}}
\newlabel{sec:Likelihood_normal}{{4.6.3}{31}}
\newlabel{eq:Likelihood_normal}{{4.6.3}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Left panel shows a normal with $\mu =70$ and $\sigma ^2 = 81$, with the area corresponding to a result as extreme as 90 indicated. This translates into a standard normal cdf shown in the right panel, which can be used to calculate this area from the first figure. This translation to the standard normal is done by taking away $\mu $, and dividing through by $\sigma $. This is done since usually only standard normal cdf tables are available.\relax }}{32}}
\newlabel{fig:Likelihood_normal}{{4.6}{32}}
\newlabel{eq:Likelihood_normalSampleOne}{{4.30}{32}}
\newlabel{eq:Likelihood_normalN}{{4.6.3}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}The subjectivity of model choice}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Maximum likelihood - a short introduction}{34}}
\newlabel{sec:Likelihood_MLE}{{4.8}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Estimating disease prevalence}{34}}
\newlabel{sec:Likelihood_diseaseMLE}{{4.8.1}{34}}
\newlabel{eq:Likelihood_binomialNew}{{4.32}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A figure with four panes. The top-left is log-likelihood as a function of likelihood. The bottom-left is likelihood as a function of theta, and log-likelihood plotted on the same axis. Top-right is a weird function of likelihood as a function of likelihood. Below it a graph of likelihood as a function of theta, with a different maximum reached for the weird function.\relax }}{35}}
\newlabel{fig:Likelihood_logMonotonicity}{{4.7}{35}}
\newlabel{eq:Likelihood_logLikelihoodBinomial}{{4.33}{35}}
\newlabel{eq:Likelihood_logRules}{{4.34}{35}}
\newlabel{eq:Likelihood_binomialderiv}{{4.35}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Log-likelihood of disease prevalence from section 4.8.1\hbox {} as a function of theta, maximised at 1/10.\relax }}{36}}
\newlabel{fig:Likelihood_MLE}{{4.8}{36}}
\newlabel{eq:Likelihood_binomialestimator}{{4.36}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Estimating the mean and variance in intelligence scores}{37}}
\newlabel{eq:Likelihood_normalTwo}{{4.37}{37}}
\newlabel{eq:Likelihood_diseaseLogLikelihood}{{4.38}{37}}
\newlabel{eq:Likelihood_diseaseDerivativeOne}{{4.39}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Frequentist inference in Maximum Likelihood}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Two likelihoods which result in the same maximum likelihood estimates of parameters, at 0.1. The gray likelihood is less strongly-peaked, meaning we can be less confident about the estimates.\relax }}{39}}
\newlabel{fig:Likelihood_likelihoodCurvature}{{4.9}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Chapter summary}{39}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Priors}{41}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Chapter Mission statement}{41}}
\newlabel{eq:Prior_BayesHighlighted}{{5.1}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Chapter goals}{41}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}What are priors, and what do they represent?}{42}}
\citation{gelman2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Left - a prior for a doctor's pre-testing diagnostic probability of an individual having a disease. Right - a prior which represents pre-sample uncertainty in disease prevalence.\relax }}{43}}
\newlabel{fig:Prior_introduction}{{5.1}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Why don't we just normalise likelihood by choosing a unity prior?}{44}}
\newlabel{sec:Prior_unityPrior}{{5.4}{44}}
\newlabel{eq:Prior_BayesNormalisedLikelihood}{{5.4}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces A figure showing in the left hand panel the Venn diagrams when we assume that a) the coin is fair with a quarter of the area shaded, and the other half not. b) Biased, and the majority of the figure is shaded - corresponding to a high overlap between data and parameter. The right hand panel then shows the joint probability of the data and the parameters $\theta $, and we see that even though the ratio of the area is better for b), it is much less likely \textit  {a priori} that the coin is biased. The bottom panel shows the ML implied posterior distribution, with the bar for unfairness much higher than for fairness. The right shows a much more logical conclusion which takes into account their prior probabilities.\relax }}{45}}
\newlabel{fig:Prior_justification}{{5.2}{45}}
\citation{stewart2014teaching}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}The explicit subjectivity of priors}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Combining a prior and likelihood to form a posterior}{46}}
\citation{bolstad2007introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}An urn of balls\let \reserved@d =[\def \par }{47}}
\newlabel{sec:Prior_urn}{{5.6.1}{47}}
\newlabel{eq:Prior_bernoulli}{{5.3}{47}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here we assume that pre-experiment all possible numbers of red balls are equally likely, by adopting a uniform prior.\relax }}{48}}
\newlabel{tab:Prior_bayesBoxDiscreteUrns}{{5.1}{48}}
\newlabel{tab:addlabel}{{5.1}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Disease proportions revisited}{48}}
\newlabel{sec:Prior_diseaseProp}{{5.6.2}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 5.6.1\hbox {}. The prior in the upper panel gives uniform weighting to all possible numbers of red balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{49}}
\newlabel{fig:Prior_urnStacked}{{5.3}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces A Bayes' box showing how to calculate the posterior for the case of drawing balls from an urn containing 5 red and white balls, one of which has been drawn and shown to be red. Here a higher weighting is given to more equal numbers of red and white balls in the prior.\relax }}{49}}
\newlabel{tab:Prior_bayesBoxUrnUpdated}{{5.2}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The prior, likelihood and posterior for the urn of balls example described in 5.6.1\hbox {}. The prior in the upper panel gives more weighting to more equal numbers of red and white balls. This is then multiplied by the likelihood (in the middle panel) at each number of balls, and normalised to make the posterior density shown in the bottom panel.\relax }}{50}}
\newlabel{fig:Prior_bayesUrnUpdated}{{5.4}{50}}
\newlabel{eq:Prior_binomial}{{5.4}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Constructing priors}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The prior, likelihood and posterior for the disease proportion example described in section 5.6.2\hbox {}. Each point in $\theta $ along the continuous prior curve (top panel) is multiplied by the corresponding value of likelihood (middle panel), to form the numerator of Bayes' rule. The numerator is then normalised to make the posterior probability density shown in the bottom panel.\relax }}{52}}
\newlabel{fig:Prior_disease}{{5.5}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Vague priors}{53}}
\newlabel{sec:Prior_vague}{{5.7.1}{53}}
\newlabel{eq:Prior_BayesFlatPrior}{{5.7.1}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The probability density for obtaining all diseased individuals within sample sizes of 1, 2 and 10 respectively. Starting out with a flat prior for the probability that one individual has a disease has resulted in non-flat priors for the other 2 probabilities.\relax }}{54}}
\newlabel{fig:Prior_jeffreysIntro}{{5.6}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Informative priors}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Two viable prior distributions for the average time taken before the onset of lung cancer after patients begin smoking.\relax }}{56}}
\newlabel{fig:Prior_lungcancerFlatandGammaPriors}{{5.7}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The SAT scores for past students of a class. The mean and variance of this hypothetical sample are 1404, and 79,716 respectively, which are used to fit a normal distribution to the data, and is shown in red.\relax }}{57}}
\newlabel{fig:Prior_SATScoresHistogram}{{5.8}{57}}
\citation{gill2007bayesian}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}The numerator of Bayes' rule determines the shape}{58}}
\newlabel{eq:Prior_BayesNumerator}{{5.7.3}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.4}Eliciting priors}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Hypothetical data for the 25th and 75th percentiles of the estimated wage premium from 10 experts. In the left hand panel we regress these percentiles on the corresponding percentiles from a standard normal distribution, yielding estimates of the mean and variance of a normal prior, which is shown on the right.\relax }}{59}}
\newlabel{fig:Prior_elicitingRegression}{{5.9}{59}}
\newlabel{eq:Prior_elicitingPriorNormal}{{5.7}{59}}
\newlabel{eq:Prior_elicitingPriorNormalRegression}{{5.8}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}A strong model is not heavily influenced by priors}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The effect of increasing sample size on the posterior density for the prevalence of a disease in a population. The leftmost column has N=10, the middle N=100, and the rightmost N=1,000. All three have the same proportion of disease cases in the sample.\relax }}{61}}
\newlabel{fig:Prior_weakPriorEffect}{{5.10}{61}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Chapter summary}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Appendix}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10.1}Bayes' rule for the urn}{62}}
\newlabel{app:Prior_bayesUrn}{{5.10.1}{62}}
\newlabel{eq:Prior_bayesDiscreteForm}{{5.9}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10.2}The probabilities of having a disease}{63}}
\newlabel{app:Prior_diseaseJeffreys}{{5.10.2}{63}}
\newlabel{eq:Prior_appChangeOfVariables}{{5.10}{63}}
\newlabel{eq:Prior_appChangeOfVariablesSolved}{{5.11}{63}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The difficulty is in the denominator}{65}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:denominator}{{6}{65}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}An introduction to distributions for the mathematically-un-inclined}{67}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:distributions}{{7}{67}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conjugate priors and their place in Bayesian analysis}{69}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Objective Bayesian analysis}{71}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ObjectiveBayes}{{9}{71}}
\bibstyle{plain}
\bibdata{Bayes}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Hierarchical models}{73}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:hierarchicalModels}{{10}{73}}
\bibcite{bolstad2007introduction}{{1}{}{{}}{{}}}
\bibcite{epstein2008model}{{2}{}{{}}{{}}}
\bibcite{gelman2013bayesian}{{3}{}{{}}{{}}}
\bibcite{gill2007bayesian}{{4}{}{{}}{{}}}
\bibcite{stewart2014teaching}{{5}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
